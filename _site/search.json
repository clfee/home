[
  {
    "objectID": "posts/2025-02-01-Graphs-for-Communication/index.html",
    "href": "posts/2025-02-01-Graphs-for-Communication/index.html",
    "title": "Graphs for Communication - Creating Animated Maps",
    "section": "",
    "text": "Making the data easier to understand!\nGoal : To demonstrate how to create animated maps with ggplot2 and gganimate in R to track MRSA BSI incidence in the State of California."
  },
  {
    "objectID": "posts/2025-02-01-Graphs-for-Communication/index.html#graphs-for-communication",
    "href": "posts/2025-02-01-Graphs-for-Communication/index.html#graphs-for-communication",
    "title": "Graphs for Communication - Creating Animated Maps",
    "section": "Graphs for Communication",
    "text": "Graphs for Communication\nEarlier this year, I prepared a presentation for our stakeholders to showcase our business performance, particularly after the lifting of restrictions on large events and gatherings. While we saw overall growth, the performance varied across different locations. One of the key focuses of the presentation was to illustrate how sales at wedding venues and restaurants evolved over time.\nTo make the data not only accurate but also engaging, I used the ggplot2 and gganimate packages in R. These tools allowed me to transform raw numbers into visually compelling animations, making the data more understandable and impactful for the audience.\nThis same storytelling approach can be applied to datasets tracking Methicillin-resistant Staphylococcus aureus (MRSA) bloodstream infections (BSI) (BSI) in California Hospitals.\nThese datasets include 95% confidence intervals for the Standardized Infection Ratio (SIR) and statistical interpretations to compare MRSA BSI incidence against the national baseline. Additionally, the data tracks hospital progress towards national Healthcare-Associated Infection (HAI) reduction goals. Hospitals need to meet or exceed incremental SIR targets each year to stay on track.\nIn this exercise, we will create an animated map to show whether MRSA BSI incidence in California was the same, better, or worse than the national baseline from 2019 to 2023. The map will use a color scale ranging from purple (-1, indicating worse than the national average) to yellow (1, indicating better than the national average) to depict these comparisons."
  },
  {
    "objectID": "posts/2025-02-01-Graphs-for-Communication/index.html#steps",
    "href": "posts/2025-02-01-Graphs-for-Communication/index.html#steps",
    "title": "Graphs for Communication - Creating Animated Maps",
    "section": "Steps",
    "text": "Steps\nThe first section simply loads the libraries and data that will be used.\n\n\nCode\n```{r}\n#| context: setup\n#| code-fold: true\n#| warning: false\n\n# Load required packages\nlibrary(gganimate)\nlibrary(gifski)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\n# Load data\nurl_2022 &lt;- \"https://data.chhs.ca.gov/dataset/a345bba7-25ed-4019-b6e3-0b597bea7368/resource/3ff809c6-c1e2-4107-be0c-f976521aadb2/download/mrsa_bsi_odp_2022.csv\"\nurl_2021 &lt;- \"https://data.chhs.ca.gov/dataset/a345bba7-25ed-4019-b6e3-0b597bea7368/resource/23bef156-ae4d-4800-8512-d29660b2269f/download/cdph_mrsa_bsi_odp_2021.csv\"\nurl_2019 &lt;- \"https://data.chhs.ca.gov/dataset/a345bba7-25ed-4019-b6e3-0b597bea7368/resource/62aead61-e320-4a74-b64c-1f65efa72b35/download/cdph_mrsa_bsi_odp_2019.csv\"\nurl_2023 &lt;-\"https://data.chhs.ca.gov/dataset/a345bba7-25ed-4019-b6e3-0b597bea7368/resource/15c4962f-e357-4b03-9d24-655a2bc8030c/download/mrsa_bsi_odp_2023.csv\"\n\ndata_2023 &lt;- read_csv(url_2023)\ndata_2022 &lt;- read_csv(url_2022)\ndata_2021 &lt;- read_csv(url_2021)\ndata_2019 &lt;- read_csv(url_2019)\n\n# Combine data\nmrsa_combine &lt;- bind_rows(data_2023,data_2022, data_2021, data_2019)\n\n# Remove redundant data\nmrsa_combine &lt;- unique(mrsa_combine)\n# Lon and lat info\nus_counties &lt;- map_data(\"county\")|&gt; filter(region ==\"california\")|&gt; mutate( subregion = toupper( subregion))|&gt; dplyr::select(-c( order,  region)) \n\n# Data transformation\nmrsa_combine &lt;- mrsa_combine |&gt;\n  dplyr::select(-State, -HAI, -Met_2020_Goal, -SIR_CI_95_Lower_Limit,-SIR_CI_95_Upper_Limit, -On_Track, -Notes) |&gt;\n  mutate(Months = as.numeric(ifelse(is.na(Months), 13, Months)),\n         Year = as.integer(ifelse(is.na(Year), 2021, Year)),\n         Comparison = case_when(\n      Comparison == \"Worse\" ~ -1,\n      Comparison == \"Better\" ~ 1,\n      Comparison == \"Same\" ~ 0,\n      .default = 0),\n         Patient_Stay = cut(      \n           Patient_Days,\n           breaks = quantile(Patient_Days, probs = seq(0, 1, by = 0.1), na.rm = TRUE),\n           labels = 1:10),\n         Months = as.character(Months),\n         Year = as.character(Year),\n         Infections_Reported = as.numeric(Infections_Reported))|&gt;\n  filter(!grepl(\"Pooled\", Facility_Name, ignore.case = TRUE),\n         !grepl(\"Pooled\", Facility_Type, ignore.case = TRUE))|&gt; \n  mutate( subregion = toupper(County))\n```\n\n\n\nStep 1\nAs gganimate extends the grammar of graphics as implemented by ggplot2 to include the description of animation. Therefore, we will need to generate a base map using ggplot2. Then, add a function of facet_wrap(Hospital_Category_RiskAdjustment) to track the average performance of each hospital category in each county across California.\n\n\nStep 2\nThen, the gganimate library is used to animate the display of color by year (2019 – 2023). Here we define the transition time (in years), add a title and subtitle.\nNote: 1. transition_*() defines how the data should be spread out and how it relates to itself across time.  2. The comparison is depicted values and colors range from -1 (purple, worse than the national average) to 1 (yellow, better than the national average).\n\n\nCode\n```{r}\n#| title: Bloodstream infections (BSI) compare to national overtime\n#| code-fold: true\n#| warning: false\n#| fig-width: 8\n#| fig-height: 8\n\nst &lt;- mrsa_combine|&gt;\n    dplyr::select(subregion, Year, Comparison,Hospital_Category_RiskAdjustment)|&gt;\n  mutate(Year = as.integer(Year))|&gt;\n  group_by(subregion, Year,Hospital_Category_RiskAdjustment)|&gt; \n  summarise(`national baseline` = round(mean(na.omit(as.numeric(Comparison))),2)) \n\nst &lt;- left_join(st, unique(us_counties), by= \"subregion\", relationship = \"many-to-many\") \n\na &lt;-\nst|&gt; \n\n  ggplot(mapping = aes(x = long, y = lat, group = group, fill = `national baseline` ))+\n              geom_polygon(color = \"gray90\", linewidth = 0.3) +\n  coord_map(projection = \"albers\", lat0 = 39, lat1 = 45) +\n  scale_fill_continuous(type = \"viridis\")+\n  theme(axis.line=element_blank(),\n        axis.text=element_blank(),\n        axis.ticks=element_blank(),\n        axis.title=element_blank(),\n        panel.background=element_blank(),\n        panel.border=element_blank(),\n        panel.grid=element_blank())+\n        scale_color_gradient(low = \"green\", high = \"red\")+ \n   facet_wrap(~Hospital_Category_RiskAdjustment)\n            \n            \na + transition_time(Year) +labs(title = \"MRSA BSI incidence compares to the national baseline, Year: {frame_time}\")\n```\n\n\n\n\n\n\n\n\n\n\n\nBonus\nAlso compatible with other ggplot graphs!\nIn this example we see trend plots of continuous variables infections cases and days of patient hospital stays between 2019 and 2023.\nDid you notice the length of patient stay in a hospital is not necessarily positive correlated with numbers of reported cases (except for the acute care hospital category)?\n\n\nCode\n```{r}\n#| title: Patients Days vs Reported infection cases\n#| code-fold: true\n#| warning: false\n#| fig-width: 8\n#| fig-height: 8\n\nst &lt;- left_join(mrsa_combine, unique(us_counties), by= \"subregion\", relationship = \"many-to-many\") \n\np &lt;-     \n  st|&gt; \n  ggplot(aes(x= Patient_Days, y= Infections_Reported))+\n    geom_point(alpha = 0.5, show.legend = FALSE, color = \"#167bb2\") +\n    scale_size(range = c(2, 12)) +\n    theme_bw()+\n    ggtitle(paste0(\"Length of Patients stay vs Reported infection cases\")) +\n    theme(#axis.text.x=element_blank(),\n          axis.text=element_text(size=12),\n          legend.position = \"bottom\",\n          axis.line = element_line(colour = \"black\"),\n          panel.grid.major = element_blank(),\n          panel.grid.minor = element_blank(),\n          panel.border = element_blank(),\n          panel.background = element_blank())+\n          facet_wrap(~Hospital_Category_RiskAdjustment)+\n    xlab(\"Days\")+ylab(\"Cases\")+labs(caption=\"Produced by CF Lee\")\n\n\np + transition_time(as.integer(Year))+  \n  enter_fade() +\n  exit_fade()+labs(title = \"Year: {frame_time}\")\n```"
  },
  {
    "objectID": "posts/2025-02-01-Graphs-for-Communication/index.html#resources",
    "href": "posts/2025-02-01-Graphs-for-Communication/index.html#resources",
    "title": "Graphs for Communication - Creating Animated Maps",
    "section": "Resources",
    "text": "Resources\n\ngganimate link\nCalifornia Hospitals data."
  },
  {
    "objectID": "posts/2024-08-16-automated-custom-reports/index.html",
    "href": "posts/2024-08-16-automated-custom-reports/index.html",
    "title": "Automated Reporting: A Game-Changer for Data Analysis",
    "section": "",
    "text": "In the fast-paced world of data analysis, efficiency is key. Automated reporting is revolutionizing how we handle recurring data-rich reports, offering on-demand production and enhanced data exploration capabilities. This innovative approach not only saves time but also allows for deeper, more strategic insights. Parameterized reporting is a cutting-edge technique that enables simultaneous generation of multiple reports. This method not only boosts productivity but also significantly reduces errors associated with manual data handling.\nGoal : The objective of this report is to demonstrate how to create automated custom reports with Quarto and Purrr in R to track patient days from public data."
  },
  {
    "objectID": "posts/2024-08-16-automated-custom-reports/index.html#real-world-application-california-mrsa-data-analysis",
    "href": "posts/2024-08-16-automated-custom-reports/index.html#real-world-application-california-mrsa-data-analysis",
    "title": "Automated Reporting: A Game-Changer for Data Analysis",
    "section": "Real-World Application: California MRSA Data Analysis",
    "text": "Real-World Application: California MRSA Data Analysis\nTo illustrate the power of automated reporting, let’s consider a case study using data from the California California Health and Human Services Open Data Portal. The demo uses the data from All California general acute care hospitals are required to report Methicillin-resistant Staphylococcus aureus (MRSA) bloodstream infection (BSI) cases that occur following hospitalization. MRSA is a serious, contagious bacterial infection that starts on your skin. It’s a type of staph infection that resists most common antibiotics, making it especially dangerous. Without treatment, MRSA can be deadly."
  },
  {
    "objectID": "posts/2024-08-16-automated-custom-reports/index.html#step-by-step-guide",
    "href": "posts/2024-08-16-automated-custom-reports/index.html#step-by-step-guide",
    "title": "Automated Reporting: A Game-Changer for Data Analysis",
    "section": "Step-by-Step Guide",
    "text": "Step-by-Step Guide\n\nStep 1: Create a Single Report Template\nWe began by preparing a single report template using data from 2022. This template serves as the foundation for all subsequent reports. Let’s prepare a single report as a template from the data in 2022.\n\nHealthcare Associated Infection Report, 2022\n\nResults\nFigure 1: Box plots of Standardized Infection Ratio (SIR) in 2022.\n\n\nCode\n```{r}\n#| label: Boxplots\n#| code-fold: true\n#| warning: false\n\ncolor_palette &lt;- c(\"#167bb2\",\"#2e3b4f\",\"#7A6C5D\",\"#2e4f42\",\"#A54657\",\"#624151\",\"#16b29b\",\"#002c47\",\"#9d72d0\"\n)\n\n mrsa_combine|&gt;\n    filter(Year == params$year) |&gt;\n    filter(County %in% c(\"San Diego\" ,\"San Francisco\", \"Los Angeles\", \"Sacramento\", \"Orange\",\"Yolo\" ))|&gt;\n    ggplot(aes(x= Hospital_Type, y= SIR, fill=Hospital_Type))+\n     geom_boxplot()+\n    scale_fill_manual(values=color_palette)+\n       theme(\n      legend.position=\"none\",\n      plot.title = element_text(size=11),\n      axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)\n    ) +\n    ggtitle(paste0(\"Standardized Infection Ratio (SIR) in \",params$year)) +\n    xlab(\"\")+ylab(\"\")+labs(caption=\"Produced by CF Lee\")+\n   facet_wrap(~County)\n```\n\n\n\n\n\n\n\n\n\nFigure 2: Patients Days vs Reported infection cases in 2022\n\n\nCode\n```{r}\n#| code-fold: true\n#| warning: false\n\n mrsa_combine$Months &lt;- as.numeric(mrsa_combine$Months)\n\n mrsa_combine |&gt; \n    filter(Year == params$year) |&gt;\n    mutate(Quarters = case_when(\n                         Months &gt; 0 & Months &lt; 4 ~ 1,\n                         Months &gt; 3 & Months &lt; 7 ~ 2,\n                         Months &gt; 6 & Months &lt; 10 ~ 3 , \n                         Months &gt; 9 & Months &lt; 13 ~ 4 , \n                                .default = 4))|&gt;\n  # group_by(Quarters,Hospital_Category_RiskAdjustment, Year)|&gt;\n  #  summarise(mean = mean(na.omit(Infections_Reported)))|&gt; \n    \n    ggplot(aes(x= Patient_Days, y= Infections_Reported, \n               colour = as.integer(Quarters)))+\n     geom_point(alpha = 0.7, show.legend = FALSE) +\n\n    theme_bw()+\n    ggtitle(paste0(\"Patients Days vs Reported infection cases in \",params$year)) +\n    theme(#axis.text.x=element_blank(),\n          axis.text=element_text(size=12),\n          legend.position = \"bottom\",\n          axis.line = element_line(colour = \"black\"),\n          panel.grid.major = element_blank(),\n          panel.grid.minor = element_blank(),\n          panel.border = element_blank(),\n          panel.background = element_blank())+\n          facet_wrap(~Hospital_Category_RiskAdjustment)+\n    xlab(\"Days\")+ylab(\"Cases\")+labs(caption=\"Produced by CF Lee\")\n```\n\n\n\n\n\n\n\n\n\nNote: Patient Days is define as below. A count of the number of patients in the hospital during a time period, excluding IRFs and IPFs with their own CCN numbers. A rehabilitation unit (IRF) with its own CCN number reports separately from the rest of the hospital. To calculate patient days, the number of patients is recorded at the same time each day for each day of the month. At the end of the month, the daily counts are summed. See the NHSN website: http://www.cdc.gov/nhsn.\n\n\n\nStep 2: Generate All Report Variations\nAfter creating the single report, we used an R script to render the parameterized Quarto template with each defined parameter. This step allows for the simultaneous generation of all report variations, ensuring consistency and accuracy.To do this, write an R script to render the parameterized Quarto template with each of the defined parameters.\nIn our example the code looks like this,\n# 1. First create a dataframe \n\ndata &lt;- expand.grid(\n  year = c(2019,2021:2023),\n  record_date = Sys.Date(), \n  stringsAsFactors = FALSE)\n\ndf &lt;- data |&gt; \n  dplyr::mutate(\n    output_format = \"html\",       # Output format (html, word, etc.)\n    output_file = paste(          # Output file name\n      year, \"report.html\",\n      sep = \"-\"\n    ),\n    execute_params = purrr::map2( # Named list of parameters\n      record_date, year, \n      \\(record_date, year) list(record_date = record_date, year = year)\n    )\n  ) |&gt; \n  dplyr::select(-c(record_date, year))\n\ndf\n\n# 2. Use purrr::pwalk() to map over each row of the dataframe and render each report variation.\n\npurrr::pwalk(\n  .l = df,                      # Dataframe to map over\n  .f = quarto::quarto_render,   # Quarto render function\n  input = \"demo_autoreport.qmd\",       # Named arguments of .f\n  .progress = TRUE              # Optionally, show a progress bar\n)\nBy adopting this automated approach, analysts can shift their focus from report creation to data interpretation, leading to more impact decision-making and strategic planning.\nAutomatically generated reports, one after another!"
  },
  {
    "objectID": "posts/2024-08-16-automated-custom-reports/index.html#resources",
    "href": "posts/2024-08-16-automated-custom-reports/index.html#resources",
    "title": "Automated Reporting: A Game-Changer for Data Analysis",
    "section": "Resources",
    "text": "Resources\n\nThe raw data is obtained from California Health and Human Services Open Data Portal datasets\nFull code link"
  },
  {
    "objectID": "posts/2021-08-31-malaria-cell-classification - p2/index.html",
    "href": "posts/2021-08-31-malaria-cell-classification - p2/index.html",
    "title": "Image Classification (Part2)",
    "section": "",
    "text": "When building a predictive model, there are two important criteria: predictive accuracy and interpretability, which generally have a trade-off relationship. In the previous post, we have shown that the DenseNet121 model can achieve high accuracy in detecting cells infected with parasites.\nHere, I am going to introduce a powerful technique GRAD-CAM (gradient-weighted class activation mapping) to visualize which parts of an image are most important to the predictions of an image regression network. GRAD-CAM is a generalization of the CAM technique which determines the importance of each neuron in a network prediction by considering the gradients of the target flowing through the deep network. Unlike CAM which requires a particular kind of CNN architecture to perform global average pooling prior to prediction and forces us to change the base model retrain the network. In contrast, GRAD-CAM is accessing intermediate activations in the deep learning model and computing gradients with respect to the class output. For more details, please see.\nWorkflow: - Obtain predicted class/index - Determine which intermediate layer(s) to use. Lower-level convolution layers capture low level features such as edges, and lines. Higher-level layers usually have more abstract information. - Calculate the gradients with respect to the outout of the class/index - Generate a heatmap by weighing the convolution outputs with the computed gradients - Super-impose the heatmap to the original image\nLoad base model\nWe first load the base model and will only train the last 4 layers.\ndef build_model(input_shape=(224, 224,3),pos_weights,neg_weights):\n  # load the base DenseNet121 model\n  base_model = DenseNet121(input_shape = input_shape, \n                      weights='imagenet', \n                      include_top=False)\n  \n  # add a GAP layer\n  output = layers.GlobalAveragePooling2D()(base_model.output)\n\n  # output has two neurons for the 2 classes (uninfected and parasite)\n  output = layers.Dense(2, activation='softmax')(output)\n\n  # set the inputs and outputs of the model\n  model = Model(base_model.input, output)\n\n  # freeze the earlier layers\n  for layer in base_model.layers[:-4]:\n      layer.trainable=False\n\n  # configure the model for training\n  model.compile(loss= get_weighted_loss(neg_weights, pos_weights), \n                optimizer=adam, \n                metrics=['accuracy'])\n  \n  return model\n \nWe then create a new model that has the original model’s inputs, but two different outputs. The first output contains the activation layers outputs that in this case is the final convolutional layer in the original model. And the second output is the model’s prediction for the image.\ndef get_CAM(model, processed_image, actual_label, layer_name): \n    \"\"\"\n    GradCAM method for visualizing input saliency.\n    \n    Args:\n        model (Keras.model): model to compute cam for\n        image (tensor): input to model, shape (1, H, W, 3)\n        cls (int): class to compute cam with respect to\n        layer_name (str): relevant layer in model\n        H (int): input height\n        W (int): input width\n    Return:\n        heatmap()\n    \"\"\"    \n\n    model_grad = Model([model.inputs], \n                       [model.get_layer(layer_name).output, model.output])\n    \n    with tf.GradientTape() as tape:\n        conv_output_values, predictions = model_grad(processed_image)\n\n        # assign gradient tape to monitor the conv_output\n        tape.watch(conv_output_values)\n        \n        # use binary cross entropy loss, actual_label = 0 if uninfected\n        # get prediction probability of infected  \n        pred_prob = predictions[:,1] \n        \n        # make sure actual_label is a float, like the rest of the loss calculation\n        actual_label = tf.cast(actual_label, dtype=tf.float32)\n        \n        # add a tiny value to avoid log of 0\n        smoothing = 0.00001 \n        \n        # Calculate loss as binary cross entropy\n        loss = -1 * (actual_label * tf.math.log(pred_prob + smoothing) + (1 - actual_label) * tf.math.log(1 - pred_prob + smoothing))\n        print(f\"binary loss: {loss}\")\n    \n    # get the gradient of the loss with respect to the outputs of the last conv layer\n    grads_values = tape.gradient(loss, conv_output_values)\n    grads_values = K.mean(grads_values, axis=(0,1,2))\n    \n    conv_output_values = np.squeeze(conv_output_values.numpy())\n    grads_values = grads_values.numpy()\n    \n    # weight the convolution outputs with the computed gradients\n    for i in range(grads_values.shape[-1]): \n        conv_output_values[:,:,i] *= grads_values[i]\n    heatmap = np.mean(conv_output_values, axis=-1)\n    \n    heatmap = np.maximum(heatmap, 0)\n    heatmap /= heatmap.max()\n    \n    del model_grad, conv_output_values, grads_values, loss\n   \n    return heatmap\n\n\n\nResult\n\n\nNote: Instead of using max pooling that only keeps the highest valued ones. Average pooling allows some of the lesser intensity pixels to pass on in the pooling layer. It is important as we look at the small size of the image once it reaches this layer, max pooling could leave us with very little information.\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Chris Lee",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "assets/other/year-archive.html",
    "href": "assets/other/year-archive.html",
    "title": "Posts by Year",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "assets/other/sitemap.html",
    "href": "assets/other/sitemap.html",
    "title": "Website Structure",
    "section": "",
    "text": "Pages\n\n{% for post in site.pages %} {% include archive-single.html %} {% endfor %}\n\nPosts\n\n{% for post in site.posts %} {% include archive-single.html %} {% endfor %}\n{% capture written_label %}‘None’{% endcapture %}\n{% for collection in site.collections %} {% unless collection.output == false or collection.label == “posts” %} {% capture label %}{{ collection.label }}{% endcapture %} {% if label != written_label %}\n\n{{ label }}\n\n{% capture written_label %}{{ label }}{% endcapture %} {% endif %} {% endunless %} {% for post in collection.docs %} {% unless collection.output == false or collection.label == “posts” %} {% include archive-single.html %} {% endunless %} {% endfor %} {% endfor %}\n\n\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "\nHello\n",
    "section": "",
    "text": "Hello\n\n\n\n\n\n\n\n\nI’m driven by helping others successfully solve problems like turning complex data into clear, actionable insights that help businesses thrive. My toolkit to deliver these solutions draws on blending business understanding with machine learning using SQL, R, SAS, and Python. I’m passionate about using data to solve real-world problems, streamline processes, and find new growth opportunities. Balancing the technical side with a strong understanding of business needs, I create solutions that make a real impact and keep things moving forward.\nFeel free to reach out to me on LinkedIn or drop me an email to discuss.\n\n\n\n\n\nEducation:\n\nUniversity of Manchester | UK PhD in Cancer Studies | Sept 2004 - Jan 2009\nNational Yang-Ming University | Taipei, Taiwan Msc in Genetics| Sept 2001 - Sept 2003\n\nExperiences:\n\nFUJI WEDDING VENUE GROUP | Senior Data Analyst | Jan 2023 - PRESENT\nFUJI WEDDING VENUE GROUP | Data Analyst | Jan 2016 - Dec 2022\nProtea | Senior Scientist | Jan 2014 - Mar 2015\n\n\n\n\n Back to top"
  },
  {
    "objectID": "assets/other/tags-archive.html",
    "href": "assets/other/tags-archive.html",
    "title": "Posts by Tags",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "gallery.html",
    "href": "gallery.html",
    "title": "Chris Lee",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\n\n\nTitle\n\n\n\n\n\n\n2025-02-01\n\n\n\n\n\nForecasting: exploratory data analaysis\n\n\n\n\n2024-09-01\n\n\n\n\n\nGraphs for Communication - Creating Animated Maps\n\n\n\n\n2024-08-16\n\n\n\n\n\nAutomated Reporting: A Game-Changer for Data Analysis\n\n\n\n\n2024-08-01\n\n\n\n\n\nTying the data together\n\n\n\n\n2024-07-15\n\n\n\n\n\nMarketing Campaign at a Glance\n\n\n\n\n2024-06-25\n\n\n\n\n\nCreating a Serveless Dashboard\n\n\n\n\n2021-08-31\n\n\n\n\n\nImage Classification (Part2)\n\n\n\n\n2021-08-31\n\n\n\n\n\nImage Classification (Part1)\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "posts/2021-08-31-malaria-cell-classification/index.html",
    "href": "posts/2021-08-31-malaria-cell-classification/index.html",
    "title": "Image Classification (Part1)",
    "section": "",
    "text": "I remember years ago seeing my colleague spent hours under a microscopes counting cells underwent of apoptosis or Dauer larva formation. I mean it is fun doing experiments in the lab but telling differences of these tiny worms would probably is the last thing I’d want to do. This task does take lots of valuable time from a researcher. Imagine, how many more novel anti-agents like this article Yongsoon could bring us if the deep learning techniques were ready to use back in 2011.\n\n\n\nKim Y, Sun H (2012) PLOS ONE 7(9): e45890\n\n\nThanks to the advancement in deep learning field, neural network model architectures can be readily reused and, in most cases, are tested across multiple applications to establish robustness. Here, I’m going to show how easy it is to implement transfer learning using Keras in Python for Malaria cell classification. The basic concept of transfer learning is using the knowledge (architecture or weights) gained from a neural network model that was trained to recognize animals to recognize cats. The dataset used here came from NIH, along with recent publications1,2.\n\nWorkflow\n\nLoading data and data pre-processing\nTransfer learning and fine-tuning (DenseNet121)\nResult evaluation\n\n\n\n\nData Overview\nThere are many ways to create train/valid/test data sets. Below is one of the methods using R to create csv files containing file paths and classifications from train and test folders.\n# R code\nlibrary(fs)\ndataset_dir &lt;- \"Data/cell_images/\"\ntest_dir   &lt;- paste0(dataset_dir, \"/test/\")\n# stored image paths in the image column\ntest_parasite &lt;- dir_ls(path=paste0(test_dir, \"parasite\"),glob = \"*.png\")\ntest_uninfected &lt;- dir_ls(path=paste0(test_dir, \"uninfected\"),glob = \"*.png\")\ntest_par &lt;- as.data.frame(matrix('parasite', length(test_parasite), 1))\ntest_unin &lt;- as.data.frame(matrix('uninfected', length(test_parasite), 1))\ntest_par$image &lt;- test_parasite \ntest_unin$image &lt;- test_uninfected\n\ntest &lt;- rbind(test_par,test_unin)\ncolnames(test)[1] &lt;- 'label'\ntest$normal  &lt;- ifelse(test$label != 'parasite', 1,0)\ntest$parasite &lt;- ifelse(test$label == 'parasite', 1,0)\nAnd the csv file looks like this. \nIn reality, we don’t usually see many cells infected with parasites, therefore less than 1/3 of the infected samples were used in this exercise.\n# Python\n# get ids for each label\nall_img_ids = list(new_df.uninfected.index.unique())\ntrain_ids, test_ids = train_test_split(all_img_ids, test_size=0.01, random_state=21)\ntrain_ids, valid_ids = train_test_split(train_ids, test_size=0.1, random_state=21)\nMaking sure, the proportion of the infected cell is as expected after data split. \nLet’s also check few images. The images come with different sizes. They will need to reshape and normalize before xx.\n# Extract numpy values from image column in data frame\ntrain_df = new_df.iloc[train_ids,:]\nimages = train_df['image'].values\n# Extract 9 random images \nrandom_images = [np.random.choice(images) for i in range(9)]\nimg_dir = 'C:/Users/your_image_folder'\nprint('Display Random Images')\n# Adjust the size of your images\nplt.figure(figsize=(20,10))\nfor i in range(9):\n    plt.subplot(3, 3, i + 1)\n    img = plt.imread(os.path.join(img_dir, random_images[i]))\n    plt.imshow(img)\n    plt.axis('off')\n\n\n\nrandom_images\n\n\n\n\nLoading data\nNext is building generators from the Keras framework. The purpose of building generator is that it allows to generate batches of tensor image data with real-time data augmentation(ex: random horizontal flipping of images). We also use the generator to transform the values in each batch so that their mean is 0 and their standard deviation is 1.Here is the information of ImageDataGenerator and a short tutorial. We’ll also need to build a sereperate generator for valid and test sets. Since each image will be normailized using mean and standard deviation derived from its own batch. In a real life scenario, we process one image at a time. And the incoming image is normalized using the statistics computed from the training set.\n# Train generator\ndef get_train_generator(df, image_dir, x_col, y_cols, shuffle=True, batch_size=8, seed=1, target_w = 224, target_h = 224):\n    \"\"\"\n\n    Args:\n      train_df (dataframe): dataframe specifying training data.\n      image_dir (str): directory where image files are held.\n      x_col (str): name of column in df that holds filenames.\n      y_cols (list): list of strings that hold y labels for images.\n      sample_size (int): size of sample to use for normalization statistics.\n      batch_size (int): images per batch to be fed into model during training.\n      seed (int): random seed.\n      target_w (int): final width of input images.\n      target_h (int): final height of input images.\n    \n    Returns:\n        train_generator (DataFrameIterator): iterator over training set\n    \"\"\"        \n    print(\"getting train generator...\") \n    # normalize images\n    image_generator = ImageDataGenerator(\n        samplewise_center=True,\n        samplewise_std_normalization= True)\n    \n    # flow from directory with specified batch size and target image size\n    generator = image_generator.flow_from_dataframe(\n            dataframe=df,\n            directory=image_dir,\n            x_col=x_col,\n            y_col=y_cols,\n            class_mode=\"raw\",\n            batch_size=batch_size,\n            shuffle=shuffle,\n            seed=seed,\n            target_size=(target_w,target_h))\n    \n    return generator  \nBefore, model building we’ll need to define a loss function to address class imbalance. We can give more weight for the less frequent class and less weight for the other one, see here . We can write the overall average cross-entropy loss over the entire training set D of size N as follows:\n\n\n\nloss\n\n\nNext, we will use a pre-trained DenseNet121 model which we can load directly from Keras and then add two layers on top of it.\n\nSet include_top=False, to remove the orginal fully connect dense layer (so you can adjust the ouptut prediction clsses or\nactivation function).\nUse specific layer using get_layer(). For example: base_model.get_layer(‘conv5_block16_conv’)\n\nA GlobalAveragePooling2D layer to get the average of the last convolution layers from DenseNet121. The pooling layer typically uses a filter to extract representative features (e.g., maximum, average, etc.) for different locations. The method of extracting features from the pooling filter is called a pooling function. The commonly used pooling functions include the maximum pooling function, average pooling function, L2 normalization, and weighted average pooling function based on the distance from the center pixel. In short, the pooling layer summarizes all the feature information centered on each position of the input feature map, which makes it reasonable that the output data of the pooling layer is less than the input data. This method reduces the input data to the next layer and improves the computational efficiency of the CNN.\nThe output of the pooling layer is flattening to convert the pooled features maps into a single dimensional array. This is done in order for the data to be fed into densely connected hidden layers.\nA Dense layer with sigmoid activation to get the prediction logits for each of our classes. We can set our custom loss function for the model by specifying the loss parameter in the compile() function.\n# Build model\ndef create_dense121_model():\n    \n    pretrained = 'fine_tuned.hdf5'\n    train_df = pd.read_csv(\"train_df.csv\")\n    labels = ['uninfected', 'parasite']  \n    \n    class_pos = train_df.loc[:, labels].sum(axis=0)\n    class_neg = len(train_df) - class_pos\n    class_total = class_pos + class_neg\n\n    pos_weights =  class_pos / class_total #[0.5,class_pos / class_total]\n    neg_weights =  class_neg / class_total #[0.5,class_neg / class_total]\n    print(\"Got loss weights\")\n    \n    def create_model(input_shape=(224, 224,3)):\n        base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=input_shape)\n       \n        # add a global spatial average pooling layer\n        x = GlobalAveragePooling2D()(base_model.output)\n        x = Flatten()(x)\n        x = Dense(1024, activation='relu', name='dense_post_pool')(x)\n        x = Dropout(0.8)(x)\n        # output has two neurons for the 2 classes (uninfected and parasite)\n        predictions = Dense(len(labels), activation='sigmoid')(x)\n        model = Model(inputs = base_model.input, outputs = predictions)\n\n        # freeze the earlier layers\n        for layer in base_model.layers[:-4]:\n            layer.trainable=False\n        \n        return model\n    \n    def get_weighted_loss(neg_weights, pos_weights, epsilon=1e-7):\n        def weighted_loss(y_true, y_pred):\n            y_true = tf.cast(y_true, tf.float32)\n            #print(f'neg_weights : {neg_weights}, pos_weights: {pos_weights}')\n            #print(f'y_true : {y_true}, y_pred: {y_pred}')\n            # L(X, y) = −w * y log p(Y = 1|X) − w *  (1 − y) log p(Y = 0|X)\n            # from https://arxiv.org/pdf/1711.05225.pdf\n            loss = 0\n            \n            for i in range(len(neg_weights)):\n                loss -= (neg_weights[i] * y_true[:, i] * K.log(y_pred[:, i] + epsilon) + \n                         pos_weights[i] * (1 - y_true[:, i]) * K.log(1 - y_pred[:, i] + epsilon))\n            \n            loss = K.sum(loss)\n            return loss\n        return weighted_loss\n    \n   \n    model = create_model()\n    model.load_weights(pretrained)\n    print(\"Loaded Model\")\n    \n    model.compile(optimizer='adam', loss= get_weighted_loss(neg_weights, pos_weights)) \n    print(\"Compiled Model\")   \n          \n    return model\nModel is fine tuned using ModelCheckpoint and only the model’s weights will be saved.\n# CallBack \n# -------------------------------------------------------------------------------------------------\n# Callback Function 1\nfname = 'dense121(V)_Epoch[{epoch:02d}].ValLoss[{val_loss:.3f}].hdf5'\nfullpath = fname\n# https://keras.io/api/callbacks/model_checkpoint/\ncallback_func1 = ModelCheckpoint(filepath=fullpath,             \n                                monitor='val_loss',             \n                                verbose=1,                      \n                                save_best_only=True,            \n                                save_weights_only=True, # save weights       \n                                mode='min',                     \n                                period=1)                       \n\n# Callback Function 2\n# https://keras.io/callbacks/#tensorboard\ncallback_func2 = keras.callbacks.TensorBoard(log_dir='./logs/log2', histogram_freq=1)\n\n# Callback Function\ncallbacks = []\ncallbacks.append(callback_func1)\ncallbacks.append(callback_func2)\n\n# Training and Plotting\n# -------------------------------------------------------------------------------------------------\nhistory = model.fit(train_generator, \n                              validation_data=valid_generator,\n                              steps_per_epoch=100, \n                              validation_steps=25, \n                              epochs = 15,\n                              callbacks=callbacks)\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.ylabel(\"loss\")\nplt.xlabel(\"epoch\")\nplt.title(\"Training Loss Curve\")\nplt.show()\nTrain versus validation loss for all epochs is shown here. The orange and blue lines indicate train loss and validation loss respectively. We can see the model may be under-fitted. One way to overcome this is simply increase the number of epochs. Also with the callback function, we can re-use the best weights saved at 12th epoch.\n\n\n\nhistory\n\n\n\n\nEvaluation\nThe ROC curve is created by plotting the true positive rate against the false positive rate. We can see the model performs reasonable well.\n\n\n\nROC\n\n\nWe can try different approaches to improve the model perfromance, such as train the model for a longer time or use all the training data (since only 1/3 of the parasite data was used). We can also try a different base model, the previous publication, shows 99.32% accuracy with VGG-19 alone.\n\n\nVisualize class activation maps\nNext, I will show how to produce visual explanation using Grad-CAM. The purpose of doing this is as following:\n\nDebug your model and visually validate that it is “looking” and “activating” at the correct locations in an image.\nGrad-CAM works by (1) finding the final convolutional layer in the network and then (2) examining the gradient information flowing into that layer.\n\n\n\nNotes\nNote 1: AUC is the area below these ROC curves. Therefore, in other words, AUC is a great indicator of how well a classifier functions. Note 2: A good tutorial for to learn neural network image classification from scratch and Andrew Ng’s deep learning course.\nNote 3:\n# packages used \nimport os\nimport sklearn\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport keras\nfrom keras.applications.densenet import DenseNet121\nfrom keras.models import Model\nfrom keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization, GlobalAveragePooling2D\nfrom keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, TensorBoard\nfrom keras import backend as K\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import ImageDataGenerator\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2024-06-25-create-a-Quarto-dashboard/index.html",
    "href": "posts/2024-06-25-create-a-Quarto-dashboard/index.html",
    "title": "Creating a Serveless Dashboard",
    "section": "",
    "text": "A while ago I wrote about creating a serverless dashboard (with no server dependencies) using Flexdashboard. As some of the functions now are no longer supported, I found a good alternative called “Quarto Dashboards”. Quarto Dashboards allow to create dashboards using Python, R, Julia, and Observable. More details are here.\nDashboards can be created either using Jupyter notebooks (.ipynb) or using plain text markdown (.qmd). I am showing how to create one using RStudio.\n\nClick File -&gt; New File -&gt; Quarto Document\n\n\n\nA new .qmd file is created\n\nHere is the code for the visual version of the dashboard link\nHere is the plain text .qmd version of the dashboard.\n\nThe document options define the title and author for the navigation bar as well as specifying the use of the dashboard format.\nBy default, dashboard pages are laid out first by row, then by column. In this demo, I changed this by specifying the orientation: columns document option:\n\ntitle: \"Marketing Campaign at a Glance\"\nauthor:\n    name: Chris Lee\n    url: https://clfee.github.io/\ndate: 2024-07-15\nformat: \n  dashboard:\n    orientation: columns\n    nav-buttons: [github]\n    github: https://github.com/clfee\nlogo: \"/assets/images/cmm1.PNG\"\ntheme: custom.scss\neditor_options: \n  chunk_output_type: console\n\nEach row in the dashboard that is not given an explicit height will determine its size by either filling available space or by flowing to its natural height.Here I changed the figure height by specifying the #| fig-height: 6 \n\n#| title: Correlations\n#| fig-height: 6\n#| padding: 0;\ndt &lt;- train\nMore details about full code.\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2024-08-01-tying-the-data-together/index.html",
    "href": "posts/2024-08-01-tying-the-data-together/index.html",
    "title": "Tying the data together",
    "section": "",
    "text": "Have you ever get tired of typing functions into vlookup? In this post, we will use tidyverse in R to modify, combine, search, and merge several datasets. We will also create a shiny app allowing users to download the organized data."
  },
  {
    "objectID": "posts/2024-08-01-tying-the-data-together/index.html#background-challenges",
    "href": "posts/2024-08-01-tying-the-data-together/index.html#background-challenges",
    "title": "Tying the data together",
    "section": "Background & challenges",
    "text": "Background & challenges\nRecently, we reviewed our operational costs and found that a significant portion was allocated to credit card processing fees. After thorough research and negotiation, we secured a better deal with a new merchant service provider, saving about $1,500 USD per month and qualifying for next-day funding—a major win for our business.\nHowever, our bookkeeper quickly identified an issue: the new provider’s reporting system is less straightforward. Unlike our previous provider, which offered a comprehensive master report with detailed order and funding information, the new service splits this data into three separate reports: Sales, Transaction, and Funded. To complicate matters, some data points, like order details, are labeled differently across the reports (e.g., order.ifo in Sales versus order_detail in Transaction). Additionally, a single order can have multiple funding references, making it challenging to track everything accurately.\n\n\n\nDifferent column names and multiple funded entries for one order"
  },
  {
    "objectID": "posts/2024-08-01-tying-the-data-together/index.html#actions",
    "href": "posts/2024-08-01-tying-the-data-together/index.html#actions",
    "title": "Tying the data together",
    "section": "Actions",
    "text": "Actions\nTo fully benefit from the cost savings and stick with the new provider, I decided to tackle this problem head-on using R and Shiny.\nOne of our goals was to create a master statement with all the action organized in one report as below. To do this, the steps are as below:\n\nWe first re-organized the raw report by renaming the column names.\nSkipped the data where it is empty or contains not useful information.\nData merge based on the same column names.\n\n\n\n\nDesired master report\n\n\n\n\n\n\n\n\nExpand for Code\n\n\n\n\n\n\nlibrary(shiny)\n\n# identify columns appear in both data sets\nrenderUI({\n    var_names &lt;- intersect(names(data_a()), names(data_b()))\n    selectInput(\"var_ab\", \"Select merge variable for Transaction and Sales\", choices = var_names)\n  })\n\n\n\n# merge data \n  merged_data &lt;- reactive({\n    req(data_a(), data_b(), data_c(), input$var_ab, input$var_bc)\n    temp_ab &lt;- dplyr::full_join(data_a(), data_b(), by = input$var_ab)\n    unique(dplyr::full_join(temp_ab, data_c(), by = input$var_bc))\n  })\n\n\n\n\nI also wanted to save time for our bookkeeper as they often need to cross-check the internal documents. This step takes time to find the correct range of the data, and input Vlookup functions before finding and labeling the matches in Excel. To streamline the process and to reduce the manual input errors, I replicated this function in the app (full code as below and on github).\nThe output result is shown below. This example shows that the order 891631 with two different retrieval references were received, the transactions were successful ,and the amount was deposited into the account. These records (114,115) also appeared in the uploaded internal file.\n\n\n\n\n\n\n\nExpand for Code\n\n\n\n\n\n\nreactive({\n    req(merged_data(),data_f())\n    dt_org &lt;- dplyr::full_join(merged_data(),data_f(), by = input$var_checklist)\n    dt_org &lt;- dt_org |&gt;\n        select(`Date and Time`,`Transaction Date`,`Transaction Count`,`Merchant Reference Number`,`Summary          Transaction Slip`,Retrieval_Ref,\n              `Transaction Gross`,`Transaction Net`,`Gross Amount`,`Net Amount`,Currency,`Reversal Flag`,\n               `Transaction Type`)\n    counts &lt;- dt_org |&gt;\n      filter(`Summary Transaction Slip` != \"\")|&gt;\n      group_by(`Summary Transaction Slip`) |&gt;\n      mutate(Check_merchantID = list(`Merchant Reference Number`), count = n())|&gt;\n      mutate(Check_Retrieval_Ref = list(Retrieval_Ref), countx = n())|&gt;\n      mutate(has_Summary_Transaction_Slip = \"Yes\")|&gt;\n      select(-count, -countx)\n\n    not_in_counts &lt;- dt_org |&gt;\n      filter(is.na(`Summary Transaction Slip`))|&gt;\n      mutate(Check_merchantID = list(0))|&gt;\n      mutate(Check_Retrieval_Ref = list(0))|&gt;\n      mutate(has_Summary_Transaction_Slip = \"No\")\n     counts_summary &lt;- apply(rbind(counts,not_in_counts),2,as.character)\n     unique(counts_summary)\n  })"
  },
  {
    "objectID": "posts/2024-08-01-tying-the-data-together/index.html#results",
    "href": "posts/2024-08-01-tying-the-data-together/index.html#results",
    "title": "Tying the data together",
    "section": "Results",
    "text": "Results\nAfter the data merge is done, the files are ready to download to the local drive by pressing the Download buttons. This customized shiny app allows our accounting staff to run and download the report anytime/anywhere.\n\nBy developing a custom solution, I was able to streamline the reporting process, merge the data consistently, and keep our operations running smoothly—all while cutting down on expenses.\nFull code on Github\n\n\n\n\n\n\nExpand for Full Code\n\n\n\n\n\n\nlibrary(shiny)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\nlibrary(readxl)\nlibrary(readr)\nlibrary(DT)\n\n\nAttaching package: 'DT'\n\n\nThe following objects are masked from 'package:shiny':\n\n    dataTableOutput, renderDataTable\n\nui &lt;- fluidPage(\n  titlePanel(\"File Merger\"),\n  sidebarLayout(\n    sidebarPanel(\n      p(\"1. Convert Retrieval Reference Number to General\"),\n      fileInput(\"file_a\", \"Transaction file (CSV or XLSX)\",\n                accept = c(\".csv\", \".xlsx\")),\n      p(\"2a. Convert Retrieval Ref to General\"),\n      p(\"2b. Remove all rows above Merchant ID \"),\n      fileInput(\"file_b\", \"Sales (CSV or XLSX)\",\n                accept = c(\".csv\", \".xlsx\")),\n      p(\"3. Remove all rows above Summary Transaction Slip\"),\n      p(\"   Output: Merged File; Auto-Checked File \"),\n      fileInput(\"file_c\", \"Funded Deposit (CSV or XLSX)\",\n                accept = c(\".csv\", \".xlsx\")),\n      p(\"------------------------------------------\"),\n      p(\"4. (Optional) Upload a payportal list. Output: Manual-Checked File\"),\n      p(\"Must contain a column called **Merchant Reference Number**!\"),\n      checkboxInput(\"checklist\", \"Manual Check\", value = FALSE),\n      fileInput(\"file\", \"Checklist (CSV or XLSX)\", accept = c(\".csv\", \".xlsx\")),\n      \n      uiOutput(\"merge_vars_a_b\"),\n      uiOutput(\"merge_vars_b_c\"),\n      uiOutput(\"merge_vars_checklist\"),\n      br(),\n      #p(\"------------------------------------------\"),\n      p(\"A: Contains Summary Transaction Slip & Retrival Ref (matched & unmatched)\"),\n      downloadButton(\"downloadData\", \"Download Merged File\"),\n      #p(\"------------------------------------------\"),\n      p(\"B: Contains non-empty Summary Transaction Slip, count matched Retrival Ref & Merchant ref\"),\n      downloadButton(\"exportData\",   \"Download Auto-Checkeds File\"),\n      #p(\"------------------------------------------\"),\n      p(\"C: Using uploaded checklist to match and count non-empty Summary Transaction Slip, Retrival Ref & Merchant ref\"),\n      downloadButton(\"manual_input\", \"Download Manual-Checked File\")\n    ),\n    mainPanel(\n      tabsetPanel(\n        tabPanel(\"Transaction List\", DT::dataTableOutput(\"table_a\")),\n        tabPanel(\"Sales List\", DT::dataTableOutput(\"table_b\")),\n        tabPanel(\"Funded Deposit List\", DT::dataTableOutput(\"table_c\")),\n        tabPanel(\"Merged File\", DT::dataTableOutput(\"table_d\")),\n        tabPanel(\"Auto-Checked File\", DT::dataTableOutput(\"table_e\")),\n        tabPanel(\"Checklist\", DT::dataTableOutput(\"table_f\")),\n        tabPanel(\"Manual-Checked File\", DT::dataTableOutput(\"table_g\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  \n  read_data &lt;- function(file) {\n    if (is.null(file)) {\n      return(NULL)\n    }\n    ext &lt;- tools::file_ext(file$name)\n    if (ext == \"csv\") {\n      df &lt;- read_csv(file$datapath)\n    } else if (ext == \"xlsx\") {\n      df &lt;- read_excel(file$datapath)\n    } else {\n      return(NULL)\n    }\n    return(df)\n  }\n  \n  data_a &lt;- reactive({\n    validate(\n      need(input$file_a != \"\", label = \"data set\")\n    )\n    \n    da &lt;- read_data(input$file_a)%&gt;%\n      mutate(Retrieval_Ref = `Retrieval Reference Number`)%&gt;% \n      select(`Date and Time`, Retrieval_Ref,`Merchant Reference Number`,Amount) \n    return(da)\n    \n  })\n  \n  # remove all rows above Merchant ID and change \n  data_b &lt;- reactive({\n    req(input$file_b)\n    db &lt;- read_data(input$file_b)%&gt;% \n      mutate(Retrieval_Ref = `Retrieval Ref.`)%&gt;% \n      select(`Transaction Date`, Retrieval_Ref, `Summary Transaction Slip`,\n             `Payment Transaction Slip`, `Transaction Gross`,`Transaction Net`)\n    db \n  })\n  \n  \n  # remove all rows above Summary Transaction Slip\n  data_c &lt;- reactive({\n    req(input$file_c)\n    dc &lt;- read_data(input$file_c)\n    dc&lt;- dc[!grepl('Payment', dc$`Summary Transaction Slip`)&\n              !grepl('Sale', dc$`Summary Transaction Slip`)&\n              !grepl('/', dc$`Summary Transaction Slip`)&\n              !grepl('Summary', dc$`Summary Transaction Slip`),]\n    dc$`Summary Transaction Slip` &lt;-  as.numeric(dc$`Summary Transaction Slip`)\n    \n    #colnames(dc)[2:7] &lt;-  c(\"Summary Transaction Slip\",\"Transaction Type\",\"Transaction Count\", \"Reversal Flag\",\n    #                   \"Currency\",\"Gross Amount\", \"Net Amount\")\n    return(dc) \n  })\n  \n  output$merge_vars_a_b &lt;- renderUI({\n    #req(data_a(), data_b())\n    validate(\n      need(input$file_a != \"\", label = \"data set\")\n    )\n    var_names &lt;- intersect(names(data_a()), names(data_b()))\n    selectInput(\"var_ab\", \"Select merge variable for Transaction and Sales\", choices = var_names)\n  })\n  \n  output$merge_vars_b_c &lt;- renderUI({\n    #req(data_b(), data_c())\n    validate(\n      need(input$file_a != \"\", label = \"data set\")\n    )\n    #get(input$file_a, 'package:datasets')\n    var_names &lt;- intersect(names(data_b()), names(data_c()))\n    selectInput(\"var_bc\", \"Select merge variable for Sales and Funded Depos it\", choices = var_names)\n  })\n  \n  \n  output$merge_vars_checklist &lt;- renderUI({\n    validate(\n      need(input$checklist != FALSE, label = \"data set\")\n    )\n    #get(input$file_a, 'package:datasets')\n    var_names &lt;- intersect(names(merged_data()), names(data_f()))\n    selectInput(\"var_checklist\", \"Select merge variable for Merge data and Checklist\", choices = var_names)\n  })\n  data_f &lt;- reactive({\n    validate(\n      need(input$file != \"\", label = \"data set\")\n    )\n    dt &lt;- read_data(input$file)\n    return(dt)\n  })\n  \n  output$table_a &lt;- DT::renderDataTable({\n    #req(data_a())\n    DT::datatable(data = unique(data_a()),\n                  options = list(pageLength = 100, rownames = FALSE) \n    )  \n  })\n  \n  output$table_b &lt;- DT::renderDataTable({\n    #req(data_b())\n    DT::datatable(data = unique(data_b()),\n                  options = list(pageLength = 100, rownames = FALSE) \n    )  \n  })\n  \n  output$table_c &lt;- DT::renderDataTable({\n    #req(data_c())\n    DT::datatable(data = unique(data_c()),\n                  options = list(pageLength = 100, rownames = FALSE) \n    )  \n  })\n  \n  merged_data &lt;- reactive({\n    req(data_a(), data_b(), data_c(), input$var_ab, input$var_bc)\n    temp_ab &lt;- dplyr::full_join(data_a(), data_b(), by = input$var_ab)\n    unique(dplyr::full_join(temp_ab, data_c(), by = input$var_bc))\n  })\n  \n  processed_data &lt;- reactive({\n    req(merged_data())\n    df &lt;-\n      merged_data() %&gt;%\n      filter(`Summary Transaction Slip` != \"\")%&gt;%\n      group_by(`Summary Transaction Slip`) %&gt;%\n      mutate(Check_merchantID = list(`Merchant Reference Number`), count = n())%&gt;%\n      mutate(Check_Retrieval_Ref = list(Retrieval_Ref), countx = n())%&gt;%\n      select(`Date and Time`,`Transaction Date`,`Transaction Count`,`Merchant Reference Number`,`Summary Transaction Slip`,Retrieval_Ref,\n             `Check_merchantID`,Check_Retrieval_Ref,`Transaction Gross`,`Transaction Net`,`Gross Amount`,`Net Amount`,Currency,`Reversal Flag`,\n             `Transaction Type`)\n    apply(df,2,as.character)\n    \n  })\n  \n  manual_check &lt;-reactive({\n    req(merged_data(),data_f())\n    dt_org &lt;- dplyr::full_join(merged_data(),data_f(), by = input$var_checklist)\n    dt_org &lt;- dt_org |&gt;\n        select(`Date and Time`,`Transaction Date`,`Transaction Count`,`Merchant Reference Number`,`Summary Transaction Slip`,Retrieval_Ref,\n              `Transaction Gross`,`Transaction Net`,`Gross Amount`,`Net Amount`,Currency,`Reversal Flag`,\n               `Transaction Type`)\n\n    counts &lt;- dt_org |&gt;\n      filter(`Summary Transaction Slip` != \"\")|&gt;\n      group_by(`Summary Transaction Slip`) |&gt;\n      mutate(Check_merchantID = list(`Merchant Reference Number`), count = n())|&gt;\n      mutate(Check_Retrieval_Ref = list(Retrieval_Ref), countx = n())|&gt;\n      mutate(has_Summary_Transaction_Slip = \"Yes\")|&gt;\n      select(-count, -countx)\n\n    not_in_counts &lt;- dt_org |&gt;\n      filter(is.na(`Summary Transaction Slip`))|&gt;\n      mutate(Check_merchantID = list(0))|&gt;\n      mutate(Check_Retrieval_Ref = list(0))|&gt;\n      mutate(has_Summary_Transaction_Slip = \"No\")\n\n     #counts_summary &lt;- rbind(counts,not_in_counts)\n     counts_summary &lt;- apply(rbind(counts,not_in_counts),2,as.character)#rbind(apply(counts,2,as.character),apply(not_in_counts,2,as.character))\n    \n    \n     unique(counts_summary)\n    \n  })\n  \n  output$table_d &lt;- DT::renderDataTable({\n    req(merged_data())\n    DT::datatable(data = unique(merged_data()),\n                  options = list(pageLength = 100, rownames = FALSE) \n    )  \n  })\n  \n  output$table_e &lt;- DT::renderDataTable({\n    req(merged_data())\n    DT::datatable(data = unique(processed_data()),\n                  options = list(pageLength = 100, rownames = FALSE) \n    )  \n  })\n  \n  # Render the data table only if the checkbox is checked\n  output$table_f &lt;- renderDT({\n    req(input$checklist)\n    #manual_check()\n    \n    DT::datatable(data = unique(data_f()),\n                  options = list(pageLength = 100, rownames = FALSE)\n    )\n  })\n  \n  output$table_g &lt;- renderDT({\n    req(input$checklist)\n    \n    DT::datatable(data = unique(manual_check()),\n                  options = list(pageLength = 100, rownames = FALSE)\n    )\n  })\n  \n  ######### Download ############\n  output$downloadData &lt;- downloadHandler(\n    filename = function() {\n      paste0(Sys.Date(), \"merged_data.csv\")\n    },\n    content = function(file) {\n      write.csv(unique(merged_data()), file, row.names = FALSE)\n    }\n  )\n  \n  \n  output$exportData &lt;- downloadHandler(\n    filename = function() {\n      paste0(Sys.Date(), \"auto_checked.csv\")\n    },\n    content = function(file) {\n      write.csv(unique(processed_data()), file, row.names = FALSE)\n    }\n  )\n  \n  output$manual_input &lt;- downloadHandler(\n    filename = function() {\n      paste0(Sys.Date(), \"manual_checked.csv\")\n    },\n    content = function(file) {\n      write.csv(unique( manual_check()), file, row.names = FALSE)\n    }\n  ) \n  \n}\nshinyApp(ui, server)\n\nPhantomJS not found. You can install it with webshot::install_phantomjs(). If it is installed, please make sure the phantomjs executable can be found via the PATH variable.\n\n\nShiny applications not supported in static R Markdown documents"
  },
  {
    "objectID": "posts/2025-01-15-forecasting-for-data-analysis/index.html#overview",
    "href": "posts/2025-01-15-forecasting-for-data-analysis/index.html#overview",
    "title": "Forecasting: exploratory data analaysis",
    "section": "",
    "text": "Why Forecasting Matters for Our Business\nPredicting customer demand is crucial for growing business. Without reliable forecasts, we risk stockouts or excess inventory, leading to wasted resources. This is especially important for our wedding and event catering service, where we work with perishable ingredients. By precisely estimating daily and weekly demand, we can optimize inventory, reduce waste, control costs, and consistently meet customer expectations.\nBefore diving into forecasting, we’ll start with data exploration. This includes analyzing variable distributions, handling missing values, and uncovering key insights. For this analysis, we’ll use a dataset from Kaggle. You can explore it here: Kaggle - Food Demand Forecasting.\nThe data contains information from 77 stores over a period of 145 weeks.\nTwo types of promotions (web, email) were shown in the data as below:\n- 10.61% stores ran one type of promotion.\n- 4.21% stores ran both types at the same time.\n- 85.18% stores didn’t run any promotions.\nOur initial analysis is aim to discover:\n(1) Did revenue or profit increase?\n(2) Which promotion worked best?\n(3) To predict future demand for our menu items.\n\n\nThe data contains no missing values.\n\n\nCode\n```{r}\n#| label: Overview data\n#| fig-cap: \"Indentify missing values\"\n#| code-fold: true\n#| fig-width: 8\n#| fig-height: 6\n\n# Check for missing values\nkable(colSums(is.na(train)))\n```\n\n\n\n\n\n\nx\n\n\n\n\nid\n0\n\n\nweek\n0\n\n\ncenter_id\n0\n\n\nmeal_id\n0\n\n\ncheckout_price\n0\n\n\nbase_price\n0\n\n\nemailer_for_promotion\n0\n\n\nhomepage_featured\n0\n\n\nnum_orders\n0\n\n\ncity_code\n0\n\n\nregion_code\n0\n\n\ncenter_type\n0\n\n\nop_area\n0\n\n\ncategory\n0\n\n\ncuisine\n0\n\n\npromotions\n0\n\n\nprofit\n0\n\n\npromotion_flag\n0\n\n\n\nIndentify missing values\n\n\n\n\nCenter Type: Three types, with ‘TYPE_A’ being the most frequent.\nMeal Category: Fourteen categories, ‘Beverages’ being the most frequent.\nCuisine: Four types, with ‘Italian’ being the most common.\n\n\nCode\n```{r}\n#| label: Overview categorical data\n#| fig-cap: \" Distribution of the categorical variables\"\n#| warning: false\n#| code-fold: true\n#| fig-width: 8\n#| fig-height: 6\n\nfreq_data &lt;- function(df){\n\n  df &lt;- as.data.frame(df)\n  results_list &lt;- vector(\"list\", ncol(df) - 1)\n  \n  # Loop through the columns starting from the 2nd column\n  for (i in 2:ncol(df)) {\n       # Store the result of cal_freq(i) in the list\n       results_list[[i - 1]] &lt;- cal_freqx(i, df)\n      }\n  df &lt;- do.call(rbind, results_list)\n\n  # label bar graph\n  df$vars &lt;- ifelse(df$Freqx &gt; 0.08, as.character(df$Var1), \"\")\n  return(df)\n}\n p &lt;-  freq_data(train |&gt; select(-promotion_flag) |&gt; mutate(store_id = as.factor(center_id)))\n\n p |&gt;\n       ggplot(aes(x = round(Freqx,2), y = fct_rev(namex), fill = Var1)) + \n       geom_col(position = \"fill\", color = \"white\") +\n       scale_x_continuous(labels = label_percent()) +\n       labs(title = \"Distribution of Categorical Variables:\",\n       subtitle = paste(\"Top to down :\" , unique(p$namex)[2],\",\" ,unique(p$namex)[1],\",\" , unique(p$namex)[3],\",\" , unique(p$namex)[4],\",\" , unique(p$namex)[5]),y = NULL, x = NULL, fill = \"var1\")+\n       guides(fill=\"none\") +\n       geom_text(aes(label = vars), position = position_stack(vjust = 0.5),\n                 color = \"white\")+\n      theme_void()\n```\n\n\n\n\n\nDistribution of the categorical variables\n\n\n\n\n\n\n\nID Variables: id, week, center_id, meal_id\nPricing Variables: checkout_price, base_price\nDemand Variable (Target): num_orders\nLocation and Size Variables: city_code, region_code, op_area\n\n\nCode\n```{r}\n#| label: data distribution\n#| fig-cap: \"\"\n#| code-fold: true\n#| fig-width: 8\n#| fig-height: 4\n\n# Plot the original distribution of num_orders\ns1 &lt;- \n  train|&gt;\n  ggplot(aes(x = (num_orders))) +\n  geom_histogram(bins = 50, fill = \"steelblue\", alpha = 0.7) +\n  geom_density(alpha = 0.5, color = \"#d88bb4\") +\n  ggtitle(\"Distribution of num_orders\") +\n  xlab(\"num_orders\") +\n  ylab(\"Frequency\")+\n  theme_minimal()\n\ns2 &lt;- \n  train|&gt;\n  ggplot(aes(x = (checkout_price))) +\n  geom_histogram(bins = 50, fill = \"steelblue\", alpha = 0.7) +\n  geom_density(alpha = 0.3, color = \"#d88bb4\", outline.type = \"upper\") +\n  ggtitle(\"Distribution of checkout_price\") +\n  xlab(\"checkout_price\") +\n  ylab(\"Frequency\")+\n  theme_minimal()\n\ns3 &lt;- \n  train|&gt;\n  ggplot(aes(x = (base_price))) +\n  geom_histogram(bins = 50, fill = \"steelblue\", alpha = 0.7) +\n  geom_density(alpha = 0.5, color = \"#d88bb4\") +\n  ggtitle(\"Distribution of base_price\") +\n  xlab(\"base_price\") +\n  ylab(\"Frequency\")+\n  theme_minimal()\n\ns4 &lt;- \n  train|&gt;\n  ggplot(aes(x = (op_area ))) +\n  geom_histogram(bins = 50, fill = \"steelblue\", alpha = 0.7) +\n  geom_density(alpha = 0.5, color = \"#d88bb4\") +\n  ggtitle(\"Distribution of base_price\") +\n  xlab(\"operation area\") +\n  ylab(\"Frequency\")+\n  theme_minimal()\n\n\n\n#kable(summary(train))\n#grid.arrange(s1, s2, s3, ncol = 3)\nplot_grid(s1, s2, s3, s4, labels=\"AUTO\")\n```\n\n\n\n\n\n\n\n\n\n\n\n\nComputes correlations to identify feature importance.\n\n\nCode\n```{r}\n#| label: time series engineering\n#| fig-cap: \"\"\n#| code-fold: true\n#| fig-width: 8\n#| fig-height: 6\n\ndf_filtered &lt;- train |&gt;\n  #filter(center_id %in% c(24:26))|&gt;\n  group_by(week,center_id,meal_id, promotion_flag) |&gt;\n  summarise(avg_order = mean(num_orders), .groups = 'drop')|&gt;\n  arrange(week) |&gt;\n  mutate(\n    num_orders_lag_1 = lag(avg_order, 1, default = 0),\n    num_orders_lag_2 = lag(avg_order, 2, default = 0),\n    num_orders_lag_4 = lag(avg_order, 4, default = 0),\n    num_orders_ma_2 = rollapply(avg_order, width = 2, FUN = mean, fill = NA, align = \"right\"),\n    num_orders_ma_4 = rollapply(avg_order, width = 4, FUN = mean, fill = NA, align = \"right\"),\n    week_sin = sin(2 * pi * week / 52),\n    week_cos = cos(2 * pi * week / 52)\n  )\n\n# ---------- Compute Correlation and Feature Importance -----\n cor_matrix_menu &lt;- cor(df_filtered |&gt; select( -promotion_flag), use = \"complete.obs\")\ncorrplot(cor_matrix_menu,  method = \"color\",col = COL2('PiYG', 100), addCoef.col = \"black\", tl.col = \"black\", type = 'lower', tl.srt = 45)\n```"
  },
  {
    "objectID": "posts/2025-01-15-forecasting-for-data-analysis/index.html#results",
    "href": "posts/2025-01-15-forecasting-for-data-analysis/index.html#results",
    "title": "Forecasting: exploratory data analaysis",
    "section": "Results",
    "text": "Results\nSince we are trying to forecast future demand, we can plot sales and profit of different promotions over time. To do this, we average the data across stores, as this can allow smoothing of the noise to see more of the signal.\n\n\nVisulization: Effect of Promotion types on Weekly Profit in each store\nAs one of our goals is to identify patterns and correlations between promotion types, individual store performance, and weekly profit trends. We first used a heatmap to help us visualize how well our stores are performing over time and how different promotions impact our profits. Each cell in the heatmap represents the average profit for a specific store (rows) during a specific week (columns). The color intensity (ranging from light to dark green) indicates the magnitude of average profit, with darker greens representing higher profits. The Color bar represents a profit from 10.0 to 20.0. The border of the cell represents the promotion type being carried out in the store for the corresponding week. Namely Promotion, Double promotion, Email promotion, No promotion, and Homepage promotion.\n\n\nWhile some stores (e.g., Store IDs around 40-50) consistently show higher profits (darker green), other stores (e.g., Store IDs around 140-150) show consistently lower profits (lighter green). This indicates that the strategies for these stores need a revisit.\n\nWeeks with promotions (identified by the border type) often correlate with higher profits (darker green cells). However, the effectiveness varies.\n\nPatterns in the heatmap suggest potential seasonal cycles, with profit fluctuations happening in waves.\n\n\n\nCode\n```{r}\n#| label: Heatmaps\n#| fig-cap: \" How Promotions Impact overall Profit?\"\n#| warning: false\n#| code-fold: true\n#| fig-width: 8\n#| fig-height: 6\n\n#library(pheatmap) ## for heatmap generation\n#library(ggplotify) ## to convert pheatmap to ggplot2\n#library(heatmaply) ## for constructing interactive heatmap\n\ntstx &lt;- train |&gt;\n    group_by(promotion_flag, center_id) |&gt;\n      dplyr::summarise(`Average Profit (log2)` = log(mean(profit),2))\n\ntestx &lt;- tstx |&gt;\n  tidyr::pivot_wider(names_from = center_id, values_from = `Average Profit (log2)`)|&gt;\n  mutate(across(everything(), ~replace_na(.x, 0)))\n\ntestx &lt;- data.frame(testx)\nrownames(testx) &lt;- testx$promotion_flag\n\nflag &lt;- data.frame(testx$promotion_flag)\nrownames(flag) &lt;- testx$promotion_flag\ncolnames(flag) &lt;- \"promotions\"\ncols &lt;- colorRampPalette(c( \"white\", \"lightgreen\", \"darkgreen\"))(100)  \npheatmap(testx[,-c(1)],cutree_cols=2, cutree_rows=2, color = cols)\n```\n\n\n\n\n\nHow Promotions Impact overall Profit?\n\n\n\n\n\n\nCode\n```{r}\n#| label: Overview profit with heatmap\n#| fig-cap: \"Overview profit with heatmap \"\n#| warning: false\n#| code-fold: true\n#| fig-width: 10\n#| fig-height: 10\n\n#--------------- Heatmap overall------\ntrain |&gt;\n    group_by(week, promotion_flag, center_id) |&gt;\n    dplyr::summarise(`Average Profit (log2)` = log(mean(profit),2))|&gt;\n  ggplot(aes(x = as.factor(week), y = as.factor(center_id), fill = `Average Profit (log2)`)) +\n  geom_tile(aes(color = promotion_flag), linewidth = 0.5) + # Color outline for promotion\n  scale_fill_gradient(low = \"ivory\", high = \"darkgreen\", name = \"Average profit\") +\nscale_color_manual(values = c(\"Double promotion\" = \"red\", \"Email promotion\" =\n            \"yellow\", \"Homepage promotion\" = \"orange\",\"No promotion\" = \"white\"),\n                    name = \"Promotion\") +\n  \n  labs(title = \" \", #Heatmap of Weekly Profit and Promotions per Store\n       x = \"Week\", y = \"Store ID\") +\n  theme_minimal() +\n    theme(legend.position=\"bottom\",\n          axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n```\n\n\n\n\n\nOverview profit with heatmap\n\n\n\n\nSince we are trying to forecast future demand, we can plot sales and profit of different promotions over time. To do this, we average the data across stores, as this can allow smoothing of the noise to see more of the signal.\n\n\nVisulization: Average Sales and Revenue Over Time\nThe blue line (No Promotions) is consistently lower and less volatile compared to other lines (Homepage promotion, Email promotion, Double promotion) in figure 1A. This suggests that promotions drive short-term spikes in sales, but the effect is not stable over time. There are frequent peaks and drops, indicating that sales surge when promotions are active but return to normal levels afterward. The profit trend (Figure 1B) mirrors the sales trend, confirming that higher sales during promotions contribute to revenue increases.However, the fluctuations in revenue suggest that while sales increase, revenue might not be growing at the same rate, possibly due to discounting.\nNote: The non-promoted sales and revenue (blue line) remain relatively stable over time. The promoted periods exhibit more volatility, meaning promotions may be shifting demand rather than creating long-term sales growth.\n\n\nCode\n```{r}\n#| label: Time series plots of sales volume and revenue\n#| fig-cap: \" \"\n#| warning: false\n#| code-fold: true\n#| fig-width: 8\n#| fig-height: 6\n\n# ---------------------------\n# 1. Time Series Plot of Sales Volume & Revenue\n# ---------------------------\nsales_trend &lt;- train |&gt;\n  group_by(week, promotion_flag) |&gt;\n  dplyr::summarise(average_sales = log(mean(num_orders),2), \n            average_profit = log(mean(profit),2),\n            .groups = 'drop')\n\n# Plot Sales Volume Over Time\n\n a1 &lt;-\n   ggplot(sales_trend, aes(x = week, y = average_sales, color = as.factor(promotion_flag))) +\n  geom_line(size = 1) +\n  labs(title = \"1A) Average Sales Volume Over Time\", x = \"\", y = \"Average Sales (log2)\", color = \"Promotion\") +\n  theme_minimal()+\n   theme(legend.title=element_blank(),\n         legend.position=\"bottom\",\n    panel.background = element_blank())+\n    theme(axis.line = element_line(color = 'black'))+\n    guides(color=\"none\")\n\n# Plot Revenue Over Time\n a2 &lt;-\n  ggplot(sales_trend, aes(x = week, y = average_profit, color = as.factor(promotion_flag))) +\n  geom_line(size = 0.8) +\n  labs(title = \"1B) Profit Over Time\", x = \"Week\", y = \"Average Profit (log2)\", color = \"Promotion\") +\n  theme_minimal()+\n   theme(legend.title=element_blank(),\n         legend.position=\"bottom\",\n    panel.background = element_blank())+\n    theme(axis.line = element_line(color = 'black'))\n \n grid.arrange(a1, a2, nrow = 2)\n```\n\n\n\n\n\n\n\n\n\nPromotion Effectiveness: Average Weekly Sales & Revenue by Promotion Type\nSales Volume\nFigure 2A shows how many orders were placed under different types of promotions.The y-axis is in a log scale (log2), meaning the values grow exponentially. The median order volume is highest for double promotions (both email and homepage), followed by homepage promotions, no promotions, and email promotions. The spread (height of the box) indicates variability, with double promotions showing more fluctuation in sales. The presence of numerous outliers at the upper end indicates that some stores experience exceptionally high sales under promotional influence.\nCheckout Price\nFigure 2B shows the range of prices customers paid at checkout under different promotions. Unlike the sales volume, there is no major difference in checkout price across promotions. There is some right-skewness with outliers, suggesting a small number of high-value purchases, but the overall trend remains stable. This suggests that promotions influence the number of items bought, but not the price per visit.\nProfit\nRevenue is also log-transformed for better visualization as shown in figure 2C. Similar to the sales volume trend, double promotions generate the highest median revenue, followed by homepage promotions, no promotions, and email promotions. The spread of revenue is wider, meaning revenue varies significantly under all conditions. The presence of high-value outliers suggests that under certain conditions, promotions can lead to exceptionally high revenue generation.\n\n\nCode\n```{r}\n#| label: Boxplot of sales volume, checkout price and revenue\n#| fig-cap: \" \"\n#| warning: false\n#| code-fold: true\n#| fig-width: 8\n#| fig-height: 4\n\n# ---------------------------\n# Boxplots of Sales with vs. without Promotion\n# ---------------------------\na &lt;- ggplot(train, aes(x = promotion_flag, y = log(num_orders,2), fill = promotion_flag)) +\n  geom_boxplot() +\n  labs(title = \"2A) Sales Distribution\", x = \"\", y = \"Order Volume (log2)\") +\n  theme_minimal()+\n    theme(\n      legend.position=\"bottom\",\n      plot.title = element_text(size=11),\n      axis.text.x = element_text(angle = 45, vjust = 0.5, hjust=0.2))+\n  guides(fill=\"none\")\n\nb&lt;- ggplot(train, aes(x = promotion_flag, y = checkout_price,  fill = promotion_flag)) +\n  geom_boxplot() +\n  labs(title = \"2B) Checkout Price Distribution\", x = \"Promotion\", y = \"Checkout Price\") +\n  theme_minimal()+\n    theme(\n      legend.position=\"bottom\",\n      plot.title = element_text(size=11),\n      axis.text.x = element_text(angle = 45, vjust = 0.5, hjust=0.2))+\n  guides(fill=\"none\")\n\nc&lt;- ggplot(train, aes(x = promotion_flag, y = log((profit),2), fill = promotion_flag)) +\n  geom_boxplot() +\n  labs(title = \"2C) Profit Distribution\", x = \"\", y = \"Revenue (log2)\") +\n  theme_minimal()+\n    theme(\n      legend.position=\"bottom\",\n      plot.title = element_text(size=11),\n      axis.text.x = element_text(angle = 45, vjust = 0.5, hjust=0.2))+\n  guides(fill=\"none\")\n\ngrid.arrange(a,b,c, ncol = 3)\n```\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Meal Popularity During Promotions and No-Promotions\nIn figure 3A, the Meal IDs are arranged from left to right in descending order based on their overall popularity (total percentage of orders). This visualization helps identify which meals are popular and how promotions influence demand.\nImpact of Promotions:\nSeveral meals (especially on the left side of the plot) show a significant increase in order percentage when a promotion is applied. This indicates that promotions can be effective in boosting the popularity of certain meals.For some meals, the Double promotion (email+homepage) or Email promotion seems to have a noticeable impact compared to “No promotion”.\nHomepage Promotion Baseline:\nHomepage promotion seems to be always present at some level for all meals, which suggests that homepage promotion might be a baseline strategy applied to all meals.\nMeal-Specific Promotion Effectiveness:\nSome meals respond well to specific types of promotions. For example, Meal ID 1962 sees a significant portion of its orders come from “promotion_flag”. Other meals, like Meal ID 1558, benefits more from “Email promotion”.\nMeals with Low Promotion Impact:\nOn the right side of the plot, the meals have low percentages overall, and the impact of promotions appears to be less significant. This could mean these meals are less popular regardless of promotions, or that the current promotions are not effective for these meals.\n\n\nCode\n```{r}\n#| label: del Promotions Impact popular items\n#| fig-cap: \" How Promotions Impact checkout price?\"\n#| warning: false\n#| code-fold: true\n#| fig-width: 8\n#| fig-height: 8\n\nc1 &lt;- \ntrain |&gt;\n  group_by(promotion_flag)|&gt;\n  dplyr::count(meal_id) |&gt;\n  mutate(p = round(100*(n / sum(n)),2))|&gt;\n  select(-n)|&gt;\n  ggplot(aes(x = reorder(factor(meal_id),p,, decreasing = TRUE),  fill = promotion_flag)) +\n  geom_bar(aes(y = p), stat='identity', alpha=0.7) +\n  scale_color_manual(values=c('Order Percent'=\"#d88bb4\")) +\n  labs(x = '', y = 'Percentage', \n       title = '3A) Popular Meals During Promotions and No Promotions') +\n  theme_minimal()+\n  theme(legend.position=\"bottom\",\n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=0.5))+\n  guides(fill=\"none\")\nc2 &lt;- \ntrain|&gt;\n  #filter(meal_id %in% radom_meal)|&gt;\n  group_by(meal_id, promotion_flag) |&gt;\n  summarise(avg_profit = mean(profit), .groups = 'drop')|&gt;\n  ggplot(aes(x = as.factor(meal_id), y = log(avg_profit,2), fill = as.factor(promotion_flag))) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"3B) Average Profit per Menu Item (With & Without Promotion)\", x = \"Menu ID\", y = \"Avg Profit (log2)\", fill = \"Promotion\") +\n  theme_minimal()+\n  #guides(fill=\"none\") +\n    theme(legend.position=\"bottom\",\n        axis.text.x = element_text(angle = 90, vjust = 0.2, hjust=0.1))\n\ngrid.arrange(c1, c2, nrow = 2)\n```\n\n\n\n\n\nHow Promotions Impact checkout price?\n\n\n\n\n\nAnalysis of Meal Popularity During Promotions and No-Promotions\nThe average profit per menu item (on the log2 scale) generally falls between approximately 12 and 16 in figure 3B. There’s noticeable variation in average profit across different menu items (identified by their Menu ID). Indeed, we compare the mean profits for No promotion vs. promotins (homepage, email, and homepage+email), the p-value is less than 0.05 from ANOVA test results show promotions substantially impact profits across menu items.\nWe also see some items are inherently more profitable than others, regardless of the promotion strategy. Certain menu items (e.g., 1248, 1445, 1558) show higher profits under double (homepage and email) promotion, suggesting targeted discounts may work well for these meals. This may indicate that these meals experience higher order volume during promotions, making up for the reduced margin. Since, this isn’t universally true for all menu items. Promotions should be customized per meal item—apply them only where they result in a net positive profit effect.\n\n\nCode\n```{r}\n#| label: Promotions Impact checkout price\n#| fig-cap: \" How Promotions Impact checkout price?\"\n#| warning: false\n#| code-fold: true\n#| fig-width: 8\n#| fig-height: 6\n\n# Compute average checkout_price and number of orders before and during promotions for each meal\n\ncheckout_price_comparison &lt;- train|&gt;\n  group_by(meal_id, promotions) |&gt;\n  summarise(avg_checkout_price = mean(checkout_price, na.rm = TRUE)) |&gt;\n  spread(promotions, avg_checkout_price)|&gt;\n  mutate(Price_Change_Percent = 100*(Yes - No)/No)\n\ncomparison_df_order &lt;- train |&gt;\n  group_by(meal_id, promotions) |&gt;\n  summarise(avg_num_orders = mean(num_orders, na.rm = TRUE))|&gt;\n  spread(key = promotions, value = avg_num_orders)|&gt;\n  mutate(Order_Change_Percent = 100*(Yes - No)/No)\n\ncomparison_df &lt;- left_join(comparison_df_order|&gt; select(meal_id, Order_Change_Percent),\n                           checkout_price_comparison|&gt; select(meal_id, Price_Change_Percent),\n                           by = \"meal_id\") \n# Check correlation between Price_Change_Percent vs Order_Change_Percent\ncorrelation_po &lt;- round(cor(comparison_df$Order_Change_Percent , \n                            comparison_df$Price_Change_Percent , use = \"complete.obs\"),2)\n\n\n# Plot the results\nggplot(comparison_df, aes(x = reorder(factor(meal_id),round(Price_Change_Percent,2)))) +\n  geom_bar(aes(y = Price_Change_Percent), stat='identity', fill='steelblue', alpha=0.7) +\n  geom_line(aes(y = 0.1*(Order_Change_Percent+4), group=1, color='Order Change Percent'), size=0.8) +\n  geom_point(aes(y = 0.1*(Order_Change_Percent+4), color='darkgray'),shape=21, size=2) +\n  scale_color_manual(values=c('Order Change Percent'=\"#d88bb4\")) +\n  labs(x = 'Meal ID', y = 'Percentage Change', \n       title = '4) Checkout Price Change & Order Volume Change During Promotions',\n       subtitle = paste(\"Correlation : Price Change vs. Order Volumne :\" , correlation_po )) +\n  theme_minimal()+\n  theme(legend.position=\"bottom\",\n        axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))\n```\n\n\n\n\n\nHow Promotions Impact checkout price?\n\n\n\n\n\n\n\nKeytakeaways\nAs shown in the heatmap, we know not all stores respond promotions, and some can maintain high profitability without them. We should consider store-specific promotion strategies instead of universal discounting. While double promotions drive the highest sales and revenue, homepage and email promotions individually are not as effective as using both together.\nThe data shows checkout price per transaction remains stable regardless of the promotion type, meaning the increase in revenue comes from selling more, not from charging more. We can identify stores where promotions significantly boost profits and focus campaigns on those locations. For low-response stores, explore alternative marketing (e.g., bundling, loyalty programs)."
  },
  {
    "objectID": "posts/2025-01-15-forecasting-for-data-analysis/index.html",
    "href": "posts/2025-01-15-forecasting-for-data-analysis/index.html",
    "title": "Forecasting: exploratory data analaysis",
    "section": "",
    "text": "Why Forecasting Matters for Our Business\nPredicting customer demand is crucial for growing business. Without reliable forecasts, we risk stockouts or excess inventory, leading to wasted resources. This is especially important for our wedding and event catering service, where we work with perishable ingredients. By precisely estimating daily and weekly demand, we can optimize inventory, reduce waste, control costs, and consistently meet customer expectations.\nBefore diving into forecasting, we’ll start with data exploration. This includes analyzing variable distributions, handling missing values, and uncovering key insights. For this analysis, we’ll use a dataset from Kaggle. You can explore it here: Kaggle - Food Demand Forecasting.\nThe data contains information from 77 stores over a period of 145 weeks.\nTwo types of promotions (web, email) were shown in the data as below:\n- 10.61% stores ran one type of promotion.\n- 4.21% stores ran both types at the same time.\n- 85.18% stores didn’t run any promotions.\nOur initial analysis is aim to discover:\n(1) Did revenue or profit increase?\n(2) Which promotion worked best?\n(3) To predict future demand for our menu items.\n\n\nThe data contains no missing values.\n\n\nCode\n```{r}\n#| label: Overview data\n#| fig-cap: \"Indentify missing values\"\n#| code-fold: true\n#| fig-width: 8\n#| fig-height: 6\n\n# Check for missing values\nkable(colSums(is.na(train)))\n```\n\n\n\n\n\n\nx\n\n\n\n\nid\n0\n\n\nweek\n0\n\n\ncenter_id\n0\n\n\nmeal_id\n0\n\n\ncheckout_price\n0\n\n\nbase_price\n0\n\n\nemailer_for_promotion\n0\n\n\nhomepage_featured\n0\n\n\nnum_orders\n0\n\n\ncity_code\n0\n\n\nregion_code\n0\n\n\ncenter_type\n0\n\n\nop_area\n0\n\n\ncategory\n0\n\n\ncuisine\n0\n\n\npromotions\n0\n\n\nprofit\n0\n\n\npromotion_flag\n0\n\n\n\nIndentify missing values\n\n\n\n\nCenter Type: Three types, with ‘TYPE_A’ being the most frequent.\nMeal Category: Fourteen categories, ‘Beverages’ being the most frequent.\nCuisine: Four types, with ‘Italian’ being the most common.\n\n\nCode\n```{r}\n#| label: Overview categorical data\n#| fig-cap: \" Distribution of the categorical variables\"\n#| warning: false\n#| code-fold: true\n#| fig-width: 8\n#| fig-height: 6\n\nfreq_data &lt;- function(df){\n\n  df &lt;- as.data.frame(df)\n  results_list &lt;- vector(\"list\", ncol(df) - 1)\n  \n  # Loop through the columns starting from the 2nd column\n  for (i in 2:ncol(df)) {\n       # Store the result of cal_freq(i) in the list\n       results_list[[i - 1]] &lt;- cal_freqx(i, df)\n      }\n  df &lt;- do.call(rbind, results_list)\n\n  # label bar graph\n  df$vars &lt;- ifelse(df$Freqx &gt; 0.08, as.character(df$Var1), \"\")\n  return(df)\n}\n p &lt;-  freq_data(train |&gt; select(-promotion_flag) |&gt; mutate(store_id = as.factor(center_id)))\n\n p |&gt;\n       ggplot(aes(x = round(Freqx,2), y = fct_rev(namex), fill = Var1)) + \n       geom_col(position = \"fill\", color = \"white\") +\n       scale_x_continuous(labels = label_percent()) +\n       labs(title = \"Distribution of Categorical Variables:\",\n       subtitle = paste(\"Top to down :\" , unique(p$namex)[2],\",\" ,unique(p$namex)[1],\",\" , unique(p$namex)[3],\",\" , unique(p$namex)[4],\",\" , unique(p$namex)[5]),y = NULL, x = NULL, fill = \"var1\")+\n       guides(fill=\"none\") +\n       geom_text(aes(label = vars), position = position_stack(vjust = 0.5),\n                 color = \"white\")+\n      theme_void()\n```\n\n\n\n\n\nDistribution of the categorical variables\n\n\n\n\n\n\n\nID Variables: id, week, center_id, meal_id\nPricing Variables: checkout_price, base_price\nDemand Variable (Target): num_orders\nLocation and Size Variables: city_code, region_code, op_area\n\n\nCode\n```{r}\n#| label: data distribution\n#| fig-cap: \"\"\n#| code-fold: true\n#| fig-width: 8\n#| fig-height: 4\n\n# Plot the original distribution of num_orders\ns1 &lt;- \n  train|&gt;\n  ggplot(aes(x = (num_orders))) +\n  geom_histogram(bins = 50, fill = \"steelblue\", alpha = 0.7) +\n  geom_density(alpha = 0.5, color = \"#d88bb4\") +\n  ggtitle(\"Distribution of num_orders\") +\n  xlab(\"num_orders\") +\n  ylab(\"Frequency\")+\n  theme_minimal()\n\ns2 &lt;- \n  train|&gt;\n  ggplot(aes(x = (checkout_price))) +\n  geom_histogram(bins = 50, fill = \"steelblue\", alpha = 0.7) +\n  geom_density(alpha = 0.3, color = \"#d88bb4\", outline.type = \"upper\") +\n  ggtitle(\"Distribution of checkout_price\") +\n  xlab(\"checkout_price\") +\n  ylab(\"Frequency\")+\n  theme_minimal()\n\ns3 &lt;- \n  train|&gt;\n  ggplot(aes(x = (base_price))) +\n  geom_histogram(bins = 50, fill = \"steelblue\", alpha = 0.7) +\n  geom_density(alpha = 0.5, color = \"#d88bb4\") +\n  ggtitle(\"Distribution of base_price\") +\n  xlab(\"base_price\") +\n  ylab(\"Frequency\")+\n  theme_minimal()\n\ns4 &lt;- \n  train|&gt;\n  ggplot(aes(x = (op_area ))) +\n  geom_histogram(bins = 50, fill = \"steelblue\", alpha = 0.7) +\n  geom_density(alpha = 0.5, color = \"#d88bb4\") +\n  ggtitle(\"Distribution of base_price\") +\n  xlab(\"operation area\") +\n  ylab(\"Frequency\")+\n  theme_minimal()\n\n\n\n#kable(summary(train))\n#grid.arrange(s1, s2, s3, ncol = 3)\nplot_grid(s1, s2, s3, s4, labels=\"AUTO\")\n```\n\n\n\n\n\n\n\n\n\n\n\n\nComputes correlations to identify feature importance.\n\n\nCode\n```{r}\n#| label: time series engineering\n#| fig-cap: \"\"\n#| code-fold: true\n#| fig-width: 8\n#| fig-height: 6\n\ndf_filtered &lt;- train |&gt;\n  #filter(center_id %in% c(24:26))|&gt;\n  group_by(week,center_id,meal_id, promotion_flag) |&gt;\n  summarise(avg_order = mean(num_orders), .groups = 'drop')|&gt;\n  arrange(week) |&gt;\n  mutate(\n    num_orders_lag_1 = lag(avg_order, 1, default = 0),\n    num_orders_lag_2 = lag(avg_order, 2, default = 0),\n    num_orders_lag_4 = lag(avg_order, 4, default = 0),\n    num_orders_ma_2 = rollapply(avg_order, width = 2, FUN = mean, fill = NA, align = \"right\"),\n    num_orders_ma_4 = rollapply(avg_order, width = 4, FUN = mean, fill = NA, align = \"right\"),\n    week_sin = sin(2 * pi * week / 52),\n    week_cos = cos(2 * pi * week / 52)\n  )\n\n# ---------- Compute Correlation and Feature Importance -----\n cor_matrix_menu &lt;- cor(df_filtered |&gt; select( -promotion_flag), use = \"complete.obs\")\ncorrplot(cor_matrix_menu,  method = \"color\",col = COL2('PiYG', 100), addCoef.col = \"black\", tl.col = \"black\", type = 'lower', tl.srt = 45)\n```"
  },
  {
    "objectID": "posts/2024-07-15-marketing-compaign-dashboard/index.html",
    "href": "posts/2024-07-15-marketing-compaign-dashboard/index.html",
    "title": "Marketing Campaign at a Glance",
    "section": "",
    "text": "rm(list=ls())\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n✔ readr     2.1.5     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(gt)\nlibrary(reshape)\n\n\nAttaching package: 'reshape'\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, smiths\n\nThe following object is masked from 'package:dplyr':\n\n    rename\n\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nlibrary(DT)\nlibrary(echarts4r)\n\nWarning: package 'echarts4r' was built under R version 4.4.1\n\nlibrary(echarts4r.maps)\nlibrary(gridExtra)  \n\nWarning: package 'gridExtra' was built under R version 4.4.1\n\n\n\nAttaching package: 'gridExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nlibrary(data.table)\n\n\nAttaching package: 'data.table'\n\nThe following object is masked from 'package:reshape':\n\n    melt\n\nThe following objects are masked from 'package:lubridate':\n\n    hour, isoweek, mday, minute, month, quarter, second, wday, week,\n    yday, year\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\nsource(\"~/data/helper.R\")\ntrain &lt;- read.csv(\"~/data/train.csv\")\nstate &lt;- read.csv(\"~/data/states.csv\")\n\n#| label: prep-data\n\ntrain   &lt;- train |&gt;\n    mutate(\n    success = case_when(\n      conversion_status == \"converted\"  ~ 1,\n      conversion_status == \"not_converted\"  ~ 0\n    ),\n    request_update = case_when(\n      update_me == \"Y\"  ~ 1,\n      update_me == \"N\"  ~ 0\n    ))\n# converted rate on average\navg_converted_rate &lt;- train |&gt;\n  #group_by(geoid)|&gt;\n  count(conversion_status) |&gt;\n  mutate(p = round(100*(n / sum(n)),2))|&gt;\n  filter(conversion_status == \"converted\")|&gt;\n  pull(p)\n\n# average converted rate based on communication channel\ncomm_converted_rate &lt;- train |&gt;\n  group_by(communication_channel)|&gt;\n  count(conversion_status) |&gt;\n  mutate(p = round(100*(n / sum(n)),2))|&gt;\n  filter(conversion_status == \"converted\")\n\n# state-wise average converted rate \nstate_converted_rate &lt;- train |&gt;\n  group_by(geoid)|&gt;\n  count(conversion_status) |&gt;\n  mutate(p = round(100*(n / sum(n)),2))|&gt;\n  filter(conversion_status == \"converted\")\n\n# quarter average converted rate \nquarter_converted_rate &lt;- train |&gt;\n  group_by(quater_res)|&gt;\n  count(conversion_status) |&gt;\n  mutate(p = round(100*(n / sum(n)),2))|&gt;\n  filter(conversion_status == \"converted\") \n\nbest_comm_pct &lt;- max(comm_converted_rate$p)\nbest_comm_way &lt;- comm_converted_rate|&gt;\n             filter(p == best_comm_pct)|&gt;\n             pull(communication_channel)\n\nbest_state_pct &lt;- max(state_converted_rate$p)\nbest_state &lt;- state_converted_rate |&gt;\n             filter(p == best_state_pct)|&gt;\n             pull(geoid)\nbest_q_pct &lt;- max(quarter_converted_rate$p)\nbest_q &lt;- quarter_converted_rate |&gt;\n             filter(p == best_q_pct )|&gt;\n             pull(quater_res)"
  },
  {
    "objectID": "posts/2024-07-15-marketing-compaign-dashboard/index.html#column",
    "href": "posts/2024-07-15-marketing-compaign-dashboard/index.html#column",
    "title": "Marketing Campaign at a Glance",
    "section": "Column",
    "text": "Column\n\ncolnames(state)[2]= \"geoid\"\nst &lt;- merge(train, state, by= \"geoid\")\n\n  st |&gt;  \n  group_by(State,quater_res)|&gt;\n  count(conversion_status) |&gt;\n  mutate(p = 100*(n / sum(n))) |&gt;\n  filter(conversion_status == \"converted\")|&gt;\n  select(State, quater_res, p)|&gt;\n  #pivot_wider(names_from = quater_res, values_from = p)|&gt;\n  #tidyr::gather(\"Key\",  \"Value\", q1, q2, q4) |&gt; \n  group_by(quater_res)|&gt;\n  e_chart(State, timeline = TRUE) |&gt;\n  em_map(\"USA\") |&gt; \n  e_map(p, map = \"USA\") |&gt;\n  e_visual_map(min= 0, max= 30, color = c(\"darkred\",\"red\",\"orange\",\"yellow\",\"lightgreen\",\"steelblue\",\"darkblue\"))|&gt;\n  e_timeline_opts(autoPlay = TRUE) \n\n\n\n\n\n\nstate_converted_rate[,c(1,3:4)] |&gt;\n  datatable(\n    colnames = c(\"State\",\"Converted #\", \"Converted %\"),\n    options = list(dom = 'ftp', paging = TRUE)\n    )"
  },
  {
    "objectID": "posts/2024-07-15-marketing-compaign-dashboard/index.html#column-1",
    "href": "posts/2024-07-15-marketing-compaign-dashboard/index.html#column-1",
    "title": "Marketing Campaign at a Glance",
    "section": "Column",
    "text": "Column\n\nresults_list &lt;- vector(\"list\", ncol(train) - 1)\n# Loop through the columns starting from the 2nd column\nfor (i in 2:ncol(train)) {\n  # Store the result of cal_freq(i) in the list\n  results_list[[i - 1]] &lt;- cal_freq(i)\n}\ndt &lt;- do.call(rbind, results_list)\n\n# label bar graph\ndt$vars &lt;- ifelse(dt$Freqx &gt; 0.15, as.character(dt$Var1), \"\")\n\na &lt;- train |&gt;\n  group_by(previous_campaign_outcome,quater_res)|&gt;\n  count(conversion_status) |&gt;\n  mutate(p = round(100*(n / sum(n)),2))|&gt;\n  filter(conversion_status == \"converted\")|&gt;\n  ggplot(aes(x = quater_res, y = p, group = previous_campaign_outcome, color\n             =previous_campaign_outcome )) +\n  geom_line()+\n  geom_point()+\n  ggtitle(\"Previous Campaign vs Conversion rate\") +\n  xlab(NULL) + \n  ylab(NULL)+ \n  theme(legend.title=element_blank(),\n    panel.background = element_blank())+\n    theme(axis.line = element_line(color = 'black'))\n  \nb &lt;- train |&gt;\n  group_by(communication_channel,quater_res)|&gt;\n  count(conversion_status) |&gt;\n  mutate(p = round(100*(n / sum(n)),2))|&gt;\n  filter(conversion_status == \"converted\")|&gt;\n  ggplot(aes(x = quater_res, y = p, group = communication_channel, color\n             =communication_channel )) +\n  geom_line()+\n  geom_point()+\n  ggtitle(\"Communication channel vs Conversion rate\") +\n  xlab(NULL) + \n  ylab(NULL)+ \n  theme(legend.title=element_blank(),\n    panel.background = element_blank())+\n    theme(axis.line = element_line(color = 'black'))\n\nc &lt;- train |&gt;\n  group_by(update_me, quater_res)|&gt;\n  count(conversion_status) |&gt;\n  mutate(p = round(100*(n / sum(n)),2))|&gt;\n  filter(conversion_status == \"converted\")|&gt;\n  ggplot(aes(x = quater_res, y = p, group = update_me, color =update_me )) +\n  geom_line()+\n  geom_point()+\n  ggtitle(\"Request update vs Conversion rate\") +\n  xlab(NULL) + \n  ylab(NULL)+ \n  theme(legend.title=element_blank(),\n    panel.background = element_blank())+\n    theme(axis.line = element_line(color = 'black'))  \n\nd &lt;- train |&gt;\n  group_by(quater_res)|&gt;\n  count(conversion_status) |&gt;\n  mutate(p = round(100*(n / sum(n)),2))|&gt;\n  filter(conversion_status == \"converted\")|&gt;\n  ggplot(aes(x = quater_res, y = p , group = 1)) +\n  geom_line(color = \"steelblue\",linewidth = 1)+\n  geom_point(color = 5)+\n  coord_cartesian(ylim = c(10, 14))+\n  ggtitle(\"23'Q2-24'Q2 Overall Conversion rate\") +\n  xlab(NULL) + \n  ylab(NULL)+ \n  theme(legend.title=element_blank(),\n    panel.background = element_blank())+\n    theme(axis.line = element_line(color = 'black'))\n\ngrid.arrange(d, a, b, nrow = 3) \n\n\n\n\n\n\n\n\n\ndt &lt;- train |&gt;\n    select(\"success\", \"call_duration\", \"call_frequency\",\n           \"request_update\",\"call_frequency\",\"len_business\" )\n\ncormat &lt;- round(cor(dt),2)\ncormat[lower.tri(cormat)]&lt;- NA\ncormat &lt;-  reshape2::melt(cormat)\ncolnames(cormat) &lt;- c(\"X1\",\"X2\",\"value\")\n\na &lt;-\n  cormat|&gt;\n  ggplot( aes(x=X2, y=X1, fill=value)) + \n  geom_tile()+\n  geom_text(aes(X2, X1, label = value), color = \"white\", size = 4)+\n  theme_minimal() +\n  theme(axis.line = element_line(color = 'black'))+\n  labs(y = NULL, x = NULL)\n  \nb &lt;- category_cor(data_= train, var_ =\"communication_channel\", \n                  target_ = \"success\" )\n\nWarning: The melt generic in data.table has been passed a matrix and will\nattempt to redirect to the relevant reshape2 method; please note that reshape2\nis superseded and is no longer actively developed, and this redirection is now\ndeprecated. To continue using melt methods from reshape2 while both libraries\nare attached, e.g. melt.list, you can prepend the namespace, i.e.\nreshape2::melt(cormat). In the next version, this warning will become an error.\n\ngrid.arrange( a, b,  nrow = 2)   \n\nWarning: Removed 10 rows containing missing values or values outside the scale range\n(`geom_text()`).\n\n\nWarning: Removed 6 rows containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\n\n\n\n\n\n\nresults_list &lt;- vector(\"list\", ncol(train) - 1)\n# Loop through the columns starting from the 2nd column\nfor (i in 2:ncol(train)) {\n  # Store the result of cal_freq(i) in the list\n  results_list[[i - 1]] &lt;- cal_freq(i)\n}\ndt &lt;- do.call(rbind, results_list)\n\n# label bar graph\ndt$vars &lt;- ifelse(dt$Freqx &gt; 0.15, as.character(dt$Var1), \"\")\n\ndt |&gt;\nggplot(aes(x = round(Freqx,2), y = fct_rev(namex), fill = Var1)) +\n  geom_col(position = \"fill\", color = \"white\") +\n  scale_x_continuous(labels = label_percent()) +\n  labs(y = NULL, x = NULL, fill = \"var1\",caption=\"Produced by C.Lee\")+\n  guides(fill=\"none\") +\n  geom_text(aes(label = vars), position = position_stack(vjust =   0.5), \n            color = \"white\")+\n  theme_minimal()"
  }
]