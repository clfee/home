[
  {
    "objectID": "projects.html#project-2",
    "href": "projects.html#project-2",
    "title": "Projects",
    "section": "Project 2",
    "text": "Project 2"
  },
  {
    "objectID": "projects.html#project-3",
    "href": "projects.html#project-3",
    "title": "Projects",
    "section": "Project 3",
    "text": "Project 3"
  },
  {
    "objectID": "gallery.html",
    "href": "gallery.html",
    "title": "Blog",
    "section": "",
    "text": "Image Classification - Malaria cell classification (part1)\n\n\n\n\n\n\n\n\n\n\n\nAug 2021\n\n\nChris Lee\n\n\n\n\n\n\n\n\n\n\n\n\nImage Classification - Malaria cell classification (part2)\n\n\n\n\n\n\n\n\n\n\n\nAug 2021\n\n\nChris Lee\n\n\n\n\n\n\n\n\n\n\n\n\nMapping SMPD1/ASM dependent Lipid Raft Proteome in cancer cells\n\n\n\n\n\n\n\n\n\n\n\nFeb 2023\n\n\nChris Lee\n\n\n\n\n\n\n\n\n\n\n\n\nMarket Campaign at a Glance\n\n\n\n\n\n\n\n\n\n\n\nJul 2024\n\n\nChris Lee\n\n\n\n\n\n\n\n\n\n\n\n\nRepurposing desipramine for neurological disorders?\n\n\n\n\n\n\n\n\n\n\n\nJun 2024\n\n\nChris Lee\n\n\n\n\n\n\n\n\n\n\n\n\nreports\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’m driven by helping others successfully solve problems like turning complex data into clear, actionable insights that help businesses thrive. My toolkit to deliver these solutions draws on blending business understanding with machine learning using SQL, R, SAS, and Python . I’m passionate about using data to solve real-world problems, streamline processes, and find new growth opportunities. Balancing the technical side with a strong understanding of business needs, I create solutions that make a real impact and keep things moving forward.\nFeel free to reach out to me on LinkedIn or drop me an email to discuss.\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact",
    "section": "",
    "text": "meowoo@outlook.com\n\n\nPhone  282-351-0640\n\n\n&lt; i class=“fa fa-anchor” style=“font-size:24px”&gt;   MD & CH, Sheridan, WY 82801\n\n\n    \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Chris Lee",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nImage Classification (Part1)\n\n\n\n\n\nSave time & Money! \n\n\n\n\n\n2021-08-31\n\n\n14 min\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "projects.html#project-5",
    "href": "projects.html#project-5",
    "title": "Projects",
    "section": "Project 5",
    "text": "Project 5"
  },
  {
    "objectID": "reports.html",
    "href": "reports.html",
    "title": "reports",
    "section": "",
    "text": "This is test!\n\n\n\n\n Back to top"
  },
  {
    "objectID": "reports/test1.html",
    "href": "reports/test1.html",
    "title": "reports",
    "section": "",
    "text": "This is test!\n\n\n\n\n Back to top"
  },
  {
    "objectID": "reports/market campaign.html",
    "href": "reports/market campaign.html",
    "title": "Market Campaign at a Glance",
    "section": "",
    "text": "rm(list=ls())\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n✔ readr     2.1.5     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(gt)\nlibrary(reshape)\n\n\nAttaching package: 'reshape'\n\nThe following object is masked from 'package:lubridate':\n\n    stamp\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, smiths\n\nThe following object is masked from 'package:dplyr':\n\n    rename\n\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nlibrary(DT)\nlibrary(echarts4r)\n\nWarning: package 'echarts4r' was built under R version 4.4.1\n\nlibrary(echarts4r.maps)\nlibrary(gridExtra)  \n\nWarning: package 'gridExtra' was built under R version 4.4.1\n\n\n\nAttaching package: 'gridExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nlibrary(data.table)\n\n\nAttaching package: 'data.table'\n\nThe following object is masked from 'package:reshape':\n\n    melt\n\nThe following objects are masked from 'package:lubridate':\n\n    hour, isoweek, mday, minute, month, quarter, second, wday, week,\n    yday, year\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\nsource(\"~/data/helper.R\")\ntrain &lt;- read.csv(\"~/data/train.csv\")\nstate &lt;- read.csv(\"~/data/states.csv\")\n\n#| label: prep-data\n\ntrain   &lt;- train |&gt;\n    mutate(\n    success = case_when(\n      conversion_status == \"converted\"  ~ 1,\n      conversion_status == \"not_converted\"  ~ 0\n    ),\n    request_update = case_when(\n      update_me == \"Y\"  ~ 1,\n      update_me == \"N\"  ~ 0\n    ))\n# converted rate on average\navg_converted_rate &lt;- train |&gt;\n  #group_by(geoid)|&gt;\n  count(conversion_status) |&gt;\n  mutate(p = round(100*(n / sum(n)),2))|&gt;\n  filter(conversion_status == \"converted\")|&gt;\n  pull(p)\n\n# average converted rate based on communication channel\ncomm_converted_rate &lt;- train |&gt;\n  group_by(communication_channel)|&gt;\n  count(conversion_status) |&gt;\n  mutate(p = round(100*(n / sum(n)),2))|&gt;\n  filter(conversion_status == \"converted\")\n\n# state-wise average converted rate \nstate_converted_rate &lt;- train |&gt;\n  group_by(geoid)|&gt;\n  count(conversion_status) |&gt;\n  mutate(p = round(100*(n / sum(n)),2))|&gt;\n  filter(conversion_status == \"converted\")\n\n# quarter average converted rate \nquarter_converted_rate &lt;- train |&gt;\n  group_by(quater_res)|&gt;\n  count(conversion_status) |&gt;\n  mutate(p = round(100*(n / sum(n)),2))|&gt;\n  filter(conversion_status == \"converted\")\n\nbest_comm_pct &lt;- max(comm_converted_rate$p)\nbest_comm_way &lt;- comm_converted_rate|&gt;\n             filter(p == best_comm_pct)|&gt;\n             pull(communication_channel)\n\nbest_state_pct &lt;- max(state_converted_rate$p)\nbest_state &lt;- state_converted_rate |&gt;\n             filter(p == best_state_pct)|&gt;\n             pull(geoid)\nbest_q_pct &lt;- max(quarter_converted_rate$p)\nbest_q &lt;- quarter_converted_rate |&gt;\n             filter(p == best_q_pct )|&gt;\n             pull(quater_res)"
  },
  {
    "objectID": "reports/market campaign.html#column",
    "href": "reports/market campaign.html#column",
    "title": "Market Campaign at a Glance",
    "section": "Column",
    "text": "Column\n\ncolnames(state)[2]= \"geoid\"\nst &lt;- merge(train, state, by= \"geoid\")\n\n  st |&gt;  \n  group_by(State,quater_res)|&gt;\n  count(conversion_status) |&gt;\n  mutate(p = 100*(n / sum(n))) |&gt;\n  filter(conversion_status == \"converted\")|&gt;\n  select(State, quater_res, p)|&gt;\n  #pivot_wider(names_from = quater_res, values_from = p)|&gt;\n  #tidyr::gather(\"Key\",  \"Value\", q1, q2, q4) |&gt; \n  group_by(quater_res)|&gt;\n  e_chart(State, timeline = TRUE) |&gt;\n  em_map(\"USA\") |&gt; \n  e_map(p, map = \"USA\") |&gt;\n  e_visual_map(min= 0, max= 30, color = c(\"darkred\",\"red\",\"orange\",\"yellow\",\"lightgreen\",\"steelblue\",\"darkblue\"))|&gt;\n  e_timeline_opts(autoPlay = TRUE) \n\n\n\n\n\n\nstate_converted_rate[,c(1,3:4)] |&gt;\n#  arrange(id) |&gt;\n  datatable(\n    colnames = c(\"State\",\"Converted #\", \"Converted %\"),\n    options = list(dom = 'ftp', paging = TRUE)\n    )"
  },
  {
    "objectID": "reports/market campaign.html#column-1",
    "href": "reports/market campaign.html#column-1",
    "title": "Market Campaign at a Glance",
    "section": "Column",
    "text": "Column\n\nresults_list &lt;- vector(\"list\", ncol(train) - 1)\n# Loop through the columns starting from the 2nd column\nfor (i in 2:ncol(train)) {\n  # Store the result of cal_freq(i) in the list\n  results_list[[i - 1]] &lt;- cal_freq(i)\n}\ndt &lt;- do.call(rbind, results_list)\n\n# label bar graph\ndt$vars &lt;- ifelse(dt$Freqx &gt; 0.15, as.character(dt$Var1), \"\")\n\nf &lt;- dt |&gt;\nggplot(aes(x = round(Freqx,2), y = fct_rev(namex), fill = Var1)) +\n  geom_col(position = \"fill\", color = \"white\") +\n  scale_x_continuous(labels = label_percent()) +\n  labs(y = NULL, x = NULL, fill = \"var1\")+\n  guides(fill=\"none\") +\n  geom_text(aes(label = vars), position = position_stack(vjust =   0.5), color = \"white\")+\n  theme_minimal()\n\nd &lt;- train |&gt;\n group_by(quater_res)|&gt;\n  count(conversion_status) |&gt;\n  mutate(p = round(100*(n / sum(n)),2))|&gt;\n  filter(conversion_status == \"converted\")|&gt;\n  ggplot(aes(x = quater_res, y = p , group = 1)) +\n  geom_line(color = \"steelblue\",linewidth = 1)+\n  geom_point(color = 5)+\n  coord_cartesian(ylim = c(10, 14))+\n  ggtitle(\"23'Q2-24'Q2 Overall Conversion rate\") +\n  xlab(NULL) + \n  ylab(NULL)+ \n  theme(legend.title=element_blank(),\n    panel.background = element_blank())+\n    theme(axis.line = element_line(color = 'black'))\n\na &lt;- train |&gt;\n group_by(previous_campaign_outcome,quater_res)|&gt;\n  count(conversion_status) |&gt;\n  mutate(p = round(100*(n / sum(n)),2))|&gt;\n  filter(conversion_status == \"converted\")|&gt;\n  ggplot(aes(x = quater_res, y = p, group = previous_campaign_outcome, color =previous_campaign_outcome )) +\n  geom_line()+\n  geom_point()+\n  ggtitle(\"Previous Campaign vs Conversion rate\") +\n  xlab(NULL) + \n  ylab(NULL)+ \n  theme(legend.title=element_blank(),\n    panel.background = element_blank())+\n    theme(axis.line = element_line(color = 'black'))\n  \nb &lt;- train |&gt;\n group_by(communication_channel,quater_res)|&gt;\n  count(conversion_status) |&gt;\n  mutate(p = round(100*(n / sum(n)),2))|&gt;\n  filter(conversion_status == \"converted\")|&gt;\n  ggplot(aes(x = quater_res, y = p, group = communication_channel, color =communication_channel )) +\n  geom_line()+\n  geom_point()+\n  ggtitle(\"Communication channel vs Conversion rate\") +\n  xlab(NULL) + \n  ylab(NULL)+ \n  theme(legend.title=element_blank(),\n    panel.background = element_blank())+\n    theme(axis.line = element_line(color = 'black'))\n\nc &lt;- train |&gt;\n group_by(update_me, quater_res)|&gt;\n  count(conversion_status) |&gt;\n  mutate(p = round(100*(n / sum(n)),2))|&gt;\n  filter(conversion_status == \"converted\")|&gt;\n  ggplot(aes(x = quater_res, y = p, group = update_me, color =update_me )) +\n  geom_line()+\n  geom_point()+\n  ggtitle(\"Request update vs Conversion rate\") +\n  xlab(NULL) + \n  ylab(NULL)+ \n  theme(legend.title=element_blank(),\n    panel.background = element_blank())+\n    theme(axis.line = element_line(color = 'black'))  \n\ngrid.arrange(d, a, b, c, nrow = 4) \n\n\n\n\n\n\n\n\n\ndt &lt;- train |&gt;\n    select(\"success\", \"call_duration\", \"call_frequency\",\"request_update\",\"call_frequency\",\"len_business\" )\n\ncormat &lt;- round(cor(dt),2)\ncormat[lower.tri(cormat)]&lt;- NA\ncormat &lt;-  reshape2::melt(cormat)\ncolnames(cormat) &lt;- c(\"X1\",\"X2\",\"value\")\na &lt;-\n  cormat|&gt;\n  ggplot( aes(x=X2, y=X1, fill=value)) + \n  geom_tile()+\n  geom_text(aes(X2, X1, label = value), color = \"white\", size = 4)+\n  theme_minimal() +\n  theme(axis.line = element_line(color = 'black'))+\n  labs(y = NULL, x = NULL)\n  \nb &lt;- category_cor(data_= train, var_ =\"communication_channel\", target_ = \"success\" )\n\nWarning: The melt generic in data.table has been passed a matrix and will\nattempt to redirect to the relevant reshape2 method; please note that reshape2\nis superseded and is no longer actively developed, and this redirection is now\ndeprecated. To continue using melt methods from reshape2 while both libraries\nare attached, e.g. melt.list, you can prepend the namespace, i.e.\nreshape2::melt(cormat). In the next version, this warning will become an error.\n\nc &lt;- category_cor(data_= train, var_ =\"previous_campaign_outcome\", target_ = \"success\" )\n\nWarning: The melt generic in data.table has been passed a matrix and will\nattempt to redirect to the relevant reshape2 method; please note that reshape2\nis superseded and is no longer actively developed, and this redirection is now\ndeprecated. To continue using melt methods from reshape2 while both libraries\nare attached, e.g. melt.list, you can prepend the namespace, i.e.\nreshape2::melt(cormat). In the next version, this warning will become an error.\n\ngrid.arrange( a, b,  nrow = 2)   \n\nWarning: Removed 10 rows containing missing values or values outside the scale range\n(`geom_text()`).\n\n\nWarning: Removed 6 rows containing missing values or values outside the scale range\n(`geom_text()`).\n\n\n\n\n\n\n\n\n\n\nresults_list &lt;- vector(\"list\", ncol(train) - 1)\n# Loop through the columns starting from the 2nd column\nfor (i in 2:ncol(train)) {\n  # Store the result of cal_freq(i) in the list\n  results_list[[i - 1]] &lt;- cal_freq(i)\n}\ndt &lt;- do.call(rbind, results_list)\n\n# label bar graph\ndt$vars &lt;- ifelse(dt$Freqx &gt; 0.15, as.character(dt$Var1), \"\")\n\ndt |&gt;\nggplot(aes(x = round(Freqx,2), y = fct_rev(namex), fill = Var1)) +\n  geom_col(position = \"fill\", color = \"white\") +\n  scale_x_continuous(labels = label_percent()) +\n  labs(y = NULL, x = NULL, fill = \"var1\")+\n  guides(fill=\"none\") +\n  geom_text(aes(label = vars), position = position_stack(vjust =   0.5), color = \"white\")+\n  theme_minimal()"
  },
  {
    "objectID": "reports/Image Classification - Malaria cell case study.html",
    "href": "reports/Image Classification - Malaria cell case study.html",
    "title": "Image Classification - Malaria cell classification (part1)",
    "section": "",
    "text": "Image Classification - Malaria cell case study \nI remember years ago seeing my colleague spent hours under a microscopes counting cells underwent of apoptosis or Dauer larva formation. I mean it is fun doing experiments in the lab but telling differences of these tiny worms would probably is the last thing I’d want to do. This task does take lots of valuable time from a researcher. Imagine, how many more novel anti-agents like this article Yongsoon could bring us if the deep learning techniques were ready to use back in 2011.\n\n\n\nKim Y, Sun H (2012) PLOS ONE 7(9): e45890\n\n\nThanks to the advancement in deep learning field, neural network model architectures can be readily reused and, in most cases, are tested across multiple applications to establish robustness. Here, I’m going to show how easy it is to implement transfer learning using Keras in Python for Malaria cell classification. The basic concept of transfer learning is using the knowledge (architecture or weights) gained from a neural network model that was trained to recognize animals to recognize cats. The dataset used here came from NIH, along with recent publications1,2.\n\nWorkflow\n\nLoading data and data pre-processing\nTransfer learning and fine-tuning (DenseNet121)\nResult evaluation\n\n\n\n\nflow1x\n\n\n\n\nData Overview\nThere are many ways to create train/valid/test data sets. Below is one of the methods using R to create csv files containing file paths and classifications from train and test folders.\n# R code\nlibrary(fs)\ndataset_dir &lt;- \"Data/cell_images/\"\ntest_dir   &lt;- paste0(dataset_dir, \"/test/\")\n# stored image paths in the image column\ntest_parasite &lt;- dir_ls(path=paste0(test_dir, \"parasite\"),glob = \"*.png\")\ntest_uninfected &lt;- dir_ls(path=paste0(test_dir, \"uninfected\"),glob = \"*.png\")\ntest_par &lt;- as.data.frame(matrix('parasite', length(test_parasite), 1))\ntest_unin &lt;- as.data.frame(matrix('uninfected', length(test_parasite), 1))\ntest_par$image &lt;- test_parasite \ntest_unin$image &lt;- test_uninfected\n\ntest &lt;- rbind(test_par,test_unin)\ncolnames(test)[1] &lt;- 'label'\ntest$normal  &lt;- ifelse(test$label != 'parasite', 1,0)\ntest$parasite &lt;- ifelse(test$label == 'parasite', 1,0)\nAnd the csv file looks like this. \nIn reality, we don’t usually see many cells infected with parasites, therefore less than 1/3 of the infected samples were used in this exercise.\n# Python\n# get ids for each label\nall_img_ids = list(new_df.uninfected.index.unique())\ntrain_ids, test_ids = train_test_split(all_img_ids, test_size=0.01, random_state=21)\ntrain_ids, valid_ids = train_test_split(train_ids, test_size=0.1, random_state=21)\nMaking sure, the proportion of the infected cell is as expected after data split. \nLet’s also check few images. The images come with different sizes. They will need to reshape and normalize before xx.\n# Extract numpy values from image column in data frame\ntrain_df = new_df.iloc[train_ids,:]\nimages = train_df['image'].values\n# Extract 9 random images \nrandom_images = [np.random.choice(images) for i in range(9)]\nimg_dir = 'C:/Users/your_image_folder'\nprint('Display Random Images')\n# Adjust the size of your images\nplt.figure(figsize=(20,10))\nfor i in range(9):\n    plt.subplot(3, 3, i + 1)\n    img = plt.imread(os.path.join(img_dir, random_images[i]))\n    plt.imshow(img)\n    plt.axis('off')\n\n\n\nrandom_images\n\n\n\n\nLoading data\nNext is building generators from the Keras framework. The purpose of building generator is that it allows to generate batches of tensor image data with real-time data augmentation(ex: random horizontal flipping of images). We also use the generator to transform the values in each batch so that their mean is 0 and their standard deviation is 1.Here is the information of ImageDataGenerator and a short tutorial. We’ll also need to build a sereperate generator for valid and test sets. Since each image will be normailized using mean and standard deviation derived from its own batch. In a real life scenario, we process one image at a time. And the incoming image is normalized using the statistics computed from the training set.\n# Train generator\ndef get_train_generator(df, image_dir, x_col, y_cols, shuffle=True, batch_size=8, seed=1, target_w = 224, target_h = 224):\n    \"\"\"\n\n    Args:\n      train_df (dataframe): dataframe specifying training data.\n      image_dir (str): directory where image files are held.\n      x_col (str): name of column in df that holds filenames.\n      y_cols (list): list of strings that hold y labels for images.\n      sample_size (int): size of sample to use for normalization statistics.\n      batch_size (int): images per batch to be fed into model during training.\n      seed (int): random seed.\n      target_w (int): final width of input images.\n      target_h (int): final height of input images.\n    \n    Returns:\n        train_generator (DataFrameIterator): iterator over training set\n    \"\"\"        \n    print(\"getting train generator...\") \n    # normalize images\n    image_generator = ImageDataGenerator(\n        samplewise_center=True,\n        samplewise_std_normalization= True)\n    \n    # flow from directory with specified batch size and target image size\n    generator = image_generator.flow_from_dataframe(\n            dataframe=df,\n            directory=image_dir,\n            x_col=x_col,\n            y_col=y_cols,\n            class_mode=\"raw\",\n            batch_size=batch_size,\n            shuffle=shuffle,\n            seed=seed,\n            target_size=(target_w,target_h))\n    \n    return generator  \nBefore, model building we’ll need to define a loss function to adress class imbalance. We can give more weight for the less frequent class and less weight for the other one, see here . We can write the overall average cross-entropy loss over the entire training set D of size N as follows:\n\n\n\nloss\n\n\nNext, we will use a pre-trained DenseNet121 model which we can load directly from Keras and then add two layers on top of it.\n\nSet include_top=False, to remove the orginal fully connect dense layer (so you can adjust the ouptut prediction clsses or\nactivation function).\nUse specific layer using get_layer(). For example: base_model.get_layer(‘conv5_block16_conv’)\n\nA GlobalAveragePooling2D layer to get the average of the last convolution layers from DenseNet121. The pooling layer typically uses a filter to extract representative features (e.g., maximum, average, etc.) for different locations. The method of extracting features from the pooling filter is called a pooling function. The commonly used pooling functions include the maximum pooling function, average pooling function, L2 normalization, and weighted average pooling function based on the distance from the center pixel. In short, the pooling layer summarizes all the feature information centered on each position of the input feature map, which makes it reasonable that the output data of the pooling layer is less than the input data. This method reduces the input data to the next layer and improves the computational efficiency of the CNN.\nThe output of the pooling layer is flattening to convert the pooled features maps into a single dimensional array. This is done in order for the data to be fed into densely connected hidden layers.\nA Dense layer with sigmoid activation to get the prediction logits for each of our classes. We can set our custom loss function for the model by specifying the loss parameter in the compile() function.\n# Build model\ndef create_dense121_model():\n    \n    pretrained = 'fine_tuned.hdf5'\n    train_df = pd.read_csv(\"train_df.csv\")\n    labels = ['uninfected', 'parasite']  \n    \n    class_pos = train_df.loc[:, labels].sum(axis=0)\n    class_neg = len(train_df) - class_pos\n    class_total = class_pos + class_neg\n\n    pos_weights =  class_pos / class_total #[0.5,class_pos / class_total]\n    neg_weights =  class_neg / class_total #[0.5,class_neg / class_total]\n    print(\"Got loss weights\")\n    \n    def create_model(input_shape=(224, 224,3)):\n        base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=input_shape)\n       \n        # add a global spatial average pooling layer\n        x = GlobalAveragePooling2D()(base_model.output)\n        x = Flatten()(x)\n        x = Dense(1024, activation='relu', name='dense_post_pool')(x)\n        x = Dropout(0.8)(x)\n        # output has two neurons for the 2 classes (uninfected and parasite)\n        predictions = Dense(len(labels), activation='sigmoid')(x)\n        model = Model(inputs = base_model.input, outputs = predictions)\n\n        # freeze the earlier layers\n        for layer in base_model.layers[:-4]:\n            layer.trainable=False\n        \n        return model\n    \n    def get_weighted_loss(neg_weights, pos_weights, epsilon=1e-7):\n        def weighted_loss(y_true, y_pred):\n            y_true = tf.cast(y_true, tf.float32)\n            #print(f'neg_weights : {neg_weights}, pos_weights: {pos_weights}')\n            #print(f'y_true : {y_true}, y_pred: {y_pred}')\n            # L(X, y) = −w * y log p(Y = 1|X) − w *  (1 − y) log p(Y = 0|X)\n            # from https://arxiv.org/pdf/1711.05225.pdf\n            loss = 0\n            \n            for i in range(len(neg_weights)):\n                loss -= (neg_weights[i] * y_true[:, i] * K.log(y_pred[:, i] + epsilon) + \n                         pos_weights[i] * (1 - y_true[:, i]) * K.log(1 - y_pred[:, i] + epsilon))\n            \n            loss = K.sum(loss)\n            return loss\n        return weighted_loss\n    \n   \n    model = create_model()\n    model.load_weights(pretrained)\n    print(\"Loaded Model\")\n    \n    model.compile(optimizer='adam', loss= get_weighted_loss(neg_weights, pos_weights)) \n    print(\"Compiled Model\")   \n          \n    return model\nModel is fine tuned using ModelCheckpoint and only the model’s weights will be saved.\n# CallBack \n# -------------------------------------------------------------------------------------------------\n# Callback Function 1\nfname = 'dense121(V)_Epoch[{epoch:02d}].ValLoss[{val_loss:.3f}].hdf5'\nfullpath = fname\n# https://keras.io/api/callbacks/model_checkpoint/\ncallback_func1 = ModelCheckpoint(filepath=fullpath,             \n                                monitor='val_loss',             \n                                verbose=1,                      \n                                save_best_only=True,            \n                                save_weights_only=True, # save weights       \n                                mode='min',                     \n                                period=1)                       \n\n# Callback Function 2\n# https://keras.io/callbacks/#tensorboard\ncallback_func2 = keras.callbacks.TensorBoard(log_dir='./logs/log2', histogram_freq=1)\n\n# Callback Function\ncallbacks = []\ncallbacks.append(callback_func1)\ncallbacks.append(callback_func2)\n\n# Training and Plotting\n# -------------------------------------------------------------------------------------------------\nhistory = model.fit(train_generator, \n                              validation_data=valid_generator,\n                              steps_per_epoch=100, \n                              validation_steps=25, \n                              epochs = 15,\n                              callbacks=callbacks)\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.ylabel(\"loss\")\nplt.xlabel(\"epoch\")\nplt.title(\"Training Loss Curve\")\nplt.show()\nTrain versus validation loss for all epochs is shown here. The orange and blue lines indicate train loss and validation loss respectively. We can see the model may be under-fitted. One way to overcome this is simply increase the number of epochs. Also with the callback function, we can re-use the best weights saved at 12th epoch.\n\n\n\nhistory\n\n\n\n\nEvaluation\nThe ROC curve is created by plotting the true positive rate against the false positive rate. We can see the model performs reasonable well.\n\n\n\nROC\n\n\nWe can try different approaches to improve the model perfromance, such as train the model for a longer time or use all the training data (since only 1/3 of the parasite data was used). We can also try a different base model, the previous publication, shows 99.32% accuracy with VGG-19 alone.\n\n\nVisualize class activation maps\nNext, I will show how to produce visual explanation using Grad-CAM. The purpose of doing this is as following:\n\nDebug your model and visually validate that it is “looking” and “activating” at the correct locations in an image.\nGrad-CAM works by (1) finding the final convolutional layer in the network and then (2) examining the gradient information flowing into that layer.\n\n\n\nNotes\nNote 1: AUC is the area below these ROC curves. Therefore, in other words, AUC is a great indicator of how well a classifier functions. Note 2: A good tutorial for to learn neural network image classification from scratch and Andrew Ng’s deep learning course.\nNote 3:\n# packages used \nimport os\nimport sklearn\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport keras\nfrom keras.applications.densenet import DenseNet121\nfrom keras.models import Model\nfrom keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization, GlobalAveragePooling2D\nfrom keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, TensorBoard\nfrom keras import backend as K\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import ImageDataGenerator\n\n\n\n\n Back to top"
  },
  {
    "objectID": "reports/asm1.html",
    "href": "reports/asm1.html",
    "title": "Repurposing desipramine for neurological disorders?",
    "section": "",
    "text": "Results\n\n\n\n\n\n\nTable\n\n\n\n\n\n\n\n\nTotal Proteins found\n870\n\n\nASM Sensitive\n227\n\n\nDesipramine Sensitive\n206\n\n\nMangostin Sensitive\n208\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure legends\n\n\n\n\n\nFig 1: Ontology analysis on identified 870 proteins. Fig 2: Around 40% of changes overlapped between treatments. Fig 3: The amount of protein changes in Desipramine and Mangostin treated samples were similar. More than 60% proteins remain unchanged after ASM reduction. Fig 4: Functions of protein changes in different treatments(a-c). Fig 5: Top 4 proteins affected after ASM reduction involved in (a) vesicle trafficking (b) apoptosis (c) enzymatic activities.\n\n\n\n\n\n\n\n\n\n\nSummary\n\n\n\n\n\nOur data shows the amount of autophagy related proteins were increased in lipid raft after ASM inhibition through siASM. However, the amount remained the same or decreased in cells treated with Mangostin and Desipramine, respectively.\n\n\n\n\n\n\n\n\n\n\nReferences\n\n\n\n\n\n\nXiong X, Lee CF, Li W, Yu J, Zhu L, Kim Y, Zhang H, Sun H. Acid Sphingomyelinase regulates the localization and trafficking of palmitoylated proteins. Biol Open. 2019 Oct 15;8(10):bio040311. doi: 10.1242/bio.040311. PMID: 31142470; PMCID: PMC6826292.\nPetrosyan E, Fares J, Cordero A, Rashidi A, Arrieta VA, Kanojia D, Lesniak MS. Repurposing autophagy regulators in brain tumors. Int J Cancer. 2022 Jul 15;151(2):167-180. doi: 10.1002/ijc.33965. Epub 2022 Mar 4. PMID: 35179776; PMCID: PMC9133056."
  },
  {
    "objectID": "reports/asm1.html#row",
    "href": "reports/asm1.html#row",
    "title": "Repurposing desipramine for neurological disorders?",
    "section": "Row",
    "text": "Row\n\nFigure 1\n\n\n\nRoles of ASM in neurological disorders ASM is linked to major neurological disorders such as major depression, Parkinson’s disease, and Alzheimer’s disease.Medications like desipramine and alpha-mangostin, which inhibit ASM and can cross the BBB, show potential for treating neurological disorders. (Modified from Exp Mol Med (2024);56.)\n\n\n\n\nColumn\n\n\n\nResearch Methods To explore whether ASM inhibition through siASM, desipramine, and alpha-mangostin affect the protein composition of lipid rafts in the same way, we treated cells with or without these ASM inhibitors and analyzed the results using the LC-MS/MS method. This dashboard presents the findings of our quantitative analysis. (Modified from Biomolecules. 2024 Feb; 14(2): 156.)"
  },
  {
    "objectID": "reports/asm1.html#row-1",
    "href": "reports/asm1.html#row-1",
    "title": "Repurposing desipramine for neurological disorders?",
    "section": "Row",
    "text": "Row"
  },
  {
    "objectID": "reports/asm1.html#row-2",
    "href": "reports/asm1.html#row-2",
    "title": "Repurposing desipramine for neurological disorders?",
    "section": "Row",
    "text": "Row\n\n\n\n\n\n\n\n\n\n\nColumn\n\n\nColumn"
  },
  {
    "objectID": "reports/asm1.html#row-3",
    "href": "reports/asm1.html#row-3",
    "title": "Repurposing desipramine for neurological disorders?",
    "section": "Row",
    "text": "Row\n\nColumn"
  },
  {
    "objectID": "posts/2021-08-31-malaria-cell-classification/index.html",
    "href": "posts/2021-08-31-malaria-cell-classification/index.html",
    "title": "Image Classification - Malaria cell classification (part1)",
    "section": "",
    "text": "I remember years ago seeing my colleague spent hours under a microscopes counting cells underwent of apoptosis or Dauer larva formation. I mean it is fun doing experiments in the lab but telling differences of these tiny worms would probably is the last thing I’d want to do. This task does take lots of valuable time from a researcher. Imagine, how many more novel anti-agents like this article Yongsoon could bring us if the deep learning techniques were ready to use back in 2011.\n\n\n\nKim Y, Sun H (2012) PLOS ONE 7(9): e45890\n\n\nThanks to the advancement in deep learning field, neural network model architectures can be readily reused and, in most cases, are tested across multiple applications to establish robustness. Here, I’m going to show how easy it is to implement transfer learning using Keras in Python for Malaria cell classification. The basic concept of transfer learning is using the knowledge (architecture or weights) gained from a neural network model that was trained to recognize animals to recognize cats. The dataset used here came from NIH, along with recent publications1,2.\n\nWorkflow\n\nLoading data and data pre-processing\nTransfer learning and fine-tuning (DenseNet121)\nResult evaluation\n\n\n\n\nflow1x\n\n\n\n\nData Overview\nThere are many ways to create train/valid/test data sets. Below is one of the methods using R to create csv files containing file paths and classifications from train and test folders.\n# R code\nlibrary(fs)\ndataset_dir &lt;- \"Data/cell_images/\"\ntest_dir   &lt;- paste0(dataset_dir, \"/test/\")\n# stored image paths in the image column\ntest_parasite &lt;- dir_ls(path=paste0(test_dir, \"parasite\"),glob = \"*.png\")\ntest_uninfected &lt;- dir_ls(path=paste0(test_dir, \"uninfected\"),glob = \"*.png\")\ntest_par &lt;- as.data.frame(matrix('parasite', length(test_parasite), 1))\ntest_unin &lt;- as.data.frame(matrix('uninfected', length(test_parasite), 1))\ntest_par$image &lt;- test_parasite \ntest_unin$image &lt;- test_uninfected\n\ntest &lt;- rbind(test_par,test_unin)\ncolnames(test)[1] &lt;- 'label'\ntest$normal  &lt;- ifelse(test$label != 'parasite', 1,0)\ntest$parasite &lt;- ifelse(test$label == 'parasite', 1,0)\nAnd the csv file looks like this. \nIn reality, we don’t usually see many cells infected with parasites, therefore less than 1/3 of the infected samples were used in this exercise.\n# Python\n# get ids for each label\nall_img_ids = list(new_df.uninfected.index.unique())\ntrain_ids, test_ids = train_test_split(all_img_ids, test_size=0.01, random_state=21)\ntrain_ids, valid_ids = train_test_split(train_ids, test_size=0.1, random_state=21)\nMaking sure, the proportion of the infected cell is as expected after data split. \nLet’s also check few images. The images come with different sizes. They will need to reshape and normalize before xx.\n# Extract numpy values from image column in data frame\ntrain_df = new_df.iloc[train_ids,:]\nimages = train_df['image'].values\n# Extract 9 random images \nrandom_images = [np.random.choice(images) for i in range(9)]\nimg_dir = 'C:/Users/your_image_folder'\nprint('Display Random Images')\n# Adjust the size of your images\nplt.figure(figsize=(20,10))\nfor i in range(9):\n    plt.subplot(3, 3, i + 1)\n    img = plt.imread(os.path.join(img_dir, random_images[i]))\n    plt.imshow(img)\n    plt.axis('off')\n\n\n\nrandom_images\n\n\n\n\nLoading data\nNext is building generators from the Keras framework. The purpose of building generator is that it allows to generate batches of tensor image data with real-time data augmentation(ex: random horizontal flipping of images). We also use the generator to transform the values in each batch so that their mean is 0 and their standard deviation is 1.Here is the information of ImageDataGenerator and a short tutorial. We’ll also need to build a sereperate generator for valid and test sets. Since each image will be normailized using mean and standard deviation derived from its own batch. In a real life scenario, we process one image at a time. And the incoming image is normalized using the statistics computed from the training set.\n# Train generator\ndef get_train_generator(df, image_dir, x_col, y_cols, shuffle=True, batch_size=8, seed=1, target_w = 224, target_h = 224):\n    \"\"\"\n\n    Args:\n      train_df (dataframe): dataframe specifying training data.\n      image_dir (str): directory where image files are held.\n      x_col (str): name of column in df that holds filenames.\n      y_cols (list): list of strings that hold y labels for images.\n      sample_size (int): size of sample to use for normalization statistics.\n      batch_size (int): images per batch to be fed into model during training.\n      seed (int): random seed.\n      target_w (int): final width of input images.\n      target_h (int): final height of input images.\n    \n    Returns:\n        train_generator (DataFrameIterator): iterator over training set\n    \"\"\"        \n    print(\"getting train generator...\") \n    # normalize images\n    image_generator = ImageDataGenerator(\n        samplewise_center=True,\n        samplewise_std_normalization= True)\n    \n    # flow from directory with specified batch size and target image size\n    generator = image_generator.flow_from_dataframe(\n            dataframe=df,\n            directory=image_dir,\n            x_col=x_col,\n            y_col=y_cols,\n            class_mode=\"raw\",\n            batch_size=batch_size,\n            shuffle=shuffle,\n            seed=seed,\n            target_size=(target_w,target_h))\n    \n    return generator  \nBefore, model building we’ll need to define a loss function to adress class imbalance. We can give more weight for the less frequent class and less weight for the other one, see here . We can write the overall average cross-entropy loss over the entire training set D of size N as follows:\n\n\n\nloss\n\n\nNext, we will use a pre-trained DenseNet121 model which we can load directly from Keras and then add two layers on top of it.\n\nSet include_top=False, to remove the orginal fully connect dense layer (so you can adjust the ouptut prediction clsses or\nactivation function).\nUse specific layer using get_layer(). For example: base_model.get_layer(‘conv5_block16_conv’)\n\nA GlobalAveragePooling2D layer to get the average of the last convolution layers from DenseNet121. The pooling layer typically uses a filter to extract representative features (e.g., maximum, average, etc.) for different locations. The method of extracting features from the pooling filter is called a pooling function. The commonly used pooling functions include the maximum pooling function, average pooling function, L2 normalization, and weighted average pooling function based on the distance from the center pixel. In short, the pooling layer summarizes all the feature information centered on each position of the input feature map, which makes it reasonable that the output data of the pooling layer is less than the input data. This method reduces the input data to the next layer and improves the computational efficiency of the CNN.\nThe output of the pooling layer is flattening to convert the pooled features maps into a single dimensional array. This is done in order for the data to be fed into densely connected hidden layers.\nA Dense layer with sigmoid activation to get the prediction logits for each of our classes. We can set our custom loss function for the model by specifying the loss parameter in the compile() function.\n# Build model\ndef create_dense121_model():\n    \n    pretrained = 'fine_tuned.hdf5'\n    train_df = pd.read_csv(\"train_df.csv\")\n    labels = ['uninfected', 'parasite']  \n    \n    class_pos = train_df.loc[:, labels].sum(axis=0)\n    class_neg = len(train_df) - class_pos\n    class_total = class_pos + class_neg\n\n    pos_weights =  class_pos / class_total #[0.5,class_pos / class_total]\n    neg_weights =  class_neg / class_total #[0.5,class_neg / class_total]\n    print(\"Got loss weights\")\n    \n    def create_model(input_shape=(224, 224,3)):\n        base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=input_shape)\n       \n        # add a global spatial average pooling layer\n        x = GlobalAveragePooling2D()(base_model.output)\n        x = Flatten()(x)\n        x = Dense(1024, activation='relu', name='dense_post_pool')(x)\n        x = Dropout(0.8)(x)\n        # output has two neurons for the 2 classes (uninfected and parasite)\n        predictions = Dense(len(labels), activation='sigmoid')(x)\n        model = Model(inputs = base_model.input, outputs = predictions)\n\n        # freeze the earlier layers\n        for layer in base_model.layers[:-4]:\n            layer.trainable=False\n        \n        return model\n    \n    def get_weighted_loss(neg_weights, pos_weights, epsilon=1e-7):\n        def weighted_loss(y_true, y_pred):\n            y_true = tf.cast(y_true, tf.float32)\n            #print(f'neg_weights : {neg_weights}, pos_weights: {pos_weights}')\n            #print(f'y_true : {y_true}, y_pred: {y_pred}')\n            # L(X, y) = −w * y log p(Y = 1|X) − w *  (1 − y) log p(Y = 0|X)\n            # from https://arxiv.org/pdf/1711.05225.pdf\n            loss = 0\n            \n            for i in range(len(neg_weights)):\n                loss -= (neg_weights[i] * y_true[:, i] * K.log(y_pred[:, i] + epsilon) + \n                         pos_weights[i] * (1 - y_true[:, i]) * K.log(1 - y_pred[:, i] + epsilon))\n            \n            loss = K.sum(loss)\n            return loss\n        return weighted_loss\n    \n   \n    model = create_model()\n    model.load_weights(pretrained)\n    print(\"Loaded Model\")\n    \n    model.compile(optimizer='adam', loss= get_weighted_loss(neg_weights, pos_weights)) \n    print(\"Compiled Model\")   \n          \n    return model\nModel is fine tuned using ModelCheckpoint and only the model’s weights will be saved.\n# CallBack \n# -------------------------------------------------------------------------------------------------\n# Callback Function 1\nfname = 'dense121(V)_Epoch[{epoch:02d}].ValLoss[{val_loss:.3f}].hdf5'\nfullpath = fname\n# https://keras.io/api/callbacks/model_checkpoint/\ncallback_func1 = ModelCheckpoint(filepath=fullpath,             \n                                monitor='val_loss',             \n                                verbose=1,                      \n                                save_best_only=True,            \n                                save_weights_only=True, # save weights       \n                                mode='min',                     \n                                period=1)                       \n\n# Callback Function 2\n# https://keras.io/callbacks/#tensorboard\ncallback_func2 = keras.callbacks.TensorBoard(log_dir='./logs/log2', histogram_freq=1)\n\n# Callback Function\ncallbacks = []\ncallbacks.append(callback_func1)\ncallbacks.append(callback_func2)\n\n# Training and Plotting\n# -------------------------------------------------------------------------------------------------\nhistory = model.fit(train_generator, \n                              validation_data=valid_generator,\n                              steps_per_epoch=100, \n                              validation_steps=25, \n                              epochs = 15,\n                              callbacks=callbacks)\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.ylabel(\"loss\")\nplt.xlabel(\"epoch\")\nplt.title(\"Training Loss Curve\")\nplt.show()\nTrain versus validation loss for all epochs is shown here. The orange and blue lines indicate train loss and validation loss respectively. We can see the model may be under-fitted. One way to overcome this is simply increase the number of epochs. Also with the callback function, we can re-use the best weights saved at 12th epoch.\n\n\n\nhistory\n\n\n\n\nEvaluation\nThe ROC curve is created by plotting the true positive rate against the false positive rate. We can see the model performs reasonable well.\n\n\n\nROC\n\n\nWe can try different approaches to improve the model perfromance, such as train the model for a longer time or use all the training data (since only 1/3 of the parasite data was used). We can also try a different base model, the previous publication, shows 99.32% accuracy with VGG-19 alone.\n\n\nVisualize class activation maps\nNext, I will show how to produce visual explanation using Grad-CAM. The purpose of doing this is as following:\n\nDebug your model and visually validate that it is “looking” and “activating” at the correct locations in an image.\nGrad-CAM works by (1) finding the final convolutional layer in the network and then (2) examining the gradient information flowing into that layer.\n\n\n\nNotes\nNote 1: AUC is the area below these ROC curves. Therefore, in other words, AUC is a great indicator of how well a classifier functions. Note 2: A good tutorial for to learn neural network image classification from scratch and Andrew Ng’s deep learning course.\nNote 3:\n# packages used \nimport os\nimport sklearn\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport keras\nfrom keras.applications.densenet import DenseNet121\nfrom keras.models import Model\nfrom keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization, GlobalAveragePooling2D\nfrom keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, TensorBoard\nfrom keras import backend as K\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import ImageDataGenerator\n\n\n\n\n Back to top"
  },
  {
    "objectID": "reports/Image Classification - Malaria cell case study 2.html",
    "href": "reports/Image Classification - Malaria cell case study 2.html",
    "title": "Image Classification - Malaria cell classification (part2)",
    "section": "",
    "text": "Image Classification - Malaria cell case study (part2) \nWhen building a predictive model, there are two important criteria: predictive accuracy and interpretability, which generally have a trade-off relationship. In the previous post, we have shown that the DenseNet121 model can achieve high accuracy in detecting cells infected with parasites.\nHere, I am going to introduce a powerful technique GRAD-CAM (gradient-weighted class activation mapping) to visualize which parts of an image are most important to the predictions of an image regression network. GRAD-CAM is a generalization of the CAM technique which determines the importance of each neuron in a network prediction by considering the gradients of the target flowing through the deep network. Unlike CAM which requires a particular kind of CNN architecture to perform global average pooling prior to prediction and forces us to change the base model retrain the network. In contrast, GRAD-CAM is accessing intermediate activations in the deep learning model and computing gradients with respect to the class output. For more details, please see.\nWorkflow: - Obtain predicted class/index - Determine which intermediate layer(s) to use. Lower-level convolution layers capture low level features such as edges, and lines. Higher-level layers usually have more abstract information. - Calculate the gradients with respect to the outout of the class/index - Generate a heatmap by weighing the convolution outputs with the computed gradients - Super-impose the heatmap to the original image\nLoad base model\nWe first load the base model and will only train the last 4 layers.\ndef build_model(input_shape=(224, 224,3),pos_weights,neg_weights):\n  # load the base DenseNet121 model\n  base_model = DenseNet121(input_shape = input_shape, \n                      weights='imagenet', \n                      include_top=False)\n  \n  # add a GAP layer\n  output = layers.GlobalAveragePooling2D()(base_model.output)\n\n  # output has two neurons for the 2 classes (uninfected and parasite)\n  output = layers.Dense(2, activation='softmax')(output)\n\n  # set the inputs and outputs of the model\n  model = Model(base_model.input, output)\n\n  # freeze the earlier layers\n  for layer in base_model.layers[:-4]:\n      layer.trainable=False\n\n  # configure the model for training\n  model.compile(loss= get_weighted_loss(neg_weights, pos_weights), \n                optimizer=adam, \n                metrics=['accuracy'])\n  \n  return model\n \nWe then create a new model that has the original model’s inputs, but two different outputs. The first output contains the activation layers outputs that in this case is the final convolutional layer in the original model. And the second output is the model’s prediction for the image.\ndef get_CAM(model, processed_image, actual_label, layer_name): \n    \"\"\"\n    GradCAM method for visualizing input saliency.\n    \n    Args:\n        model (Keras.model): model to compute cam for\n        image (tensor): input to model, shape (1, H, W, 3)\n        cls (int): class to compute cam with respect to\n        layer_name (str): relevant layer in model\n        H (int): input height\n        W (int): input width\n    Return:\n        heatmap()\n    \"\"\"    \n\n    model_grad = Model([model.inputs], \n                       [model.get_layer(layer_name).output, model.output])\n    \n    with tf.GradientTape() as tape:\n        conv_output_values, predictions = model_grad(processed_image)\n\n        # assign gradient tape to monitor the conv_output\n        tape.watch(conv_output_values)\n        \n        # use binary cross entropy loss, actual_label = 0 if uninfected\n        # get prediction probability of infected  \n        pred_prob = predictions[:,1] \n        \n        # make sure actual_label is a float, like the rest of the loss calculation\n        actual_label = tf.cast(actual_label, dtype=tf.float32)\n        \n        # add a tiny value to avoid log of 0\n        smoothing = 0.00001 \n        \n        # Calculate loss as binary cross entropy\n        loss = -1 * (actual_label * tf.math.log(pred_prob + smoothing) + (1 - actual_label) * tf.math.log(1 - pred_prob + smoothing))\n        print(f\"binary loss: {loss}\")\n    \n    # get the gradient of the loss with respect to the outputs of the last conv layer\n    grads_values = tape.gradient(loss, conv_output_values)\n    grads_values = K.mean(grads_values, axis=(0,1,2))\n    \n    conv_output_values = np.squeeze(conv_output_values.numpy())\n    grads_values = grads_values.numpy()\n    \n    # weight the convolution outputs with the computed gradients\n    for i in range(grads_values.shape[-1]): \n        conv_output_values[:,:,i] *= grads_values[i]\n    heatmap = np.mean(conv_output_values, axis=-1)\n    \n    heatmap = np.maximum(heatmap, 0)\n    heatmap /= heatmap.max()\n    \n    del model_grad, conv_output_values, grads_values, loss\n   \n    return heatmap\n\n\n\nResult\n\n\n\nNote:\nInstead of using max pooling that only keeps the highest valued ones. Average pooling allows some of the lesser intensity pixels to pass on in the pooling layer. It is important as we look at the small size of the image once it reaches this layer, max pooling could leave us with very little information.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "reports/journey.html",
    "href": "reports/journey.html",
    "title": "Mapping SMPD1/ASM dependent Lipid Raft Proteome in cancer cells",
    "section": "",
    "text": "Understanding Glioblastoma and Its Challenges \nGlioblastoma (GBM) is an aggressive brain tumor and one of the toughest cancers to treat. Despite knowing a lot about its genetic makeup, this hasn’t translated into better survival rates for patients, with most living less than two years after diagnosis. The challenges lie in its ability to infiltrate the brain, its genetic diversity, and the protection offered by the blood-brain barrier (BBB), making it resistant to treatments like surgery, radiotherapy, and chemotherapy 1,2. \n  Petrosyan et al.Int J Cancer. 2022 Jul 15;151(2):167-180.PMCID: PMC9133056. \nThe Role of Lipids in GBM Recent studies have revealed that GBM cells have unusual compositions of cholesterol and phospholipids in their membranes, affecting how growth-promoting signals operate. Scientists are particularly interested in sphingolipids, as the balance between sphingomyelin and ceramide is crucial for organizing cell membranes and clustering signaling molecules into lipid rafts. These rafts, which float in the cell membrane, can either attract or repel signaling proteins, thereby regulating cell signaling. This unique lipid environment in GBM cells could be a potential target for new treatments 3.\nExploring Acid Sphingomyelinase (ASM) and Its Inhibitors Acid sphingomyelinase (ASM) is an enzyme that breaks down sphingomyelin into ceramide and phosphocholine. In this report, we treated cells with ASM siRNA, Desipramine (an FDA-approved antidepressant), and α-mangostin (a natural compound with anti-cancer properties) Both Desiparamine and α-mangostin were shown to block ASM activities. This study’s findings are summarized in an interactive Dashboard.\nKey Findings from the Study Our research identified around 530 proteins within lipid rafts, with 40% of them being influenced by ASM and its inhibitors. While 60% of the proteins remained unchanged, 40% involved in cell death, vesicle trafficking, and autophagy (a self-digestion process in cells) were affected. Autophagy can either help cancer cells survive under stress or lead to their death. Interestingly, inhibition of ASM led to increased levels of proteins associated with autophagy. However, this increase was not observed with Desipramine treatment. Desipramine was shown to have a dual effect, stimulatory and repressive on autophagy 4. On the other hand, α-mangostin, known to cause stress and autophagy in breast cancer cells 5.We in α-mangostin treated samples, the levels of autophagy related proteins were down-regulated.\nImplications and Future Directions This study sheds light on how ASM influences the lipid raft proteome in brain tumor cells, suggesting potential new avenues for treatment. Additionally, other research has indicated that selective serotonin reuptake inhibitors (SSRIs) might have beneficial effects in various cancer models, including brain tumors. These insights could pave the way for new strategies in treating GBM, offering hope for better outcomes in the future.\n\n\n\n Back to top"
  }
]