[
  {
    "objectID": "posts/2024-09-01-Graphs-for-Communication/index.html",
    "href": "posts/2024-09-01-Graphs-for-Communication/index.html",
    "title": "Graphs for Communication - Creating Animated Maps",
    "section": "",
    "text": "Making the data easier to understand!\nGoal : To demonstrate how to create animated maps with ggplot2 and gganimate in R to track MRSA BSI incidence in the State of California."
  },
  {
    "objectID": "posts/2024-09-01-Graphs-for-Communication/index.html#graphs-for-communication",
    "href": "posts/2024-09-01-Graphs-for-Communication/index.html#graphs-for-communication",
    "title": "Graphs for Communication - Creating Animated Maps",
    "section": "Graphs for Communication",
    "text": "Graphs for Communication\nEarlier this year, I was preparing a presentation for stakeholders to showcase how our business performed, especially after large events and gatherings were no longer restricted. While we saw solid growth overall, it wasn’t consistent across all locations. One key focus of the presentation was to highlight how sales evolved over time at wedding venues and restaurants.\nI wanted to present the data in a way that was not only accurate but also engaging enough to capture everyone’s attention. To achieve this, I turned to the ggplot2 and gganimate packages in R. These tools allowed me to transform dry numbers into visually compelling animations, making the data easier to understand and far more impact for my audience.\nThe same storytelling strategy can also be applied to the datasets of Methicillin-resistant Staphylococcus aureus (MRSA) bloodstream infections (BSI) in California Hospitals.\nThe datasets include the associated 95% confidence intervals for the SIR and statistical interpretation to show whether MRSA BSI incidence was the same (no different), better (lower) or worse (higher) than the national baseline. Another performance measure in this dataset allows for tracking hospital progress in meeting national HAI reduction goals. Hospitals must have an SIR at or below incremental targets each year to be considered on track.\nIn this practice, we will generate an animated map displays whether MRSA BSI incidence in the State of California was the same (no different), better (lower) or worse (higher) than the national baseline from 2019 – 2023. The comparison is depicted values and colors range from -1 (purple, worse than the national average) to 1 (yellow, better than the national average)."
  },
  {
    "objectID": "posts/2024-09-01-Graphs-for-Communication/index.html#steps",
    "href": "posts/2024-09-01-Graphs-for-Communication/index.html#steps",
    "title": "Graphs for Communication - Creating Animated Maps",
    "section": "Steps",
    "text": "Steps\nThe first section simply loads the libraries and data that will be used.\n\n\nCode\n```{r}\n#| context: setup\n#| code-fold: true\n#| warning: false\n\n# Load required packages\nlibrary(gganimate)\nlibrary(gifski)\nlibrary(dplyr)\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\n\n\n# Load data\nurl_2022 &lt;- \"https://data.chhs.ca.gov/dataset/a345bba7-25ed-4019-b6e3-0b597bea7368/resource/3ff809c6-c1e2-4107-be0c-f976521aadb2/download/mrsa_bsi_odp_2022.csv\"\nurl_2021 &lt;- \"https://data.chhs.ca.gov/dataset/a345bba7-25ed-4019-b6e3-0b597bea7368/resource/23bef156-ae4d-4800-8512-d29660b2269f/download/cdph_mrsa_bsi_odp_2021.csv\"\nurl_2019 &lt;- \"https://data.chhs.ca.gov/dataset/a345bba7-25ed-4019-b6e3-0b597bea7368/resource/62aead61-e320-4a74-b64c-1f65efa72b35/download/cdph_mrsa_bsi_odp_2019.csv\"\nurl_2023 &lt;-\"https://data.chhs.ca.gov/dataset/a345bba7-25ed-4019-b6e3-0b597bea7368/resource/15c4962f-e357-4b03-9d24-655a2bc8030c/download/mrsa_bsi_odp_2023.csv\"\n\ndata_2023 &lt;- read_csv(url_2023)\ndata_2022 &lt;- read_csv(url_2022)\ndata_2021 &lt;- read_csv(url_2021)\ndata_2019 &lt;- read_csv(url_2019)\n\n# Combine data\nmrsa_combine &lt;- bind_rows(data_2023,data_2022, data_2021, data_2019)\n\n# Remove redundant data\nmrsa_combine &lt;- unique(mrsa_combine)\n# Lon and lat info\nus_counties &lt;- map_data(\"county\")|&gt; filter(region ==\"california\")|&gt; mutate( subregion = toupper( subregion))|&gt; dplyr::select(-c( order,  region)) \n\n# Data transformation\nmrsa_combine &lt;- mrsa_combine |&gt;\n  dplyr::select(-State, -HAI, -Met_2020_Goal, -SIR_CI_95_Lower_Limit,-SIR_CI_95_Upper_Limit, -On_Track, -Notes) |&gt;\n  mutate(Months = as.numeric(ifelse(is.na(Months), 13, Months)),\n         Year = as.integer(ifelse(is.na(Year), 2021, Year)),\n         Comparison = case_when(\n      Comparison == \"Worse\" ~ -1,\n      Comparison == \"Better\" ~ 1,\n      Comparison == \"Same\" ~ 0,\n      .default = 0),\n         Patient_Stay = cut(      \n           Patient_Days,\n           breaks = quantile(Patient_Days, probs = seq(0, 1, by = 0.1), na.rm = TRUE),\n           labels = 1:10),\n         Months = as.character(Months),\n         Year = as.character(Year),\n         Infections_Reported = as.numeric(Infections_Reported))|&gt;\n  filter(!grepl(\"Pooled\", Facility_Name, ignore.case = TRUE),\n         !grepl(\"Pooled\", Facility_Type, ignore.case = TRUE))|&gt; \n  mutate( subregion = toupper(County))\n```\n\n\n\nStep 1\nAs gganimate extends the grammar of graphics as implemented by ggplot2 to include the description of animation. Therefore, we will need to generate a base map using ggplot2. Then, add a function of facet_wrap(Hospital_Category_RiskAdjustment) to track the average performance of each hospital category in each county across California.\n\n\nStep 2\nThen, the gganimate library is used to animate the display of color by year (2019 – 2023). Here we define the transition time (in years), add a title and subtitle.\nNote: 1. transition_*() defines how the data should be spread out and how it relates to itself across time. 2. The comparison is depicted values and colors range from -1 (purple, worse than the national average) to 1 (yellow, better than the national average).\n\n\nCode\n```{r}\n#| title: Bloodstream infections (BSI) compare to national overtime\n#| code-fold: true\n#| warning: false\n#| fig-width: 8\n#| fig-height: 8\n\nst &lt;- mrsa_combine|&gt;\n    dplyr::select(subregion, Year, Comparison,Hospital_Category_RiskAdjustment)|&gt;\n  mutate(Year = as.integer(Year))|&gt;\n  group_by(subregion, Year,Hospital_Category_RiskAdjustment)|&gt; \n  summarise(`national baseline` = round(mean(na.omit(as.numeric(Comparison))),2)) \n\nst &lt;- left_join(st, unique(us_counties), by= \"subregion\", relationship = \"many-to-many\") \n\na &lt;-\nst|&gt; \n\n  ggplot(mapping = aes(x = long, y = lat, group = group, fill = `national baseline` ))+\n              geom_polygon(color = \"gray90\", linewidth = 0.3) +\n  coord_map(projection = \"albers\", lat0 = 39, lat1 = 45) +\n  scale_fill_continuous(type = \"viridis\")+\n  theme(axis.line=element_blank(),\n        axis.text=element_blank(),\n        axis.ticks=element_blank(),\n        axis.title=element_blank(),\n        panel.background=element_blank(),\n        panel.border=element_blank(),\n        panel.grid=element_blank())+\n        scale_color_gradient(low = \"green\", high = \"red\")+ \n   facet_wrap(~Hospital_Category_RiskAdjustment)\n            \n            \na + transition_time(Year) +labs(title = \"MRSA BSI incidence compares to the national baseline, Year: {frame_time}\")\n```\n\n\n\n\n\n\n\n\n\n\n\nBonus\nAlso compatible with other ggplot graphs!\nIn this example we see trend plots of continuous variables infections cases and days of patient hospital stays between 2019 and 2023.\nDid you notice the length of patient stay in a hospital is not necessarily positive correlated with numbers of reported cases (except for the acute care hospital category)?\n\n\nCode\n```{r}\n#| title: Patients Days vs Reported infection cases\n#| code-fold: true\n#| warning: false\n#| fig-width: 8\n#| fig-height: 8\n\nst &lt;- left_join(mrsa_combine, unique(us_counties), by= \"subregion\", relationship = \"many-to-many\") \n\np &lt;-     \n  st|&gt; \n  ggplot(aes(x= Patient_Days, y= Infections_Reported))+\n    geom_point(alpha = 0.5, show.legend = FALSE, color = \"#167bb2\") +\n    scale_size(range = c(2, 12)) +\n    theme_bw()+\n    ggtitle(paste0(\"Length of Patients stay vs Reported infection cases\")) +\n    theme(#axis.text.x=element_blank(),\n          axis.text=element_text(size=12),\n          legend.position = \"bottom\",\n          axis.line = element_line(colour = \"black\"),\n          panel.grid.major = element_blank(),\n          panel.grid.minor = element_blank(),\n          panel.border = element_blank(),\n          panel.background = element_blank())+\n          facet_wrap(~Hospital_Category_RiskAdjustment)+\n    xlab(\"Days\")+ylab(\"Cases\")+labs(caption=\"Produced by CF Lee\")\n\n\np + transition_time(as.integer(Year))+  \n  enter_fade() +\n  exit_fade()+labs(title = \"Year: {frame_time}\")\n```"
  },
  {
    "objectID": "posts/2024-09-01-Graphs-for-Communication/index.html#resources",
    "href": "posts/2024-09-01-Graphs-for-Communication/index.html#resources",
    "title": "Graphs for Communication - Creating Animated Maps",
    "section": "Resources",
    "text": "Resources\n\ngganimate link\nCalifornia Hospitals data."
  },
  {
    "objectID": "posts/2024-08-01-tying-the-data-together/index.html",
    "href": "posts/2024-08-01-tying-the-data-together/index.html",
    "title": "Tying the data together",
    "section": "",
    "text": "Have you ever get tired of typing functions into vlookup? In this post, we will use tidyverse in R to modify, combine, search, and merge several datasets. We will also create a shiny app allowing users to download the organized data."
  },
  {
    "objectID": "posts/2024-08-01-tying-the-data-together/index.html#background-challenges",
    "href": "posts/2024-08-01-tying-the-data-together/index.html#background-challenges",
    "title": "Tying the data together",
    "section": "Background & challenges",
    "text": "Background & challenges\nRecently, we reviewed our operational costs and realized that a significant portion was going toward credit card processing fees. After some thorough research and negotiation, we secured a better deal with a new merchant service provider. This switch saved us about $1,500 USD per month, and we also qualified for next-day funding—a major win for our business.\nHowever, our bookkeeper quickly spotted an issue: the new provider’s reporting system was much less straightforward. Unlike our previous provider, which offered a master report with detailed order and funding information, the new service splits this data into three separate reports: Sales, Transaction, and Funded. To make matters worse, some data points, like order details, are labeled differently across the reports (e.g., order.ifo in Sales versus order_detail in Transaction). Additionally, a single order could have multiple funding references, making it a headache to track everything accurately.\n\n\n\nDifferent column names and multiple funded entries for one order"
  },
  {
    "objectID": "posts/2024-08-01-tying-the-data-together/index.html#actions",
    "href": "posts/2024-08-01-tying-the-data-together/index.html#actions",
    "title": "Tying the data together",
    "section": "Actions",
    "text": "Actions\nTo fully benefit from the cost savings and stick with the new provider, I decided to tackle this problem head-on using R and Shiny.\nOne of our goals was to create a master statement with all the action organized in one report as below. To do this, the steps are as below:\n\nWe first re-organized the raw report by renaming the column names.\nSkipped the data where it is empty or contains not useful information.\nData merge based on the same column names.\n\n\n\n\nDesired master report\n\n\n\n\n\n\n\n\nExpand for Code\n\n\n\n\n\n\nlibrary(shiny)\n\n# identify columns appear in both data sets\nrenderUI({\n    var_names &lt;- intersect(names(data_a()), names(data_b()))\n    selectInput(\"var_ab\", \"Select merge variable for Transaction and Sales\", choices = var_names)\n  })\n\n\n\n# merge data \n  merged_data &lt;- reactive({\n    req(data_a(), data_b(), data_c(), input$var_ab, input$var_bc)\n    temp_ab &lt;- dplyr::full_join(data_a(), data_b(), by = input$var_ab)\n    unique(dplyr::full_join(temp_ab, data_c(), by = input$var_bc))\n  })\n\n\n\n\nI also wanted to save time for our bookkeeper as they often need to cross-check the internal documents. This step takes time to find the correct range of the data, and input Vlookup functions before finding and labeling the matches in Excel. To streamline the process and to reduce the manual input errors, I replicated this function in the app (full code as below and on github).\nThe output result is shown below. This example shows that the order 891631 with two different retrieval references were received, the transactions were successful ,and the amount was deposited into the account. These records (114,115) also appeared in the uploaded internal file.\n\n\n\n\n\n\n\nExpand for Code\n\n\n\n\n\n\nreactive({\n    req(merged_data(),data_f())\n    dt_org &lt;- dplyr::full_join(merged_data(),data_f(), by = input$var_checklist)\n    dt_org &lt;- dt_org |&gt;\n        select(`Date and Time`,`Transaction Date`,`Transaction Count`,`Merchant Reference Number`,`Summary          Transaction Slip`,Retrieval_Ref,\n              `Transaction Gross`,`Transaction Net`,`Gross Amount`,`Net Amount`,Currency,`Reversal Flag`,\n               `Transaction Type`)\n    counts &lt;- dt_org |&gt;\n      filter(`Summary Transaction Slip` != \"\")|&gt;\n      group_by(`Summary Transaction Slip`) |&gt;\n      mutate(Check_merchantID = list(`Merchant Reference Number`), count = n())|&gt;\n      mutate(Check_Retrieval_Ref = list(Retrieval_Ref), countx = n())|&gt;\n      mutate(has_Summary_Transaction_Slip = \"Yes\")|&gt;\n      select(-count, -countx)\n\n    not_in_counts &lt;- dt_org |&gt;\n      filter(is.na(`Summary Transaction Slip`))|&gt;\n      mutate(Check_merchantID = list(0))|&gt;\n      mutate(Check_Retrieval_Ref = list(0))|&gt;\n      mutate(has_Summary_Transaction_Slip = \"No\")\n     counts_summary &lt;- apply(rbind(counts,not_in_counts),2,as.character)\n     unique(counts_summary)\n  })"
  },
  {
    "objectID": "posts/2024-08-01-tying-the-data-together/index.html#results",
    "href": "posts/2024-08-01-tying-the-data-together/index.html#results",
    "title": "Tying the data together",
    "section": "Results",
    "text": "Results\nAfter the data merge is done, the files are ready to download to the local drive by pressing the Download buttons. This customized shiny app allows our accounting staff to run and download the report anytime/anywhere.\n\nBy developing a custom solution, I was able to streamline the reporting process, merge the data consistently, and keep our operations running smoothly—all while cutting down on expenses.\nFull code on Github\n\n\n\n\n\n\nExpand for Full Code\n\n\n\n\n\n\nlibrary(shiny)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\nlibrary(readxl)\nlibrary(readr)\nlibrary(DT)\n\n\nAttaching package: 'DT'\n\n\nThe following objects are masked from 'package:shiny':\n\n    dataTableOutput, renderDataTable\n\nui &lt;- fluidPage(\n  titlePanel(\"File Merger\"),\n  sidebarLayout(\n    sidebarPanel(\n      p(\"1. Convert Retrieval Reference Number to General\"),\n      fileInput(\"file_a\", \"Transaction file (CSV or XLSX)\",\n                accept = c(\".csv\", \".xlsx\")),\n      p(\"2a. Convert Retrieval Ref to General\"),\n      p(\"2b. Remove all rows above Merchant ID \"),\n      fileInput(\"file_b\", \"Sales (CSV or XLSX)\",\n                accept = c(\".csv\", \".xlsx\")),\n      p(\"3. Remove all rows above Summary Transaction Slip\"),\n      p(\"   Output: Merged File; Auto-Checked File \"),\n      fileInput(\"file_c\", \"Funded Deposit (CSV or XLSX)\",\n                accept = c(\".csv\", \".xlsx\")),\n      p(\"------------------------------------------\"),\n      p(\"4. (Optional) Upload a payportal list. Output: Manual-Checked File\"),\n      p(\"Must contain a column called **Merchant Reference Number**!\"),\n      checkboxInput(\"checklist\", \"Manual Check\", value = FALSE),\n      fileInput(\"file\", \"Checklist (CSV or XLSX)\", accept = c(\".csv\", \".xlsx\")),\n      \n      uiOutput(\"merge_vars_a_b\"),\n      uiOutput(\"merge_vars_b_c\"),\n      uiOutput(\"merge_vars_checklist\"),\n      br(),\n      #p(\"------------------------------------------\"),\n      p(\"A: Contains Summary Transaction Slip & Retrival Ref (matched & unmatched)\"),\n      downloadButton(\"downloadData\", \"Download Merged File\"),\n      #p(\"------------------------------------------\"),\n      p(\"B: Contains non-empty Summary Transaction Slip, count matched Retrival Ref & Merchant ref\"),\n      downloadButton(\"exportData\",   \"Download Auto-Checkeds File\"),\n      #p(\"------------------------------------------\"),\n      p(\"C: Using uploaded checklist to match and count non-empty Summary Transaction Slip, Retrival Ref & Merchant ref\"),\n      downloadButton(\"manual_input\", \"Download Manual-Checked File\")\n    ),\n    mainPanel(\n      tabsetPanel(\n        tabPanel(\"Transaction List\", DT::dataTableOutput(\"table_a\")),\n        tabPanel(\"Sales List\", DT::dataTableOutput(\"table_b\")),\n        tabPanel(\"Funded Deposit List\", DT::dataTableOutput(\"table_c\")),\n        tabPanel(\"Merged File\", DT::dataTableOutput(\"table_d\")),\n        tabPanel(\"Auto-Checked File\", DT::dataTableOutput(\"table_e\")),\n        tabPanel(\"Checklist\", DT::dataTableOutput(\"table_f\")),\n        tabPanel(\"Manual-Checked File\", DT::dataTableOutput(\"table_g\"))\n      )\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  \n  read_data &lt;- function(file) {\n    if (is.null(file)) {\n      return(NULL)\n    }\n    ext &lt;- tools::file_ext(file$name)\n    if (ext == \"csv\") {\n      df &lt;- read_csv(file$datapath)\n    } else if (ext == \"xlsx\") {\n      df &lt;- read_excel(file$datapath)\n    } else {\n      return(NULL)\n    }\n    return(df)\n  }\n  \n  data_a &lt;- reactive({\n    validate(\n      need(input$file_a != \"\", label = \"data set\")\n    )\n    \n    da &lt;- read_data(input$file_a)%&gt;%\n      mutate(Retrieval_Ref = `Retrieval Reference Number`)%&gt;% \n      select(`Date and Time`, Retrieval_Ref,`Merchant Reference Number`,Amount) \n    return(da)\n    \n  })\n  \n  # remove all rows above Merchant ID and change \n  data_b &lt;- reactive({\n    req(input$file_b)\n    db &lt;- read_data(input$file_b)%&gt;% \n      mutate(Retrieval_Ref = `Retrieval Ref.`)%&gt;% \n      select(`Transaction Date`, Retrieval_Ref, `Summary Transaction Slip`,\n             `Payment Transaction Slip`, `Transaction Gross`,`Transaction Net`)\n    db \n  })\n  \n  \n  # remove all rows above Summary Transaction Slip\n  data_c &lt;- reactive({\n    req(input$file_c)\n    dc &lt;- read_data(input$file_c)\n    dc&lt;- dc[!grepl('Payment', dc$`Summary Transaction Slip`)&\n              !grepl('Sale', dc$`Summary Transaction Slip`)&\n              !grepl('/', dc$`Summary Transaction Slip`)&\n              !grepl('Summary', dc$`Summary Transaction Slip`),]\n    dc$`Summary Transaction Slip` &lt;-  as.numeric(dc$`Summary Transaction Slip`)\n    \n    #colnames(dc)[2:7] &lt;-  c(\"Summary Transaction Slip\",\"Transaction Type\",\"Transaction Count\", \"Reversal Flag\",\n    #                   \"Currency\",\"Gross Amount\", \"Net Amount\")\n    return(dc) \n  })\n  \n  output$merge_vars_a_b &lt;- renderUI({\n    #req(data_a(), data_b())\n    validate(\n      need(input$file_a != \"\", label = \"data set\")\n    )\n    var_names &lt;- intersect(names(data_a()), names(data_b()))\n    selectInput(\"var_ab\", \"Select merge variable for Transaction and Sales\", choices = var_names)\n  })\n  \n  output$merge_vars_b_c &lt;- renderUI({\n    #req(data_b(), data_c())\n    validate(\n      need(input$file_a != \"\", label = \"data set\")\n    )\n    #get(input$file_a, 'package:datasets')\n    var_names &lt;- intersect(names(data_b()), names(data_c()))\n    selectInput(\"var_bc\", \"Select merge variable for Sales and Funded Depos it\", choices = var_names)\n  })\n  \n  \n  output$merge_vars_checklist &lt;- renderUI({\n    validate(\n      need(input$checklist != FALSE, label = \"data set\")\n    )\n    #get(input$file_a, 'package:datasets')\n    var_names &lt;- intersect(names(merged_data()), names(data_f()))\n    selectInput(\"var_checklist\", \"Select merge variable for Merge data and Checklist\", choices = var_names)\n  })\n  data_f &lt;- reactive({\n    validate(\n      need(input$file != \"\", label = \"data set\")\n    )\n    dt &lt;- read_data(input$file)\n    return(dt)\n  })\n  \n  output$table_a &lt;- DT::renderDataTable({\n    #req(data_a())\n    DT::datatable(data = unique(data_a()),\n                  options = list(pageLength = 100, rownames = FALSE) \n    )  \n  })\n  \n  output$table_b &lt;- DT::renderDataTable({\n    #req(data_b())\n    DT::datatable(data = unique(data_b()),\n                  options = list(pageLength = 100, rownames = FALSE) \n    )  \n  })\n  \n  output$table_c &lt;- DT::renderDataTable({\n    #req(data_c())\n    DT::datatable(data = unique(data_c()),\n                  options = list(pageLength = 100, rownames = FALSE) \n    )  \n  })\n  \n  merged_data &lt;- reactive({\n    req(data_a(), data_b(), data_c(), input$var_ab, input$var_bc)\n    temp_ab &lt;- dplyr::full_join(data_a(), data_b(), by = input$var_ab)\n    unique(dplyr::full_join(temp_ab, data_c(), by = input$var_bc))\n  })\n  \n  processed_data &lt;- reactive({\n    req(merged_data())\n    df &lt;-\n      merged_data() %&gt;%\n      filter(`Summary Transaction Slip` != \"\")%&gt;%\n      group_by(`Summary Transaction Slip`) %&gt;%\n      mutate(Check_merchantID = list(`Merchant Reference Number`), count = n())%&gt;%\n      mutate(Check_Retrieval_Ref = list(Retrieval_Ref), countx = n())%&gt;%\n      select(`Date and Time`,`Transaction Date`,`Transaction Count`,`Merchant Reference Number`,`Summary Transaction Slip`,Retrieval_Ref,\n             `Check_merchantID`,Check_Retrieval_Ref,`Transaction Gross`,`Transaction Net`,`Gross Amount`,`Net Amount`,Currency,`Reversal Flag`,\n             `Transaction Type`)\n    apply(df,2,as.character)\n    \n  })\n  \n  manual_check &lt;-reactive({\n    req(merged_data(),data_f())\n    dt_org &lt;- dplyr::full_join(merged_data(),data_f(), by = input$var_checklist)\n    dt_org &lt;- dt_org |&gt;\n        select(`Date and Time`,`Transaction Date`,`Transaction Count`,`Merchant Reference Number`,`Summary Transaction Slip`,Retrieval_Ref,\n              `Transaction Gross`,`Transaction Net`,`Gross Amount`,`Net Amount`,Currency,`Reversal Flag`,\n               `Transaction Type`)\n\n    counts &lt;- dt_org |&gt;\n      filter(`Summary Transaction Slip` != \"\")|&gt;\n      group_by(`Summary Transaction Slip`) |&gt;\n      mutate(Check_merchantID = list(`Merchant Reference Number`), count = n())|&gt;\n      mutate(Check_Retrieval_Ref = list(Retrieval_Ref), countx = n())|&gt;\n      mutate(has_Summary_Transaction_Slip = \"Yes\")|&gt;\n      select(-count, -countx)\n\n    not_in_counts &lt;- dt_org |&gt;\n      filter(is.na(`Summary Transaction Slip`))|&gt;\n      mutate(Check_merchantID = list(0))|&gt;\n      mutate(Check_Retrieval_Ref = list(0))|&gt;\n      mutate(has_Summary_Transaction_Slip = \"No\")\n\n     #counts_summary &lt;- rbind(counts,not_in_counts)\n     counts_summary &lt;- apply(rbind(counts,not_in_counts),2,as.character)#rbind(apply(counts,2,as.character),apply(not_in_counts,2,as.character))\n    \n    \n     unique(counts_summary)\n    \n  })\n  \n  output$table_d &lt;- DT::renderDataTable({\n    req(merged_data())\n    DT::datatable(data = unique(merged_data()),\n                  options = list(pageLength = 100, rownames = FALSE) \n    )  \n  })\n  \n  output$table_e &lt;- DT::renderDataTable({\n    req(merged_data())\n    DT::datatable(data = unique(processed_data()),\n                  options = list(pageLength = 100, rownames = FALSE) \n    )  \n  })\n  \n  # Render the data table only if the checkbox is checked\n  output$table_f &lt;- renderDT({\n    req(input$checklist)\n    #manual_check()\n    \n    DT::datatable(data = unique(data_f()),\n                  options = list(pageLength = 100, rownames = FALSE)\n    )\n  })\n  \n  output$table_g &lt;- renderDT({\n    req(input$checklist)\n    \n    DT::datatable(data = unique(manual_check()),\n                  options = list(pageLength = 100, rownames = FALSE)\n    )\n  })\n  \n  ######### Download ############\n  output$downloadData &lt;- downloadHandler(\n    filename = function() {\n      paste0(Sys.Date(), \"merged_data.csv\")\n    },\n    content = function(file) {\n      write.csv(unique(merged_data()), file, row.names = FALSE)\n    }\n  )\n  \n  \n  output$exportData &lt;- downloadHandler(\n    filename = function() {\n      paste0(Sys.Date(), \"auto_checked.csv\")\n    },\n    content = function(file) {\n      write.csv(unique(processed_data()), file, row.names = FALSE)\n    }\n  )\n  \n  output$manual_input &lt;- downloadHandler(\n    filename = function() {\n      paste0(Sys.Date(), \"manual_checked.csv\")\n    },\n    content = function(file) {\n      write.csv(unique( manual_check()), file, row.names = FALSE)\n    }\n  ) \n  \n}\nshinyApp(ui, server)\n\nShiny applications not supported in static R Markdown documents"
  },
  {
    "objectID": "posts/2024-06-25-create-a-Quarto-dashboard/index.html",
    "href": "posts/2024-06-25-create-a-Quarto-dashboard/index.html",
    "title": "Creating a Serveless Dashboard",
    "section": "",
    "text": "A while ago I wrote about creating a serverless dashboard (with no server dependencies) using Flexdashboard. As some of the functions now are no longer supported, I found a good alternative called “Quarto Dashboards”. Quarto Dashboards allow to create dashboards using Python, R, Julia, and Observable. More details are here.\nDashboards can be created either using Jupyter notebooks (.ipynb) or using plain text markdown (.qmd). I am showing how to create one using RStudio.\n\nClick File -&gt; New File -&gt; Quarto Document\n\n\n\nA new .qmd file is created\n\nHere is the code for the visual version of the dashboard link\nHere is the plain text .qmd version of the dashboard.\n\nThe document options define the title and author for the navigation bar as well as specifying the use of the dashboard format.\nBy default, dashboard pages are laid out first by row, then by column. In this demo, I changed this by specifying the orientation: columns document option:\n\ntitle: \"Marketing Campaign at a Glance\"\nauthor:\n    name: Chris Lee\n    url: https://clfee.github.io/\ndate: 2024-07-15\nformat: \n  dashboard:\n    orientation: columns\n    nav-buttons: [github]\n    github: https://github.com/clfee\nlogo: \"/assets/images/cmm1.PNG\"\ntheme: custom.scss\neditor_options: \n  chunk_output_type: console\n\nEach row in the dashboard that is not given an explicit height will determine its size by either filling available space or by flowing to its natural height.Here I changed the figure height by specifying the #| fig-height: 6 \n\n#| title: Correlations\n#| fig-height: 6\n#| padding: 0;\ndt &lt;- train\nMore details about full code.\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2021-08-31-malaria-cell-classification/index.html",
    "href": "posts/2021-08-31-malaria-cell-classification/index.html",
    "title": "Image Classification (Part1)",
    "section": "",
    "text": "I remember years ago seeing my colleague spent hours under a microscopes counting cells underwent of apoptosis or Dauer larva formation. I mean it is fun doing experiments in the lab but telling differences of these tiny worms would probably is the last thing I’d want to do. This task does take lots of valuable time from a researcher. Imagine, how many more novel anti-agents like this article Yongsoon could bring us if the deep learning techniques were ready to use back in 2011.\n\n\n\nKim Y, Sun H (2012) PLOS ONE 7(9): e45890\n\n\nThanks to the advancement in deep learning field, neural network model architectures can be readily reused and, in most cases, are tested across multiple applications to establish robustness. Here, I’m going to show how easy it is to implement transfer learning using Keras in Python for Malaria cell classification. The basic concept of transfer learning is using the knowledge (architecture or weights) gained from a neural network model that was trained to recognize animals to recognize cats. The dataset used here came from NIH, along with recent publications1,2.\n\nWorkflow\n\nLoading data and data pre-processing\nTransfer learning and fine-tuning (DenseNet121)\nResult evaluation\n\n\n\n\nData Overview\nThere are many ways to create train/valid/test data sets. Below is one of the methods using R to create csv files containing file paths and classifications from train and test folders.\n# R code\nlibrary(fs)\ndataset_dir &lt;- \"Data/cell_images/\"\ntest_dir   &lt;- paste0(dataset_dir, \"/test/\")\n# stored image paths in the image column\ntest_parasite &lt;- dir_ls(path=paste0(test_dir, \"parasite\"),glob = \"*.png\")\ntest_uninfected &lt;- dir_ls(path=paste0(test_dir, \"uninfected\"),glob = \"*.png\")\ntest_par &lt;- as.data.frame(matrix('parasite', length(test_parasite), 1))\ntest_unin &lt;- as.data.frame(matrix('uninfected', length(test_parasite), 1))\ntest_par$image &lt;- test_parasite \ntest_unin$image &lt;- test_uninfected\n\ntest &lt;- rbind(test_par,test_unin)\ncolnames(test)[1] &lt;- 'label'\ntest$normal  &lt;- ifelse(test$label != 'parasite', 1,0)\ntest$parasite &lt;- ifelse(test$label == 'parasite', 1,0)\nAnd the csv file looks like this. \nIn reality, we don’t usually see many cells infected with parasites, therefore less than 1/3 of the infected samples were used in this exercise.\n# Python\n# get ids for each label\nall_img_ids = list(new_df.uninfected.index.unique())\ntrain_ids, test_ids = train_test_split(all_img_ids, test_size=0.01, random_state=21)\ntrain_ids, valid_ids = train_test_split(train_ids, test_size=0.1, random_state=21)\nMaking sure, the proportion of the infected cell is as expected after data split. \nLet’s also check few images. The images come with different sizes. They will need to reshape and normalize before xx.\n# Extract numpy values from image column in data frame\ntrain_df = new_df.iloc[train_ids,:]\nimages = train_df['image'].values\n# Extract 9 random images \nrandom_images = [np.random.choice(images) for i in range(9)]\nimg_dir = 'C:/Users/your_image_folder'\nprint('Display Random Images')\n# Adjust the size of your images\nplt.figure(figsize=(20,10))\nfor i in range(9):\n    plt.subplot(3, 3, i + 1)\n    img = plt.imread(os.path.join(img_dir, random_images[i]))\n    plt.imshow(img)\n    plt.axis('off')\n\n\n\nrandom_images\n\n\n\n\nLoading data\nNext is building generators from the Keras framework. The purpose of building generator is that it allows to generate batches of tensor image data with real-time data augmentation(ex: random horizontal flipping of images). We also use the generator to transform the values in each batch so that their mean is 0 and their standard deviation is 1.Here is the information of ImageDataGenerator and a short tutorial. We’ll also need to build a sereperate generator for valid and test sets. Since each image will be normailized using mean and standard deviation derived from its own batch. In a real life scenario, we process one image at a time. And the incoming image is normalized using the statistics computed from the training set.\n# Train generator\ndef get_train_generator(df, image_dir, x_col, y_cols, shuffle=True, batch_size=8, seed=1, target_w = 224, target_h = 224):\n    \"\"\"\n\n    Args:\n      train_df (dataframe): dataframe specifying training data.\n      image_dir (str): directory where image files are held.\n      x_col (str): name of column in df that holds filenames.\n      y_cols (list): list of strings that hold y labels for images.\n      sample_size (int): size of sample to use for normalization statistics.\n      batch_size (int): images per batch to be fed into model during training.\n      seed (int): random seed.\n      target_w (int): final width of input images.\n      target_h (int): final height of input images.\n    \n    Returns:\n        train_generator (DataFrameIterator): iterator over training set\n    \"\"\"        \n    print(\"getting train generator...\") \n    # normalize images\n    image_generator = ImageDataGenerator(\n        samplewise_center=True,\n        samplewise_std_normalization= True)\n    \n    # flow from directory with specified batch size and target image size\n    generator = image_generator.flow_from_dataframe(\n            dataframe=df,\n            directory=image_dir,\n            x_col=x_col,\n            y_col=y_cols,\n            class_mode=\"raw\",\n            batch_size=batch_size,\n            shuffle=shuffle,\n            seed=seed,\n            target_size=(target_w,target_h))\n    \n    return generator  \nBefore, model building we’ll need to define a loss function to address class imbalance. We can give more weight for the less frequent class and less weight for the other one, see here . We can write the overall average cross-entropy loss over the entire training set D of size N as follows:\n\n\n\nloss\n\n\nNext, we will use a pre-trained DenseNet121 model which we can load directly from Keras and then add two layers on top of it.\n\nSet include_top=False, to remove the orginal fully connect dense layer (so you can adjust the ouptut prediction clsses or\nactivation function).\nUse specific layer using get_layer(). For example: base_model.get_layer(‘conv5_block16_conv’)\n\nA GlobalAveragePooling2D layer to get the average of the last convolution layers from DenseNet121. The pooling layer typically uses a filter to extract representative features (e.g., maximum, average, etc.) for different locations. The method of extracting features from the pooling filter is called a pooling function. The commonly used pooling functions include the maximum pooling function, average pooling function, L2 normalization, and weighted average pooling function based on the distance from the center pixel. In short, the pooling layer summarizes all the feature information centered on each position of the input feature map, which makes it reasonable that the output data of the pooling layer is less than the input data. This method reduces the input data to the next layer and improves the computational efficiency of the CNN.\nThe output of the pooling layer is flattening to convert the pooled features maps into a single dimensional array. This is done in order for the data to be fed into densely connected hidden layers.\nA Dense layer with sigmoid activation to get the prediction logits for each of our classes. We can set our custom loss function for the model by specifying the loss parameter in the compile() function.\n# Build model\ndef create_dense121_model():\n    \n    pretrained = 'fine_tuned.hdf5'\n    train_df = pd.read_csv(\"train_df.csv\")\n    labels = ['uninfected', 'parasite']  \n    \n    class_pos = train_df.loc[:, labels].sum(axis=0)\n    class_neg = len(train_df) - class_pos\n    class_total = class_pos + class_neg\n\n    pos_weights =  class_pos / class_total #[0.5,class_pos / class_total]\n    neg_weights =  class_neg / class_total #[0.5,class_neg / class_total]\n    print(\"Got loss weights\")\n    \n    def create_model(input_shape=(224, 224,3)):\n        base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=input_shape)\n       \n        # add a global spatial average pooling layer\n        x = GlobalAveragePooling2D()(base_model.output)\n        x = Flatten()(x)\n        x = Dense(1024, activation='relu', name='dense_post_pool')(x)\n        x = Dropout(0.8)(x)\n        # output has two neurons for the 2 classes (uninfected and parasite)\n        predictions = Dense(len(labels), activation='sigmoid')(x)\n        model = Model(inputs = base_model.input, outputs = predictions)\n\n        # freeze the earlier layers\n        for layer in base_model.layers[:-4]:\n            layer.trainable=False\n        \n        return model\n    \n    def get_weighted_loss(neg_weights, pos_weights, epsilon=1e-7):\n        def weighted_loss(y_true, y_pred):\n            y_true = tf.cast(y_true, tf.float32)\n            #print(f'neg_weights : {neg_weights}, pos_weights: {pos_weights}')\n            #print(f'y_true : {y_true}, y_pred: {y_pred}')\n            # L(X, y) = −w * y log p(Y = 1|X) − w *  (1 − y) log p(Y = 0|X)\n            # from https://arxiv.org/pdf/1711.05225.pdf\n            loss = 0\n            \n            for i in range(len(neg_weights)):\n                loss -= (neg_weights[i] * y_true[:, i] * K.log(y_pred[:, i] + epsilon) + \n                         pos_weights[i] * (1 - y_true[:, i]) * K.log(1 - y_pred[:, i] + epsilon))\n            \n            loss = K.sum(loss)\n            return loss\n        return weighted_loss\n    \n   \n    model = create_model()\n    model.load_weights(pretrained)\n    print(\"Loaded Model\")\n    \n    model.compile(optimizer='adam', loss= get_weighted_loss(neg_weights, pos_weights)) \n    print(\"Compiled Model\")   \n          \n    return model\nModel is fine tuned using ModelCheckpoint and only the model’s weights will be saved.\n# CallBack \n# -------------------------------------------------------------------------------------------------\n# Callback Function 1\nfname = 'dense121(V)_Epoch[{epoch:02d}].ValLoss[{val_loss:.3f}].hdf5'\nfullpath = fname\n# https://keras.io/api/callbacks/model_checkpoint/\ncallback_func1 = ModelCheckpoint(filepath=fullpath,             \n                                monitor='val_loss',             \n                                verbose=1,                      \n                                save_best_only=True,            \n                                save_weights_only=True, # save weights       \n                                mode='min',                     \n                                period=1)                       \n\n# Callback Function 2\n# https://keras.io/callbacks/#tensorboard\ncallback_func2 = keras.callbacks.TensorBoard(log_dir='./logs/log2', histogram_freq=1)\n\n# Callback Function\ncallbacks = []\ncallbacks.append(callback_func1)\ncallbacks.append(callback_func2)\n\n# Training and Plotting\n# -------------------------------------------------------------------------------------------------\nhistory = model.fit(train_generator, \n                              validation_data=valid_generator,\n                              steps_per_epoch=100, \n                              validation_steps=25, \n                              epochs = 15,\n                              callbacks=callbacks)\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.ylabel(\"loss\")\nplt.xlabel(\"epoch\")\nplt.title(\"Training Loss Curve\")\nplt.show()\nTrain versus validation loss for all epochs is shown here. The orange and blue lines indicate train loss and validation loss respectively. We can see the model may be under-fitted. One way to overcome this is simply increase the number of epochs. Also with the callback function, we can re-use the best weights saved at 12th epoch.\n\n\n\nhistory\n\n\n\n\nEvaluation\nThe ROC curve is created by plotting the true positive rate against the false positive rate. We can see the model performs reasonable well.\n\n\n\nROC\n\n\nWe can try different approaches to improve the model perfromance, such as train the model for a longer time or use all the training data (since only 1/3 of the parasite data was used). We can also try a different base model, the previous publication, shows 99.32% accuracy with VGG-19 alone.\n\n\nVisualize class activation maps\nNext, I will show how to produce visual explanation using Grad-CAM. The purpose of doing this is as following:\n\nDebug your model and visually validate that it is “looking” and “activating” at the correct locations in an image.\nGrad-CAM works by (1) finding the final convolutional layer in the network and then (2) examining the gradient information flowing into that layer.\n\n\n\nNotes\nNote 1: AUC is the area below these ROC curves. Therefore, in other words, AUC is a great indicator of how well a classifier functions. Note 2: A good tutorial for to learn neural network image classification from scratch and Andrew Ng’s deep learning course.\nNote 3:\n# packages used \nimport os\nimport sklearn\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport keras\nfrom keras.applications.densenet import DenseNet121\nfrom keras.models import Model\nfrom keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization, GlobalAveragePooling2D\nfrom keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, TensorBoard\nfrom keras import backend as K\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import ImageDataGenerator\n\n\n\n\n Back to top"
  },
  {
    "objectID": "gallery.html",
    "href": "gallery.html",
    "title": "Chris Lee",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nGraphs for Communication - Creating Animated Maps\n\n\n\n\n\n\n\n\n\n\n\n2024-09-01\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nAutomated Custom reports - parameterized reporting\n\n\n\n\n\n\n\n\n\n\n\n2024-08-16\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nTying the data together\n\n\n\n\n\nEffortless data combine, merge ,and search. \n\n\n\n\n\n2024-08-01\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\nMarketing Campaign at a Glance\n\n\n\n\n\n\n\n\n\n\n\n2024-07-15\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nCreating a Serveless Dashboard\n\n\n\n\n\nSharing results is easy! \n\n\n\n\n\n2024-06-25\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nImage Classification (Part2)\n\n\n\n\n\nSave time & Money! \n\n\n\n\n\n2021-08-31\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nImage Classification (Part1)\n\n\n\n\n\nSave time & Money! \n\n\n\n\n\n2021-08-31\n\n\n14 min\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "assets/other/tags-archive.html",
    "href": "assets/other/tags-archive.html",
    "title": "Posts by Tags",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "\nHello\n",
    "section": "",
    "text": "Hello\n\n\n\n\n\n\n\n\nI’m driven by helping others successfully solve problems like turning complex data into clear, actionable insights that help businesses thrive. My toolkit to deliver these solutions draws on blending business understanding with machine learning using SQL, R, SAS, and Python. I’m passionate about using data to solve real-world problems, streamline processes, and find new growth opportunities. Balancing the technical side with a strong understanding of business needs, I create solutions that make a real impact and keep things moving forward.\nFeel free to reach out to me on LinkedIn or drop me an email to discuss.\n\n\n\n\n\nEducation:\n\nUniversity of Manchester | UK PhD in Cancer Studies | Sept 2004 - Jan 2009\nNational Yang-Ming University | Taipei, Taiwan Msc in Genetics| Sept 2001 - Sept 2003\n\nExperiences:\n\nFUJI WEDDING VENUE GROUP | Senior Data Analyst | Jan 2023 - PRESENT\nFUJI WEDDING VENUE GROUP | Data Analyst | Jan 2016 - Dec 2022\nProtea | Senior Scientist | Jan 2014 - Mar 2015\n\n\n\n\n Back to top"
  },
  {
    "objectID": "assets/other/sitemap.html",
    "href": "assets/other/sitemap.html",
    "title": "Website Structure",
    "section": "",
    "text": "Pages\n\n{% for post in site.pages %} {% include archive-single.html %} {% endfor %}\n\nPosts\n\n{% for post in site.posts %} {% include archive-single.html %} {% endfor %}\n{% capture written_label %}‘None’{% endcapture %}\n{% for collection in site.collections %} {% unless collection.output == false or collection.label == “posts” %} {% capture label %}{{ collection.label }}{% endcapture %} {% if label != written_label %}\n\n{{ label }}\n\n{% capture written_label %}{{ label }}{% endcapture %} {% endif %} {% endunless %} {% for post in collection.docs %} {% unless collection.output == false or collection.label == “posts” %} {% include archive-single.html %} {% endunless %} {% endfor %} {% endfor %}\n\n\n\n Back to top"
  },
  {
    "objectID": "assets/other/year-archive.html",
    "href": "assets/other/year-archive.html",
    "title": "Posts by Year",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Chris Lee",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "posts/2021-08-31-malaria-cell-classification - p2/index.html",
    "href": "posts/2021-08-31-malaria-cell-classification - p2/index.html",
    "title": "Image Classification (Part2)",
    "section": "",
    "text": "When building a predictive model, there are two important criteria: predictive accuracy and interpretability, which generally have a trade-off relationship. In the previous post, we have shown that the DenseNet121 model can achieve high accuracy in detecting cells infected with parasites.\nHere, I am going to introduce a powerful technique GRAD-CAM (gradient-weighted class activation mapping) to visualize which parts of an image are most important to the predictions of an image regression network. GRAD-CAM is a generalization of the CAM technique which determines the importance of each neuron in a network prediction by considering the gradients of the target flowing through the deep network. Unlike CAM which requires a particular kind of CNN architecture to perform global average pooling prior to prediction and forces us to change the base model retrain the network. In contrast, GRAD-CAM is accessing intermediate activations in the deep learning model and computing gradients with respect to the class output. For more details, please see.\nWorkflow: - Obtain predicted class/index - Determine which intermediate layer(s) to use. Lower-level convolution layers capture low level features such as edges, and lines. Higher-level layers usually have more abstract information. - Calculate the gradients with respect to the outout of the class/index - Generate a heatmap by weighing the convolution outputs with the computed gradients - Super-impose the heatmap to the original image\nLoad base model\nWe first load the base model and will only train the last 4 layers.\ndef build_model(input_shape=(224, 224,3),pos_weights,neg_weights):\n  # load the base DenseNet121 model\n  base_model = DenseNet121(input_shape = input_shape, \n                      weights='imagenet', \n                      include_top=False)\n  \n  # add a GAP layer\n  output = layers.GlobalAveragePooling2D()(base_model.output)\n\n  # output has two neurons for the 2 classes (uninfected and parasite)\n  output = layers.Dense(2, activation='softmax')(output)\n\n  # set the inputs and outputs of the model\n  model = Model(base_model.input, output)\n\n  # freeze the earlier layers\n  for layer in base_model.layers[:-4]:\n      layer.trainable=False\n\n  # configure the model for training\n  model.compile(loss= get_weighted_loss(neg_weights, pos_weights), \n                optimizer=adam, \n                metrics=['accuracy'])\n  \n  return model\n \nWe then create a new model that has the original model’s inputs, but two different outputs. The first output contains the activation layers outputs that in this case is the final convolutional layer in the original model. And the second output is the model’s prediction for the image.\ndef get_CAM(model, processed_image, actual_label, layer_name): \n    \"\"\"\n    GradCAM method for visualizing input saliency.\n    \n    Args:\n        model (Keras.model): model to compute cam for\n        image (tensor): input to model, shape (1, H, W, 3)\n        cls (int): class to compute cam with respect to\n        layer_name (str): relevant layer in model\n        H (int): input height\n        W (int): input width\n    Return:\n        heatmap()\n    \"\"\"    \n\n    model_grad = Model([model.inputs], \n                       [model.get_layer(layer_name).output, model.output])\n    \n    with tf.GradientTape() as tape:\n        conv_output_values, predictions = model_grad(processed_image)\n\n        # assign gradient tape to monitor the conv_output\n        tape.watch(conv_output_values)\n        \n        # use binary cross entropy loss, actual_label = 0 if uninfected\n        # get prediction probability of infected  \n        pred_prob = predictions[:,1] \n        \n        # make sure actual_label is a float, like the rest of the loss calculation\n        actual_label = tf.cast(actual_label, dtype=tf.float32)\n        \n        # add a tiny value to avoid log of 0\n        smoothing = 0.00001 \n        \n        # Calculate loss as binary cross entropy\n        loss = -1 * (actual_label * tf.math.log(pred_prob + smoothing) + (1 - actual_label) * tf.math.log(1 - pred_prob + smoothing))\n        print(f\"binary loss: {loss}\")\n    \n    # get the gradient of the loss with respect to the outputs of the last conv layer\n    grads_values = tape.gradient(loss, conv_output_values)\n    grads_values = K.mean(grads_values, axis=(0,1,2))\n    \n    conv_output_values = np.squeeze(conv_output_values.numpy())\n    grads_values = grads_values.numpy()\n    \n    # weight the convolution outputs with the computed gradients\n    for i in range(grads_values.shape[-1]): \n        conv_output_values[:,:,i] *= grads_values[i]\n    heatmap = np.mean(conv_output_values, axis=-1)\n    \n    heatmap = np.maximum(heatmap, 0)\n    heatmap /= heatmap.max()\n    \n    del model_grad, conv_output_values, grads_values, loss\n   \n    return heatmap\n\n\n\nResult\n\n\nNote: Instead of using max pooling that only keeps the highest valued ones. Average pooling allows some of the lesser intensity pixels to pass on in the pooling layer. It is important as we look at the small size of the image once it reaches this layer, max pooling could leave us with very little information.\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/2024-08-16-automated-custom-reports/index.html",
    "href": "posts/2024-08-16-automated-custom-reports/index.html",
    "title": "Automated Custom reports - parameterized reporting",
    "section": "",
    "text": "Automatically generated reports, one after another from an R script!\nParameterized reporting is a technique that allows to generate multiple reports simultaneously. The technique also makes your work more accurate, as it avoids copy-and-paste errors.\nGoal :  The objective of this report is to demonstrate how to create automated custom reports with Quarto and Purrr in R to track patient days from public data.\nThe demo uses the data from California Health and Human Services Open Data Portal. All California general acute care hospitals are required to report Methicillin-resistant Staphylococcus aureus (MRSA) bloodstream infection (BSI) cases that occur following hospitalization. MRSA is a serious, contagious bacterial infection that starts on your skin. It’s a type of staph infection that resists most common antibiotics, making it especially dangerous. Without treatment, MRSA can be deadly."
  },
  {
    "objectID": "posts/2024-08-16-automated-custom-reports/index.html#first-step---single-report",
    "href": "posts/2024-08-16-automated-custom-reports/index.html#first-step---single-report",
    "title": "Automated Custom reports - parameterized reporting",
    "section": "First Step - single report",
    "text": "First Step - single report\nFirst, let’s prepare a single report as a template from the data in 2022.\n\nHealthcare Associated Infection Report, 2022\n\nResults\nFigure 1. Box plots of Standardized Infection Ratio (SIR) in 2022\n\n\nCode\n```{r}\n#| label: Boxplots\n#| code-fold: true\n#| warning: false\n\ncolor_palette &lt;- c(\"#167bb2\",\"#2e3b4f\",\"#7A6C5D\",\"#2e4f42\",\"#A54657\",\"#624151\",\"#16b29b\",\"#002c47\",\"#9d72d0\"\n)\n\n mrsa_combine|&gt;\n    filter(Year == params$year) |&gt;\n    filter(County %in% c(\"San Diego\" ,\"San Francisco\", \"Los Angeles\", \"Sacramento\", \"Orange\",\"Yolo\" ))|&gt;\n    ggplot(aes(x= Hospital_Type, y= SIR, fill=Hospital_Type))+\n     geom_boxplot()+\n    scale_fill_manual(values=color_palette)+\n       theme(\n      legend.position=\"none\",\n      plot.title = element_text(size=11),\n      axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)\n    ) +\n    ggtitle(paste0(\"Standardized Infection Ratio (SIR) in \",params$year)) +\n    xlab(\"\")+ylab(\"\")+labs(caption=\"Produced by CF Lee\")+\n   facet_wrap(~County)\n```\n\n\n\n\n\n\n\n\n\nFigure 2. Patients Days vs Reported infection cases in 2022\n\n\nCode\n```{r}\n#| code-fold: true\n#| warning: false\n\n mrsa_combine$Months &lt;- as.numeric(mrsa_combine$Months)\n\n mrsa_combine |&gt; \n    filter(Year == params$year) |&gt;\n    mutate(Quarters = case_when(\n                         Months &gt; 0 & Months &lt; 4 ~ 1,\n                         Months &gt; 3 & Months &lt; 7 ~ 2,\n                         Months &gt; 6 & Months &lt; 10 ~ 3 , \n                         Months &gt; 9 & Months &lt; 13 ~ 4 , \n                                .default = 4))|&gt;\n  # group_by(Quarters,Hospital_Category_RiskAdjustment, Year)|&gt;\n  #  summarise(mean = mean(na.omit(Infections_Reported)))|&gt; \n    \n    ggplot(aes(x= Patient_Days, y= Infections_Reported, \n               colour = as.integer(Quarters)))+\n     geom_point(alpha = 0.7, show.legend = FALSE) +\n\n    theme_bw()+\n    ggtitle(paste0(\"Patients Days vs Reported infection cases in \",params$year)) +\n    theme(#axis.text.x=element_blank(),\n          axis.text=element_text(size=12),\n          legend.position = \"bottom\",\n          axis.line = element_line(colour = \"black\"),\n          panel.grid.major = element_blank(),\n          panel.grid.minor = element_blank(),\n          panel.border = element_blank(),\n          panel.background = element_blank())+\n          facet_wrap(~Hospital_Category_RiskAdjustment)+\n    xlab(\"Days\")+ylab(\"Cases\")+labs(caption=\"Produced by CF Lee\")\n```\n\n\n\n\n\n\n\n\n\nNote: Patient Days is define as below. A count of the number of patients in the hospital during a time period, excluding IRFs and IPFs with their own CCN numbers. A rehabilitation unit (IRF) with its own CCN number reports separately from the rest of the hospital. To calculate patient days, the number of patients is recorded at the same time each day for each day of the month. At the end of the month, the daily counts are summed. See the NHSN website: http://www.cdc.gov/nhsn."
  },
  {
    "objectID": "posts/2024-08-16-automated-custom-reports/index.html#second-step--all-variations-at-once",
    "href": "posts/2024-08-16-automated-custom-reports/index.html#second-step--all-variations-at-once",
    "title": "Automated Custom reports - parameterized reporting",
    "section": "Second Step- All variations at once",
    "text": "Second Step- All variations at once\nOnce you are done with the single report. It is time to render all report variations. To do this, write an R script to render the parameterized Quarto template with each of the defined parameters.\nIn our example the code looks like this,\n# 1. First create a dataframe \n\ndata &lt;- expand.grid(\n  year = c(2019,2021:2023),\n  record_date = Sys.Date(), \n  stringsAsFactors = FALSE)\n\ndf &lt;- data |&gt; \n  dplyr::mutate(\n    output_format = \"html\",       # Output format (html, word, etc.)\n    output_file = paste(          # Output file name\n      year, \"report.html\",\n      sep = \"-\"\n    ),\n    execute_params = purrr::map2( # Named list of parameters\n      record_date, year, \n      \\(record_date, year) list(record_date = record_date, year = year)\n    )\n  ) |&gt; \n  dplyr::select(-c(record_date, year))\n\ndf\n\n# 2. Use purrr::pwalk() to map over each row of the dataframe and render each report variation.\n\npurrr::pwalk(\n  .l = df,                      # Dataframe to map over\n  .f = quarto::quarto_render,   # Quarto render function\n  input = \"demo_autoreport.qmd\",       # Named arguments of .f\n  .progress = TRUE              # Optionally, show a progress bar\n)"
  },
  {
    "objectID": "posts/2024-08-16-automated-custom-reports/index.html#resources",
    "href": "posts/2024-08-16-automated-custom-reports/index.html#resources",
    "title": "Automated Custom reports - parameterized reporting",
    "section": "Resources",
    "text": "Resources\n\nThe raw data is obtained from California Health and Human Services Open Data Portal datasets\nFull code link"
  }
]