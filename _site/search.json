[
  {
    "objectID": "posts/2021-08-31-malaria-cell-classification/index.html",
    "href": "posts/2021-08-31-malaria-cell-classification/index.html",
    "title": "Image Classification (Part1)",
    "section": "",
    "text": "I remember years ago seeing my colleague spent hours under a microscopes counting cells underwent of apoptosis or Dauer larva formation. I mean it is fun doing experiments in the lab but telling differences of these tiny worms would probably is the last thing I’d want to do. This task does take lots of valuable time from a researcher. Imagine, how many more novel anti-agents like this article Yongsoon could bring us if the deep learning techniques were ready to use back in 2011.\n\n\n\nKim Y, Sun H (2012) PLOS ONE 7(9): e45890\n\n\nThanks to the advancement in deep learning field, neural network model architectures can be readily reused and, in most cases, are tested across multiple applications to establish robustness. Here, I’m going to show how easy it is to implement transfer learning using Keras in Python for Malaria cell classification. The basic concept of transfer learning is using the knowledge (architecture or weights) gained from a neural network model that was trained to recognize animals to recognize cats. The dataset used here came from NIH, along with recent publications1,2.\n\nWorkflow\n\nLoading data and data pre-processing\nTransfer learning and fine-tuning (DenseNet121)\nResult evaluation\n\n\n\n\nflow1x\n\n\n\n\nData Overview\nThere are many ways to create train/valid/test data sets. Below is one of the methods using R to create csv files containing file paths and classifications from train and test folders.\n# R code\nlibrary(fs)\ndataset_dir &lt;- \"Data/cell_images/\"\ntest_dir   &lt;- paste0(dataset_dir, \"/test/\")\n# stored image paths in the image column\ntest_parasite &lt;- dir_ls(path=paste0(test_dir, \"parasite\"),glob = \"*.png\")\ntest_uninfected &lt;- dir_ls(path=paste0(test_dir, \"uninfected\"),glob = \"*.png\")\ntest_par &lt;- as.data.frame(matrix('parasite', length(test_parasite), 1))\ntest_unin &lt;- as.data.frame(matrix('uninfected', length(test_parasite), 1))\ntest_par$image &lt;- test_parasite \ntest_unin$image &lt;- test_uninfected\n\ntest &lt;- rbind(test_par,test_unin)\ncolnames(test)[1] &lt;- 'label'\ntest$normal  &lt;- ifelse(test$label != 'parasite', 1,0)\ntest$parasite &lt;- ifelse(test$label == 'parasite', 1,0)\nAnd the csv file looks like this. \nIn reality, we don’t usually see many cells infected with parasites, therefore less than 1/3 of the infected samples were used in this exercise.\n# Python\n# get ids for each label\nall_img_ids = list(new_df.uninfected.index.unique())\ntrain_ids, test_ids = train_test_split(all_img_ids, test_size=0.01, random_state=21)\ntrain_ids, valid_ids = train_test_split(train_ids, test_size=0.1, random_state=21)\nMaking sure, the proportion of the infected cell is as expected after data split. \nLet’s also check few images. The images come with different sizes. They will need to reshape and normalize before xx.\n# Extract numpy values from image column in data frame\ntrain_df = new_df.iloc[train_ids,:]\nimages = train_df['image'].values\n# Extract 9 random images \nrandom_images = [np.random.choice(images) for i in range(9)]\nimg_dir = 'C:/Users/your_image_folder'\nprint('Display Random Images')\n# Adjust the size of your images\nplt.figure(figsize=(20,10))\nfor i in range(9):\n    plt.subplot(3, 3, i + 1)\n    img = plt.imread(os.path.join(img_dir, random_images[i]))\n    plt.imshow(img)\n    plt.axis('off')\n\n\n\nrandom_images\n\n\n\n\nLoading data\nNext is building generators from the Keras framework. The purpose of building generator is that it allows to generate batches of tensor image data with real-time data augmentation(ex: random horizontal flipping of images). We also use the generator to transform the values in each batch so that their mean is 0 and their standard deviation is 1.Here is the information of ImageDataGenerator and a short tutorial. We’ll also need to build a sereperate generator for valid and test sets. Since each image will be normailized using mean and standard deviation derived from its own batch. In a real life scenario, we process one image at a time. And the incoming image is normalized using the statistics computed from the training set.\n# Train generator\ndef get_train_generator(df, image_dir, x_col, y_cols, shuffle=True, batch_size=8, seed=1, target_w = 224, target_h = 224):\n    \"\"\"\n\n    Args:\n      train_df (dataframe): dataframe specifying training data.\n      image_dir (str): directory where image files are held.\n      x_col (str): name of column in df that holds filenames.\n      y_cols (list): list of strings that hold y labels for images.\n      sample_size (int): size of sample to use for normalization statistics.\n      batch_size (int): images per batch to be fed into model during training.\n      seed (int): random seed.\n      target_w (int): final width of input images.\n      target_h (int): final height of input images.\n    \n    Returns:\n        train_generator (DataFrameIterator): iterator over training set\n    \"\"\"        \n    print(\"getting train generator...\") \n    # normalize images\n    image_generator = ImageDataGenerator(\n        samplewise_center=True,\n        samplewise_std_normalization= True)\n    \n    # flow from directory with specified batch size and target image size\n    generator = image_generator.flow_from_dataframe(\n            dataframe=df,\n            directory=image_dir,\n            x_col=x_col,\n            y_col=y_cols,\n            class_mode=\"raw\",\n            batch_size=batch_size,\n            shuffle=shuffle,\n            seed=seed,\n            target_size=(target_w,target_h))\n    \n    return generator  \nBefore, model building we’ll need to define a loss function to adress class imbalance. We can give more weight for the less frequent class and less weight for the other one, see here . We can write the overall average cross-entropy loss over the entire training set D of size N as follows:\n\n\n\nloss\n\n\nNext, we will use a pre-trained DenseNet121 model which we can load directly from Keras and then add two layers on top of it.\n\nSet include_top=False, to remove the orginal fully connect dense layer (so you can adjust the ouptut prediction clsses or\nactivation function).\nUse specific layer using get_layer(). For example: base_model.get_layer(‘conv5_block16_conv’)\n\nA GlobalAveragePooling2D layer to get the average of the last convolution layers from DenseNet121. The pooling layer typically uses a filter to extract representative features (e.g., maximum, average, etc.) for different locations. The method of extracting features from the pooling filter is called a pooling function. The commonly used pooling functions include the maximum pooling function, average pooling function, L2 normalization, and weighted average pooling function based on the distance from the center pixel. In short, the pooling layer summarizes all the feature information centered on each position of the input feature map, which makes it reasonable that the output data of the pooling layer is less than the input data. This method reduces the input data to the next layer and improves the computational efficiency of the CNN.\nThe output of the pooling layer is flattening to convert the pooled features maps into a single dimensional array. This is done in order for the data to be fed into densely connected hidden layers.\nA Dense layer with sigmoid activation to get the prediction logits for each of our classes. We can set our custom loss function for the model by specifying the loss parameter in the compile() function.\n# Build model\ndef create_dense121_model():\n    \n    pretrained = 'fine_tuned.hdf5'\n    train_df = pd.read_csv(\"train_df.csv\")\n    labels = ['uninfected', 'parasite']  \n    \n    class_pos = train_df.loc[:, labels].sum(axis=0)\n    class_neg = len(train_df) - class_pos\n    class_total = class_pos + class_neg\n\n    pos_weights =  class_pos / class_total #[0.5,class_pos / class_total]\n    neg_weights =  class_neg / class_total #[0.5,class_neg / class_total]\n    print(\"Got loss weights\")\n    \n    def create_model(input_shape=(224, 224,3)):\n        base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=input_shape)\n       \n        # add a global spatial average pooling layer\n        x = GlobalAveragePooling2D()(base_model.output)\n        x = Flatten()(x)\n        x = Dense(1024, activation='relu', name='dense_post_pool')(x)\n        x = Dropout(0.8)(x)\n        # output has two neurons for the 2 classes (uninfected and parasite)\n        predictions = Dense(len(labels), activation='sigmoid')(x)\n        model = Model(inputs = base_model.input, outputs = predictions)\n\n        # freeze the earlier layers\n        for layer in base_model.layers[:-4]:\n            layer.trainable=False\n        \n        return model\n    \n    def get_weighted_loss(neg_weights, pos_weights, epsilon=1e-7):\n        def weighted_loss(y_true, y_pred):\n            y_true = tf.cast(y_true, tf.float32)\n            #print(f'neg_weights : {neg_weights}, pos_weights: {pos_weights}')\n            #print(f'y_true : {y_true}, y_pred: {y_pred}')\n            # L(X, y) = −w * y log p(Y = 1|X) − w *  (1 − y) log p(Y = 0|X)\n            # from https://arxiv.org/pdf/1711.05225.pdf\n            loss = 0\n            \n            for i in range(len(neg_weights)):\n                loss -= (neg_weights[i] * y_true[:, i] * K.log(y_pred[:, i] + epsilon) + \n                         pos_weights[i] * (1 - y_true[:, i]) * K.log(1 - y_pred[:, i] + epsilon))\n            \n            loss = K.sum(loss)\n            return loss\n        return weighted_loss\n    \n   \n    model = create_model()\n    model.load_weights(pretrained)\n    print(\"Loaded Model\")\n    \n    model.compile(optimizer='adam', loss= get_weighted_loss(neg_weights, pos_weights)) \n    print(\"Compiled Model\")   \n          \n    return model\nModel is fine tuned using ModelCheckpoint and only the model’s weights will be saved.\n# CallBack \n# -------------------------------------------------------------------------------------------------\n# Callback Function 1\nfname = 'dense121(V)_Epoch[{epoch:02d}].ValLoss[{val_loss:.3f}].hdf5'\nfullpath = fname\n# https://keras.io/api/callbacks/model_checkpoint/\ncallback_func1 = ModelCheckpoint(filepath=fullpath,             \n                                monitor='val_loss',             \n                                verbose=1,                      \n                                save_best_only=True,            \n                                save_weights_only=True, # save weights       \n                                mode='min',                     \n                                period=1)                       \n\n# Callback Function 2\n# https://keras.io/callbacks/#tensorboard\ncallback_func2 = keras.callbacks.TensorBoard(log_dir='./logs/log2', histogram_freq=1)\n\n# Callback Function\ncallbacks = []\ncallbacks.append(callback_func1)\ncallbacks.append(callback_func2)\n\n# Training and Plotting\n# -------------------------------------------------------------------------------------------------\nhistory = model.fit(train_generator, \n                              validation_data=valid_generator,\n                              steps_per_epoch=100, \n                              validation_steps=25, \n                              epochs = 15,\n                              callbacks=callbacks)\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.ylabel(\"loss\")\nplt.xlabel(\"epoch\")\nplt.title(\"Training Loss Curve\")\nplt.show()\nTrain versus validation loss for all epochs is shown here. The orange and blue lines indicate train loss and validation loss respectively. We can see the model may be under-fitted. One way to overcome this is simply increase the number of epochs. Also with the callback function, we can re-use the best weights saved at 12th epoch.\n\n\n\nhistory\n\n\n\n\nEvaluation\nThe ROC curve is created by plotting the true positive rate against the false positive rate. We can see the model performs reasonable well.\n\n\n\nROC\n\n\nWe can try different approaches to improve the model perfromance, such as train the model for a longer time or use all the training data (since only 1/3 of the parasite data was used). We can also try a different base model, the previous publication, shows 99.32% accuracy with VGG-19 alone.\n\n\nVisualize class activation maps\nNext, I will show how to produce visual explanation using Grad-CAM. The purpose of doing this is as following:\n\nDebug your model and visually validate that it is “looking” and “activating” at the correct locations in an image.\nGrad-CAM works by (1) finding the final convolutional layer in the network and then (2) examining the gradient information flowing into that layer.\n\n\n\nNotes\nNote 1: AUC is the area below these ROC curves. Therefore, in other words, AUC is a great indicator of how well a classifier functions. Note 2: A good tutorial for to learn neural network image classification from scratch and Andrew Ng’s deep learning course.\nNote 3:\n# packages used \nimport os\nimport sklearn\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport keras\nfrom keras.applications.densenet import DenseNet121\nfrom keras.models import Model\nfrom keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization, GlobalAveragePooling2D\nfrom keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping, TensorBoard\nfrom keras import backend as K\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import ImageDataGenerator\n\n\n\n\n Back to top"
  },
  {
    "objectID": "gallery.html",
    "href": "gallery.html",
    "title": "Chris Lee",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nMarketing Campaign at a Glance\n\n\n\n\n\n\n\n\n\n\n\n2024-07-15\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nImage Classification (Part2)\n\n\n\n\n\nSave time & Money! \n\n\n\n\n\n2021-08-31\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nImage Classification (Part1)\n\n\n\n\n\nSave time & Money! \n\n\n\n\n\n2021-08-31\n\n\n14 min\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "assets/other/tags-archive.html",
    "href": "assets/other/tags-archive.html",
    "title": "Posts by Tags",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Chris Lee",
    "section": "",
    "text": "Hello\n\n\n\n\n\n\n\nI’m driven by helping others successfully solve problems like turning complex data into clear, actionable insights that help businesses thrive. My toolkit to deliver these solutions draws on blending business understanding with machine learning using SQL, R, SAS, and Python. I’m passionate about using data to solve real-world problems, streamline processes, and find new growth opportunities. Balancing the technical side with a strong understanding of business needs, I create solutions that make a real impact and keep things moving forward.\nFeel free to reach out to me on LinkedIn or drop me an email to discuss.\n\n\n\n\n\nEducation:\n\nUniversity of Manchester | UK PhD in Cancer Studies | Sept 2004 - Jan 2009\nNational Yang-Ming University | Taipei, Taiwan Msc in Genetics| Sept 2001 - Sept 2003\n\nExperience:\n\nFUJI WEDDING VENUE GROUP | Senior Data Analyst | Jan 2023 - PRESENT\nFUJI WEDDING VENUE GROUP | Data Analyst | Jan 2016 - Dec 2022\nProtea | Senior Scientist | Jan 2014 - Mar 2015\n\n\n\n\n Back to top"
  },
  {
    "objectID": "assets/other/sitemap.html",
    "href": "assets/other/sitemap.html",
    "title": "Website Structure",
    "section": "",
    "text": "Pages\n\n{% for post in site.pages %} {% include archive-single.html %} {% endfor %}\n\nPosts\n\n{% for post in site.posts %} {% include archive-single.html %} {% endfor %}\n{% capture written_label %}‘None’{% endcapture %}\n{% for collection in site.collections %} {% unless collection.output == false or collection.label == “posts” %} {% capture label %}{{ collection.label }}{% endcapture %} {% if label != written_label %}\n\n{{ label }}\n\n{% capture written_label %}{{ label }}{% endcapture %} {% endif %} {% endunless %} {% for post in collection.docs %} {% unless collection.output == false or collection.label == “posts” %} {% include archive-single.html %} {% endunless %} {% endfor %} {% endfor %}\n\n\n\n Back to top"
  },
  {
    "objectID": "assets/other/year-archive.html",
    "href": "assets/other/year-archive.html",
    "title": "Posts by Year",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Chris Lee",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "posts/2021-08-31-malaria-cell-classification - p2/index.html",
    "href": "posts/2021-08-31-malaria-cell-classification - p2/index.html",
    "title": "Image Classification (Part2)",
    "section": "",
    "text": "When building a predictive model, there are two important criteria: predictive accuracy and interpretability, which generally have a trade-off relationship. In the previous post, we have shown that the DenseNet121 model can achieve high accuracy in detecting cells infected with parasites.\nHere, I am going to introduce a powerful technique GRAD-CAM (gradient-weighted class activation mapping) to visualize which parts of an image are most important to the predictions of an image regression network. GRAD-CAM is a generalization of the CAM technique which determines the importance of each neuron in a network prediction by considering the gradients of the target flowing through the deep network. Unlike CAM which requires a particular kind of CNN architecture to perform global average pooling prior to prediction and forces us to change the base model retrain the network. In contrast, GRAD-CAM is accessing intermediate activations in the deep learning model and computing gradients with respect to the class output. For more details, please see.\nWorkflow: - Obtain predicted class/index - Determine which intermediate layer(s) to use. Lower-level convolution layers capture low level features such as edges, and lines. Higher-level layers usually have more abstract information. - Calculate the gradients with respect to the outout of the class/index - Generate a heatmap by weighing the convolution outputs with the computed gradients - Super-impose the heatmap to the original image\nLoad base model\nWe first load the base model and will only train the last 4 layers.\ndef build_model(input_shape=(224, 224,3),pos_weights,neg_weights):\n  # load the base DenseNet121 model\n  base_model = DenseNet121(input_shape = input_shape, \n                      weights='imagenet', \n                      include_top=False)\n  \n  # add a GAP layer\n  output = layers.GlobalAveragePooling2D()(base_model.output)\n\n  # output has two neurons for the 2 classes (uninfected and parasite)\n  output = layers.Dense(2, activation='softmax')(output)\n\n  # set the inputs and outputs of the model\n  model = Model(base_model.input, output)\n\n  # freeze the earlier layers\n  for layer in base_model.layers[:-4]:\n      layer.trainable=False\n\n  # configure the model for training\n  model.compile(loss= get_weighted_loss(neg_weights, pos_weights), \n                optimizer=adam, \n                metrics=['accuracy'])\n  \n  return model\n \nWe then create a new model that has the original model’s inputs, but two different outputs. The first output contains the activation layers outputs that in this case is the final convolutional layer in the original model. And the second output is the model’s prediction for the image.\ndef get_CAM(model, processed_image, actual_label, layer_name): \n    \"\"\"\n    GradCAM method for visualizing input saliency.\n    \n    Args:\n        model (Keras.model): model to compute cam for\n        image (tensor): input to model, shape (1, H, W, 3)\n        cls (int): class to compute cam with respect to\n        layer_name (str): relevant layer in model\n        H (int): input height\n        W (int): input width\n    Return:\n        heatmap()\n    \"\"\"    \n\n    model_grad = Model([model.inputs], \n                       [model.get_layer(layer_name).output, model.output])\n    \n    with tf.GradientTape() as tape:\n        conv_output_values, predictions = model_grad(processed_image)\n\n        # assign gradient tape to monitor the conv_output\n        tape.watch(conv_output_values)\n        \n        # use binary cross entropy loss, actual_label = 0 if uninfected\n        # get prediction probability of infected  \n        pred_prob = predictions[:,1] \n        \n        # make sure actual_label is a float, like the rest of the loss calculation\n        actual_label = tf.cast(actual_label, dtype=tf.float32)\n        \n        # add a tiny value to avoid log of 0\n        smoothing = 0.00001 \n        \n        # Calculate loss as binary cross entropy\n        loss = -1 * (actual_label * tf.math.log(pred_prob + smoothing) + (1 - actual_label) * tf.math.log(1 - pred_prob + smoothing))\n        print(f\"binary loss: {loss}\")\n    \n    # get the gradient of the loss with respect to the outputs of the last conv layer\n    grads_values = tape.gradient(loss, conv_output_values)\n    grads_values = K.mean(grads_values, axis=(0,1,2))\n    \n    conv_output_values = np.squeeze(conv_output_values.numpy())\n    grads_values = grads_values.numpy()\n    \n    # weight the convolution outputs with the computed gradients\n    for i in range(grads_values.shape[-1]): \n        conv_output_values[:,:,i] *= grads_values[i]\n    heatmap = np.mean(conv_output_values, axis=-1)\n    \n    heatmap = np.maximum(heatmap, 0)\n    heatmap /= heatmap.max()\n    \n    del model_grad, conv_output_values, grads_values, loss\n   \n    return heatmap\n\n\n\nResult\n\n\nNote: Instead of using max pooling that only keeps the highest valued ones. Average pooling allows some of the lesser intensity pixels to pass on in the pooling layer. It is important as we look at the small size of the image once it reaches this layer, max pooling could leave us with very little information.\n\n\n\n Back to top"
  }
]